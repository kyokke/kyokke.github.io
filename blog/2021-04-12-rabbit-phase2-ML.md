@def title = "ラビットチャレンジ: Phase.2 機械学習"
@def author = "kyokke" 
@def tags = [ "Deep-Learning", "Rabbit-Challenge" ]


# ラビットチャレンジ: Phase.2 機械学習 [執筆途中]

本ページはラビットチャレンジの、
Phase.2 機械学習のレポート提出を兼ねた受講記録です。
提出指示に対して、下記の方針でまとめました

1. 動画講義の要点まとめ
   - 自分がその手法を思い出して実装するのに必要な最低限の情報 (モデル定義・評価関数・解放の着想があれば十分かなぁ.. )
   - 講義で口頭補足されたトピックなどあれば。

2. 実装演習(ハンズオン実行)
   - scikit-learn 実装のハンズオンを実行
   - アルゴリズム部分を numpy 実装に置き換えて同じ問題を解いてみる
   - 上記を行った、ノートブックを jupyter nbconvert で markdown に変換しその抜粋を掲載
     - markdown 部分、コード部分ともに、blog上での表示や見やすさ考慮してあとから微調整を行う
     - 手法に関する一般的な説明・数式展開などは、要点のまとめ側に移動したものもある
     - なにか追加でやりたくなったら、適当にセル追加してやってみる

## 各論前の講義内容　[課題外]

 - プロローグ ( なぜ 非ディープラーニングの機械学習の勉強をするのか? )
   - 数学的な背景を元にした機械学習の基礎を抑えていないエンジニアは、フレームワークを使って機械学習モデルを組める「程度」の人材にしかならない

   - 機械学習のモデリングプロセスをしっかりと抑えることが重要
     1. 問題設定
        - 最終的な使われ方をイメージしよう
        - 必ずしも機械学習を使う必要は無く、ルールベースで解けるならそっちの方が楽
          - 技術的負債になりやすい
            - 自分が開発した技術を運用チームに移管するとして、移管先に専門知識を持った人間がいなければSLA(Service Level Agreement)を担保できない
          - テストしにくい・考慮すべきことが多い
          - データ集めるの大変
     2. データ選定
        - GIGO(garbage in garbage out) = ゴミを突っ込んだらゴミが出てくる
          - ex. 集めたデータに(意図しない)バイアスがかかっているなど。
     3. データの前処理
        - 開発時間の殆どは、このプロセスに費やされるといっても過言でない
        - 実務経験がモノをいうところ。練習のチャンスがない人は、kaggleなどやるといい
     4. モデルの選定
        - ディープラーニング(で扱われるモデル)も、機械学習モデルの一部に過ぎず、このプロセスの具体的な作業が異なるだけ
     5. モデルパラメータの決定
     6. モデルの評価

 - ルールベースモデルと機械学習モデル
   - 技術者として、機械学習とは?と聞かれたら何らかの形で答えられるようになっておくこと
     - 講師の体感では、機械学習ってなに? と聞かれるよりも、人工知能って何? と聞かれることの方が多いらしい 

## 線形回帰モデル [提出対象]

### 動画講義メモ

 - 回帰問題 = 入力から出力を予測する問題。線形回帰は、回帰式が線形。
   - 線形回帰モデルの形 (線形とは?) 
     - ざっくり説明 = 入出力が比例関係である (直線・平面・超平面)
     - $ y = w_0 + \sum_{i=1}^{n-1} w_i x_i $
       - $ = \sum_{i=0}^{n-1} w_i x_i $　
       - $ = \bm{w}^T \bm{x} $  ただし $ \bm{w}^T = (w_0, w_1, ... , w_{n-1})$, $ \bm{x}^T = (1, x_1, ... , x_{n-1})$ 
     - 記法としては ベクトルが便利、訳わからなったら sigma 記法にする
       - このベクトル/行列と 要素ごとの記法の行き来ができることがポイント
       - この後の記述とあわせて係数は $w$ にした
    - 出力について
      - 連続値とスライドに書かれているが、離散値でもいい
        - 例: 諸条件から、競馬の順位を予想する
          - cf. "vapnik の原理" には反する
            - 順位 = 大小関係 がわかるだけでいいのに、もっと難しい回帰問題を解くべきではないという話。
            - [論文](https://www.ism.ac.jp/editsec/toukei/pdf/58-2-141.pdf)
      - スカラーと書いてあるが、多次元のベクトルにしてもいい(マルチタスク学習など)
      - データ分割・学習
        - 未知のデータに対する予測精度をあげたいので、テスト用のデータと、学習用のデータは分ける
        - 最尤法による解とMSEの最小化は等価と書いてあったが、それは誤差を確率変数としたとき、これが正規分布に従う場合では? (他の分布なら、別の解でてくる場合もあるよね?) 
    - 誤差について
      - 必ずしも偶発的な誤差だけではなく、モデル化誤差もある
        - y = 来店者数、x = 気温 で予測ができる? いや、曜日にも影響する。このとき、曜日の項の影響は誤差になる
    - 未知数の数と方程式の数について
      - 今、重みw は m+1 次元なので、基本的には、 m+1 以上の方程式がないと厳しい
        - データの方が少ないケースを扱う手法もあるが、それは advanced 
        - DLの場合パラメータがたくさんあるので、データが必要
    - パラメータ推定方法 : 最小二乗法
      - $ \mathrm{MSE_{train}} = \frac{1}{n} \sum_{i=1}^{n_{train}} (\hat{y_i}^{(train)}-y_i^{(train)})^2 = \sum \epsilon^2 :=  J(w) $
        - MSE = mean square error
      - これをパラメータについて微分して、勾配が0になる点を求める
        - $ \bm{\hat{w}}= (X^T X) ^{-1} X^T \bm{y} $  : (train) は省略
          - $\bm{y}$ に 一般化逆行列をかけた形
        - 必ずしも誤差関数として二乗誤差は最適ではないことに注意
          - 基本的にハズレ値の存在にかなり弱くなる
          - Huber loss, Tukey loss とか　を使うとハズレ値に強くなる
       - よく使う ベクトルの微分の形
         - $ \frac{\partial}{\partial \bm{w}} \bm{w} ^T \bm{x} = x$
         - $ \frac{\partial}{\partial \bm{w}} \bm{w}^T A \bm{w} = (A+A^T) \bm{x} = 2A\bm{x}$ (Aが対称行列のとき)
         - 参考図書: Matrix Cook Book 

 - ハンズオンに関するコメント
   - 全部の説明変数を使う必要はないし、使うべきではないことがある
    - 12番目の "アフリカ系アメリカ人居住者の割合" など
   - 現実のデータセットを扱う上では、よくデータをみる必要がある
     - 得られたデータの中に使用すべきでないもの(ハズレ値など) があるかもしれない
     - 要約統計量などを適宜活用
     - pandas で 12行目まで観察 ( 頭の数行みて、怪しい所があったから。最初の5行 CHAS全部0じゃね?)
   - エイヤで学習すると、マイナスの価格が出てしまった件
     - こういう現象が出た時点でおかしい、と気付かなければいけない
     - 何がダメだったんだろう -> 1部屋のケースなどがデータに入ってないんだろうな
       - 外挿問題にDLは基本的には弱い 5~10部屋の範囲のデータから学習したとき、11部屋, 2部屋の時の予測は上手く行かない
   - scikit-learn や tensorflow で動かすのは小学生でもできる。なんとなく動かすんじゃなくて、数式に対する理解を持つこと
   - 多重回帰の場合も試しに実行してみてね
     - 学習されたモデルを評価する
      - 部屋を増やしてみる(価格増えるはず)
      - 犯罪率増やしてみる(価格下がるはず)

\textinput{skl_regression.md}


## 非線形回帰 
\textinput{np_regression.md}

np_regression に非線形回帰の話ものってる


## ロジスティック回帰
\textinput{np_regression.md}

## k-nn/means 法
\textinput{np_regression.md}

## 主成分分析 (PCA) 
\textinput{np_regression.md}

## サポートベクターマシン(SVM)
\textinput{np_regression.md}


## 取り組み概要 & 感想

下記のような日程・内容で取り組んだ

- 3/31, 4/1: まずはざっと新しい動画を視聴(1.5倍速)しながらメモをとり・微分計算。基本的に動画を途中でストップすることはなかった。約200分で完了。まぁ、問題なさそうだなぁ。という感触だけ得る。

- 4/1~28 : しばらく塩漬け状態.. (GTC21の聴講などをしていたので)

- 4/29 : ハンズオンの実行環境の構築。Collaborately はセル実行のレスポンスが遅いのでローカルに環境を整える。win10上に pyenv + poetry でを仮想環境を作る。30分程度。

- 4/29-xx : 最新動画はハンズオンのところをかなりすっ飛ばしていた印象だったので、ハンズオンを実行・コードを追いかけながら、動画を見てみる。 xx  分　
  -  

前半
12+4+2+3+2+2.5+2+2+2.5+2+2+2+2+2+3+2+1+3+3+4　= 60分
後半
12+4+2+3+2+2.5+2+2+2.5+2+2+2+2+2+3+2+1+3+3+4 = 78分
100分くらい

- xx - xx : えいやとレポートをまとめる xx 分

- xx : テスト。特に手が止まることはなく一発合格。yy 分

動画講義の最新版・旧版についてだが、




最新動画はハンズオンの部分をかなりすっ飛ばしていた印象だったので、旧動画の方も見てみる。




実行・編集したノートブックは以下のレポジトリで管理 -> 

ハンズオンの部分もとりあえず見てしまったが、かなりすっ飛ばしていた印象。



新しい動画の方はハンズオンについてはかなりすっ飛ばしていたので、とりあえず見るだけ。


ハンズオン

線形代数は 2/10-15 の間に少しずつ消化。それからしばらく、勤め先の業務の忙しさにかまけて1ヶ月計画的に放置したが、その後 3/16-18, 22 に統計学の講義動画を試聴。内容に応じて 1.2-1.5倍速で再生しつつ、基本的には動画を止めずにメモを箇条書きレベルでまとめていった。合計 5h 程度。

3/28, 29に動画講義のまとめを行う。基本的には、動画視聴中の箇条書きメモを markdown として整形しつつ、読み返して講義の内容を思い出せないところがあれば、少し見返して補った。合計 2h 程度。

3/30。演習は 20分、ステージテストは 35分 なので 合計約 1h。スタートテストと同じく、Python のインタプリタを起動して検算できるようにしつつも、紙・鉛筆も用意して、適宜使いやすい方を使う。

以上、合計 8h で終了。

演習については、線形代数の四則演算をのぞけば、
用語の定義自体を問題文で説明した上でシンプルな問に答えさせる問題や、用語の定義自体の理解を問う問題が大半だった。実力試し・理解度確認のための演習というより、講義動画の理解を助けるものという印象。例えば、自己情報量やエントロピーの説明は、講義と若干違う説明の仕方がされており、個人的には演習の説明のほうがすっきり理解できる。講義動画見ながら該当箇所の演習をやればよかったな、と思う。(次からそうしよう)


## 計画の見直し (2021/04/30 時点)

元々の計画では、3月中にステージ2まで終わらせる予定だったので、計画を下記のように見直した。

 - ~2021/2/15  : スタートテスト (2021/02/07完了)
 - ~2021/3/30  : ステージ1      (2021/03/30完了)
 - ~2021/4/18  : ステージ2      (2021/04/30完了)
 - ~2021/5/09  : ステージ3 
 - ~2021/6/06  : ステージ4 
 - ~2021/6/27  : 復習 -> 修了テスト 
 - ~2021/7/15  : Eもぎライト -> 今後の計画具体化 
 - ~2021/7/30  : シラバスの未習箇所の学習 
 - ~2021/8/26  : 全体の復習
 - 2021/8/27,28: E資格 受験 

ステージ2,3,4 のボリュームが読めないので、上記を少し前倒しですすめられるといいなとは思っている。

ラビットチャレンジの修了が後ろ倒しになったが、最近E資格を取得された方の話を聞く限り、修了後の勉強期間にそこまで長い時間を設けなくて良さそうな印象だったので問題ないと考える。