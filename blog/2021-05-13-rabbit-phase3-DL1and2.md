@def title = "ラビットチャレンジ: Stage.3 深層学習 Day1,2"
@def author = "kyokke" 
@def tags = [ "Deep-Learning", "Rabbit-Challenge" ]


# ラビットチャレンジ: Stage.3 Day1, 2  

本ページはラビットチャレンジの、
Stage.3 "深層学習 Day1,2" のレポート提出を兼ねた受講記録です。
提出指示を満たすように、下記の方針でまとめました。(事務局にも問い合わせて問題ないことを確認した)

1. 動画講義の要点まとめ
   - 自分が講義の流れを思い出せるようなメモを残す。通常であれば要点として記載すべき図・数式などがあっても、それが自分にとって既知であれば、言葉の説明ですませることもある
2. 実装演習
   - 各Sectionで取り上げられた .ipynb or .py ファイルの内容を実行した結果を記載
   - ただし、初学者向けにやや冗長な内容がある場合、抜粋することもある
3. 確認テスト
   - 確認テストの解答に相当する内容は、個別の節をもうけず、要点まとめに含めたり、コードに対するコメントとして記載する
     - 確認テストは重要事項だから、出題されているのであって、まとめ/演習と内容がかぶるはず。
     - 事務局がレポートチェックをする時のために、(確認テスト:1-2) のようなタグを付す。１つ目の数字が section番号、２つ目の数字が 「何番目のテストか?」
4. 1~3 をまとめる上で思うことがあれば、考察やコメントも適宜残しておく。


## 目次
\toc

## Day1 
### プロローグ　

 - 機械学習/DLモデルが解こうとしている問題
    - 識別 (discriminateve, backward)
      - データ $\bm{x}$ -> クラス $C_k$ 
        - $ p (C_k | \bm{x}) $ を計算する (確率最大となる $C_k$ を分類結果にする)
        - 例: 画像認識 ( 犬やネコの画像データを入力すると、犬なのか、ネコなのかを出力する) 
      - 高次元 -> 低次元
        - 学習データは比較的少なめ
      - モデルの例
        - 決定木, ロジスティック回帰, SVM, NN
      - 開発のアプローチ (Phase.2でやった話)
        - 生成的アプローチ 
          - $p(\bm{x} | C_k) , p(C_k)$ をモデル化・推定し、 ベイズの定理から $p(C_k| \bm{x}) $ を計算する
            - データの分布 $p(\bm{x}|C_k)$ は分類結果よりも複雑なことがある。
              - その分学習コスト大。
              - 単純に分類結果を得たいだけならば、識別的なアプローチを用いる。
              - 副産物として得られた生成モデルを活用したい場合は良い。
          - 確率的な識別ができる。(識別的アプローチと同様)
 - 識別的アプローチ
    - 直接　 $p(C_k| \bm{x}) $ をモデル化・推定する
    - 確率的な識別
      - 機械学習のモデルの出力は 事後確率 $p(C_k|\bm{x})$ であり、そこからどのように識別結果を得るかは、開発者の裁量。
      - 自信のある無しの度合いを測ることができる。
    - 学習コスト中
    - 確率的識別モデル
 - 識別関数
    - 入力 $ \bm{x} $ から 出力のクラスへの写像 $f(\bm{x})$ を直接推定
    - 学習データ・コストが少ない。決定的な識別
    - 決定的識別モデル
 - 生成 (generative, forward)
    - クラス $C_k$ ->  データ $\bm{x}$
      - $ p (\bm{x}|C_k ) $ の分布情報を持った上で、サンプリングする。
        - データの分布は複雑になりがち。実際に
      - 例: 犬の画像くれ、といったら 犬の画像っぽいものを出力してくれる, テキスト生成、画像の超解像など
    - 低次元 → 高次元
      - 学習データは大量に必要
    - モデルの例
      - HMM, ベイジアンネットワーク, VAE, GAN

 - 万能近似定理
   - 非線形の活性化関数を持つNeuralNetworkは任意の関数を近似できる

### 0. ニューラルネットワークの全体像
 
 - 入力層、中間層、出力層で構成される
   - 識別モデルの例では、入力層のノードにあたるのはデータ、出力層のノードはそれぞれのクラスに属する確率。 
   - (確認テスト:0-1) 
     - ディープラーニングがやろとしていること
       - 入力されたデータを変換して所望の出力に変換する数学モデルをつくる
       - ここで使用されるのはパラメータを持つ中間層を複数使用して高い自由度が得られるようなモデル
     - 学習によって最適化するのは
       - パラメータ　(3. 重み と 4. バイアス)
 - 数式も用いて説明
   - (確認テスト:0-2) 入力層2ノード, 中間層3ノード 出力層1ノード のネットワーク
~~~
<figure style="text-align:center;">
<img src="/assets/20210514-nn.svg" style="padding:0;width:150%;" alt="#1"/>
<figcaption></figcaption>
</figure>
~~~

 - ニューラルネットワークでできること
   - 回帰 : (主に) 連続的な実数値を取る関数の近似
     - 例
       - 結果の予想 (売上、株価など)
       - ランキング (競馬順位, 人気順位) 
     - NN以外の手法
       - 線形回帰, ランダムフォレスト、回帰木
   - 分類 : 数値の大小関係に意味のない、離散的な結果を予想するための分析
     - 例
       - ネコ写真判別
       - 手書き文字認識
       - 花の種類分類
     - NN以外の手法
       - ベイズ分類, ロジスティック回帰, 決定木, ランダムフォレスト
 - 深層学習の実用例
   - 深層学習モデルはとても表現力が高いため、入力と出力を数値・ベクトルに変換してしまいさえすれば、あらゆる問題に応用できる可能性がある
   - 例えば、自動売買、チャットボット、翻訳、音声解釈、囲碁/将棋AI など

### 1. 入力層-中間層

  - (確認テスト:1-1) 入力層~中間層の図に Section.0で取り上げた動物分類の実例を書き入れてみると以下のようになる 
    -  ![確認テスト:1-1](/assets/確認テスト1-1.png)
    - 愚痴コメント: "この図式に"実例を入れると書いてあるのだから、入力4次元のNNに合わせて実例の方を変えるべきですよね..  (模範解答のような回答を期待しているのならば、「実例の図から、この図式のような形で入力層~中間層までに相当する部分を抜き出せ」というような問題文になるはず)
   - $u=\sum_{i=1}^4 w_i x_i + b$ の変換を行い、それが次の中間層の入力となり、活性化関数がかかって中間層の出力となる
     - 学習に置いては、この $w_i, b$ が最適化される
     - 各計算式に対応する箇所は、実装演習のコードにコメントした。

  - "12_入力層の設計" の動画の内容
    - 深層学習モデルに入力するには、とにかく対象を数値ベクトルにすればよい
      - 例: カテゴリ変数のようなものも フラグ値として定義可能
        - 犬・ネコ・ネズミの三種のいずれかを表したい場合、犬=[1,0,0], ネコ=[0,1,0], ネズミ=[0,0,1] などとする (ワンホットラベル,ワンホットベクトル)
    - 入力層に入れるデータの選定
      - 欠損値が多いデータ、誤差の大きいデータは学習データからは取り除く
      - 解きたい問題の"生の入力" を入れて問題が解けるのが理想
        - end2end のモデルを作るほうが全体最適なモデルができあがるはずで(理屈上は)ベター.
      - 意味なく振られた数値は使えない
        - 背番号などは必ずしも人と一対一対応しない
        - 誤差計算をするのに適切でない場合 (Yes:1 No:0, どちらでもない:-1, 無回答:-1) 
          - 別の意味を持つ回答が同じ数値に割り当てられている
          - Yes - No = 1,  No - 無回答= 1 両者の違いが等しいわけではない
          - -> これらの問題を解決するには、ワンホットベクトルを使う
    - 入力データの加工例
      - 欠損値の扱い
        - ゼロで埋める
        - 欠損値がやたらと多い説明変数はそもそも使わない
        - 色々な次元で欠損値があるサンプルはそもそも使わない
      - 数値の正規化・正則化
        - 正規化: (絶対値の)最大値で割って -1 ~ 1 とか、0~1 の範囲におさめる
        - 正則化: 平均0, 分散を1にする
      - データの結合
        - 読んで時の通り。中には、数量的なデータとカテゴリ変数だったものを(ワンホットエンコードして)結合するようなこともある
      
    

#### 実装演習抜粋 ( 順伝播（3層・複数ユニット）)

数式では、$Wx+b$ なのに python では np.dot(x,W) となる?
np.dot() 関数の挙動をしっかり抑えておく必要がある。

  - 基本の動作は行列積(のようなもの)
    - 要素数が一致する次元: a の最後の axis と b の 最後から2番目のaxis 
    - 通常の行列積も包含
  - 下記が適応不可能な自体では、よしなにそれっぽい計算が行われる
    - 例:
      - if a,bの両方orどちらかがスカラー: 普通の積. (スカラーをもう一方の全要素にかける) 
      - elif b がベクトル: a の最後の axis と b の長さが一致
        - ベクトルの内積のケースも包含



```python
# 順伝播（3層・複数ユニット）

# ウェイトとバイアスを設定
# ネートワークを作成
def init_network():
    print("##### ネットワークの初期化 #####")
    network = {}
    
    # 試してみよう
    #_ネットワークの初期値ランダム生成
    network['W1'] = np.random.rand(2,3)
    network['W2'] = np.random.rand(3,2)
    network['W3'] = np.random.rand(2,2)
    network['b1'] = np.random.rand(3)
    network['b2'] = np.random.rand(2)
    network['b3'] = np.random.rand(2)

    # 試してみよう
    #_各パラメータのshapeを表示
    print("*各パラメータのサイズ情報")
    for k,v in network.items():
        print("network[%s].shape=" % k, end='')
        print(v.shape)
    print()

    # 変更前
    # network['W1'] = np.array([
    #     [0.1, 0.3, 0.5],
    #     [0.2, 0.4, 0.6]
    # ])
    # network['W2'] = np.array([
    #     [0.1, 0.4],
    #     [0.2, 0.5],
    #     [0.3, 0.6]
    # ])
    # network['W3'] = np.array([
    #     [0.1, 0.3],
    #     [0.2, 0.4]
    # ])
    # network['b1'] = np.array([0.1, 0.2, 0.3])
    # network['b2'] = np.array([0.1, 0.2])
    # network['b3'] = np.array([1, 2])

    print_vec("重み1", network['W1'] )
    print_vec("重み2", network['W2'] )
    print_vec("重み3", network['W3'] )
    print_vec("バイアス1", network['b1'] )
    print_vec("バイアス2", network['b2'] )
    print_vec("バイアス3", network['b3'] )

    

    return network

# プロセスを作成
# x：入力値
def forward(network, x):
    
    print("##### 順伝播開始 #####")

    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    # 1層の総入力
    u1 = np.dot(x, W1) + b1  # (確認テスト:1-2) 入力層の計算に該当
    
    # 1層の総出力
    z1 = functions.relu(u1)  
    
    # 2層の総入力
    u2 = np.dot(z1, W2) + b2
    
    # 2層の総出力
    z2 = functions.relu(u2) # (確認テスト:1-3) 中間層の出力に該当

    # 出力層の総入力
    u3 = np.dot(z2, W3) + b3
    
    # 出力層の総出力
    y = u3
    
    print_vec("総入力1", u1)
    print_vec("中間層出力1", z1)
    print_vec("総入力2", u2)
    print_vec("出力1", z1)
    print("出力合計: " + str(np.sum(z1)))

    return y, z1, z2

# 入力値
x = np.array([1., 2.])
print_vec("入力", x)

# ネットワークの初期化
network =  init_network()

y, z1, z2 = forward(network, x)
```

```
    *** 入力 ***
    [1. 2.]
    
    ##### ネットワークの初期化 #####
    *各パラメータのサイズ情報
    network[W1].shape=(2, 3)
    network[W2].shape=(3, 2)
    network[W3].shape=(2, 2)
    network[b1].shape=(3,)
    network[b2].shape=(2,)
    network[b3].shape=(2,)
    
    *** 重み1 ***
    [[0.46325703 0.4515028  0.9015613 ]
     [0.63300315 0.09960203 0.35035177]]
    
    *** 重み2 ***
    [[0.85561969 0.64270626]
     [0.71017196 0.35407221]
     [0.21520079 0.12526929]]
    
    *** 重み3 ***
    [[0.04970514 0.3372038 ]
     [0.80197002 0.34030274]]
    
    *** バイアス1 ***
    [0.14282813 0.02490021 0.42369957]
    
    *** バイアス2 ***
    [0.16724682 0.77646073]
    
    *** バイアス3 ***
    [0.97460445 0.08008213]
    
    ##### 順伝播開始 #####
    *** 総入力1 ***
    [1.87209147 0.67560708 2.02596442]
    
    *** 中間層出力1 ***
    [1.87209147 0.67560708 2.02596442]
    
    *** 総入力2 ***
    [2.6848315  2.47267045]
    
    *** 出力1 ***
    [1.87209147 0.67560708 2.02596442]
    
    出力合計: 4.573662972619907
```

### 2. 活性化関数

 - 活性化関数
   - 次の層への出力を決める (一般的には)非線形の関数 
     - Section.0 の図中の関数 $\mathbf{f}$ に相当
   - (確認テスト:2-1) 線形と非線形
     - 図にすると
       - 線形 ![svg](/assets/1_1_forward_propagation_16_0.svg) 
       - 非線形 ![svg](/assets/1_1_forward_propagation_16_1.svg)
     - 線形な関数は、加法性 $ f(a+b) = f(a) + f(b) $ と 斉次性(作用素との可換性)　$ f(ab) = a f(b) $ を満たす
     - 非線形な関数は、その名の通り、線形な関数 "以外" 
   - 活性化関数の例
     - 中間層で用いられる代表例
       - ステップ関数 
         - パーセプトロンで使われていた経緯があるが 0 or 1 しか表せないので 線形分離可能なものしか識別できないということで最近は使われない
       - シグモイド関数
         - すべての点で微分可能で扱いやすいが、勾配消失問題を引き起こしやすい
       - ReLU関数
         - シグモイドに比べれば勾配消失問題を起こしにくい
         - スパース化に貢献
     - 出力層でもちいられる代表例
       - ソフトマックス関数
       - 恒等写像 (これは非線形関数じゃないから)

##### 実装演習抜粋 (順伝播 単層・複数ユニット）


```python
# 順伝播（単層・複数ユニット）

# 重み
W = np.array([
    [0.1, 0.2, 0.3], 
    [0.2, 0.3, 0.4], 
    [0.3, 0.4, 0.5],
    [0.4, 0.5, 0.6]
])

## 試してみよう_配列の初期化
#W = np.zeros((4,3))
#W = np.ones((4,3))
#W = np.random.rand(4,3)
W = np.random.randint(5, size=(4,3)) # randint を試してみた例

print_vec("重み", W)

# バイアス
b = np.array([0.1, 0.2, 0.3])
print_vec("バイアス", b)

# 入力値
x = np.array([1.0, 5.0, 2.0, -1.0])
print_vec("入力", x)


#  総入力
u = np.dot(x, W) + b
print_vec("総入力", u)

# 中間層出力
z = functions.sigmoid(u)  # (確認テスト:2-2) 
print_vec("中間層出力", z)

```
```
    *** 重み ***
    [[3 1 4]
     [3 1 0]
     [4 2 3]
     [0 3 4]]
    
    *** バイアス ***
    [0.1 0.2 0.3]
    
    *** 入力 ***
    [ 1.  5.  2. -1.]
    
    *** 総入力 ***
    [26.1  7.2  6.3]
    
    *** 中間層出力 ***
    [1.         0.99925397 0.99816706]
 ```   
 
### 3. 出力層

 - 誤差関数
   - その時々のパラメータを用いて、入力データから計算したモデルの出力が、どれくらい間違っているかを計算するもの
     - 予め 入力データ(NNモデルの入力)　と 訓練データ(入力データに対応する正解値)を用意しておく
   - 例: 二乗誤差和 $ \frac{1}{2} \sum_{j=1}^{J} (y_j-d_j)^2 $ 
     - (確認テスト:3-1) 
       - 差を二乗する理由: 各要素毎の誤差を非負値にすることで、完全にベクトルが一致するとき以外誤差が0にならないようにするための一方法
       - 1/2 している理由: 最適化計算のための微分計算をすることになるのだが、二乗を微分してでてくる 2 と打ち消し合って式がよりシンプルになることを狙っている。
     - 通常分類問題では誤差関数にクロスエントロピー誤差を用いることが多いが、本講義では説明の便宜上平均二乗誤差が用いられる箇所がある
 - 出力層の活性化関数
   - 中間層とは役割が異なるため、利用される活性化関数も異なる
   - モデル出力を解いている問題にあった使いやすい形に変換するために用いる
     - 分類問題では、入力データがどの分類クラスのデータであるかを表す確率を出力させる
       - 出力ベクトルの各要素は 0~1の範囲で、全要素の和が1
   - 例
     - 回帰問題
       - 活性化関数 : 恒等写像
       - 誤差関数 : (平均)二乗誤差
     - 二値分類
       - 活性化関数 : シグモイド関数
       - 誤差関数: 交差エントロピー
     - 多クラス分類
       - 活性化関数 : ソフトマックス関数 　
       - 誤差関数: 交差エントロピー

#### 実装演習 (誤差関数定義)
厳密には、実装演習は無し。確認テストのために関数定義を確認しただけ

```python
# ソフトマックス関数　(確認テスト:3-2)
def softmax(x):
    if x.ndim == 2: # 複数データが入力された場合のための場合分け
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T

    x = x - np.max(x) # オーバーフロー対策 (プログラム動作を安定化させるためのもの)
    return np.exp(x) / np.sum(np.exp(x)) # この一行がソフトマックス関数の本質的な実装
    # (1) はあえていうなら return
    # (2) は分子の np.exp(x) 
    # (3) は分母 の np.sum(np.exp(x))
```

```python
# クロスエントロピー (確認テスト:3-3)
def cross_entropy_error(d, y):
    if y.ndim == 1: # 次元を一つ増やして、この後の処理と整合性をとる
        d = d.reshape(1, d.size)
        y = y.reshape(1, y.size)
        
    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換
    if d.size == y.size:
        d = d.argmax(axis=1)
             
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size
    # (1) はあえていうなら return
    # (2) は -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) が本質的な部分
    #    y[np.arange(batch_size), d] は 少しトリッキーだが、
    #    この関数上で yは確率ベクトルであるが、dはワンホットエンコードされたものではなく、正解ラベルである前提で
    #    np.sum() は複数データに関する足し算であって、数式上の 和とはことなる
    # + 1e-7 は logの真数を確実に正の値とするための epsilon

```
### 4. 勾配降下法


  - 勾配降下法の考え方
    - 深層学習モデルを構築するための最適化手法として用いる
      - 学習 = 誤差関数 $E(\mathbf{w})$ を最小化するパラメータ $\mathbf{w}$ を見つける
      - 最適化手法には 通常 "勾配降下法" が用いられる
    - パラメータ更新式のイメージと学習率
      - (確認テスト:4-1) -> Section.5 勾配降下法の実装演習コード上にコメント付記
      - その時々のパラメータにおける勾配と逆方向にパラメータを更新する = 極小値に向かう変更
      - 学習率 $\epsilon$ はその歩幅を決める
        - 学習率が大きすぎると、最適解(=大域的極小値)を飛び越えて最悪発散する可能性がある
        - 学習率が小さすぎると、最適解にたどり着くまでの更新回数が増える(時間がかかる)
          - 小さすぎると局所解につかまるが、適切な量に設定すると飛び越えられるという説明の図(下図)は、不適切(もしくは言葉足らず)なように思う 
            - ![](/assets/2021-05-16_142224.jpg)
            - 説明では強引に最適解まで更新し続けるような矢印が書かれているが、、一回目の更新後の勾配は正値のためパラメータは局所解の方向に向かう。勾配の絶対値も小さいのでちょうど局所最適にハマるような絵になっている。
      - 勾配降下法ベースのパラメータ更新アルゴリズム (詳細はDay2)
        - Momentum, AdaGrad, Adadelta, Adam など。
        - Adam がよく使われる
  - (バッチ)勾配降下法
    - 与えられたデータ(全部)に対する出力からエラーを計算する
    - そこから得られた勾配を使って、パラメータ(重み $W$ と バイアス $b$)を更新
    - 更新されたパラメータを用いて、次の週(エポック)へ

 - 確率的勾配降下法 (SGD)
   - パラメータを更新する毎にランダムに抽出したサンプルの誤差を計算
     - 1サンプルで1回パラメータ更新
   - メリット
     - データが冗長な場合計算コストを軽減できる
     - パラメータ更新方向にランダム性をもたせることで、望まない局所解に収束するリスクを軽減する
     - オンライン学習ができる
       - (確認テスト:4-2) オンライン学習とは
         - 新しい学習データが入ってくるたびに都度パラメータを更新する学習方法。バッチ学習は一度にすべての学習データを使ってパラメータ更新を行う。
           - 大規模な機械学習・深層学習では計算機のRAM制約でバッチ学習を実施することが非現実になることがある


 - ミニバッチ勾配降下法 (深層学習の場合は基本的にこのアプローチをとる)
   - ランダムに分割したデータの集合(ミニバッチ) $D_t$ に属するサンプルの誤差を使って勾配を計算する方法
     - 例: 全データ10万 = 500データ x 2000 バッチ
     - (要確認): 結局、1エポックに1回更新するのか、1ミニバッチ毎に1回更新するのか? 
   - メリット
     - 確率的勾配降下法のメリットを維持しつつ、計算機の計算資源を有効利用できる
       - CPUを利用したスレッド並列化や、GPUを利用したSIMD並列化など
       - (要確認): 講師の説明では、複数のバッチを同時並行的に計算すると言っているが、私の理解では、バッチ内の複数データを並列計算するというのが、通常のミニバッチ勾配降下法の実装のされ方だと思っていた
         - 分散学習などするときには、各分散ノードで異なるミニバッチの処理を同時に行うことがあるのは知っているが、現時点でその話を想定して話をしていると考えるは無理がある.. 
     - (確認テスト:4-3) $w^{(t+1)}=w^{(t)}-\epsilon  \nabla E_t$ のイメージ図
       - ![パラメータ更新イメージ](/assets/210517_gradient.png)
       - ミニバッチ勾配降下法の動画の中で取り上げられた文脈なので、$t$ は ミニバッチのインデックスかと思っていたが、動画では エポックのインデックスだという。。理解が間違っているのか? その辺りはぼかした図にしておく.

#### 実装演習 (更新式定義)
厳密には、実装演習は無し? -> 誤差逆伝播法の実装演習と兼ねる。

### 5. 誤差逆伝播法

 - 誤差逆伝播法をなぜ使うか?
   - 勾配降下法を行うためには、各々のパラメータに関する誤差関数の微分値を求める必要がある
   - 数値微分では、演算負荷が大きい。
     - $\frac{\partial E}{\partial w_m} \simeq \frac{E(w_m+h) - E(w_m-h)}{2h}$
     - 各パラメータに対して、現在の値を中心に $\pm h$ ずらした誤差関数を計算する必要があり、大量パラメータの反復更新を膨大な回数行う深層学習では、効率が悪い
   - 誤差逆伝播法は、上記と比較して効率がよい

 - 誤差逆伝播法の方法
   - 合成関数の微分(連鎖律)を用いて、出力層側から順番に微分値を計算していく
     - コメント: 図を使った講師の表現が気になる..
        - 講師の表現 : "yはuになって, u は w になって" 
        - 自分的にしっくりくる表現: "yはuの関数で、uはwの関数で" -> 後に講師もこの表現で説明してた.安心。
     - 例: 3層のNNの中間層の重み $\bm{w}^{(2)}$ の 1要素 $w^{(2)}_{ji}$ に関する勾配を求めるための計算
       - 連鎖律: $\frac{\partial E}{\partial w^{(2)}_{ji}} = \frac{\partial E}{\partial \bm{y}} \frac{\partial \bm{y}}{\partial \bm{u}^{(2)}} \frac{\partial \bm{u}^{(2)}}{\partial w^{(2)}_{ji}}$
       - $ \frac{\partial E}{\partial y}  = \bm{y}-\bm{d}$ 
       - $ \frac{\partial \bm{y}}{\partial \bm{u}^{(2)}} = 1$
       - $\frac{\partial u^{(2)}}{\partial w^{(2)}_{ji}} = [0,\ldots,z_i,\ldots,0]^T$  
    - (確認テスト:5-1) -> 実装演習コードにコメント
    - (確認テスト:5-2) -> 実装演習コードにコメント (動画で説明された模範解答は間違っていると思う..)
#### 実装演習 (確率的勾配法のコードを用いて誤差逆伝播法を理解)

ちなみに、ファイルをダウンロードした状態では、
forward/backward で活性化関数の不一致があったため、reluに統一した
(sigmoidでも同様に収束していそうなことは確認)

```python
import sys
sys.path.append('.')

import numpy as np
from common import functions
import matplotlib.pyplot as plt

def print_vec(text, vec):
    print("*** " + text + " ***")
    print(vec)
    #print("shape: " + str(x.shape))
    print("")
```
確率勾配降下法 
```python
# サンプルとする関数
#yの値を予想するAI

def f(x):
    y = 3 * x[0] + 2 * x[1]
    return y

# 初期設定
def init_network():
    # print("##### ネットワークの初期化 #####")
    network = {}
    nodesNum = 10
    network['W1'] = np.random.randn(2, nodesNum)
    network['W2'] = np.random.randn(nodesNum)
    network['b1'] = np.random.randn(nodesNum)
    network['b2'] = np.random.randn()

    # print_vec("重み1", network['W1'])
    # print_vec("重み2", network['W2'])
    # print_vec("バイアス1", network['b1'])
    # print_vec("バイアス2", network['b2'])

    return network

# 順伝播
def forward(network, x):
    # print("##### 順伝播開始 #####")
    
    W1, W2 = network['W1'], network['W2']
    b1, b2 = network['b1'], network['b2']
    u1 = np.dot(x, W1) + b1
    z1 = functions.relu(u1)    
    #z1 = functions.sigmoid(u1)
    
    u2 = np.dot(z1, W2) + b2
    y = u2

    return z1, y

# 誤差逆伝播
def backward(x, d, z1, y):
    # print("\n##### 誤差逆伝播開始 #####")    

    grad = {}
    
    W1, W2 = network['W1'], network['W2']
    b1, b2 = network['b1'], network['b2']
    # (確認テスト:5-1) 微分値再利用 & (確認テスト:5-2) 各微分値の計算 -------------ここから
    # 出力層でのデルタ
    #  dE/dy = dE/dy ・ dy/du2  の計算に該当. (yとu2が恒等写像なので) 
    #  これを保存して 後に再利用していることで演算効率をあげている
    delta2 = functions.d_mean_squared_error(d, y)

    # b2の勾配
    grad['b2'] = np.sum(delta2, axis=0) # delta2 の再利用 
    # W2の勾配
    grad['W2'] = np.dot(z1.T, delta2) # delta 2 の再利用, dE/dw2 = dE/dy ・ dy/du2 ・ du2/dw2

    # 中間層でのデルタ
    # dE/du1 = dE/dy・dy/du2・ du2/dz ・dz/du1 の保存
    # これを保存して 後に再利用していることで演算効率をあげている
    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)   
    #delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1) 

    delta1 = delta1[np.newaxis, :]
    # b1の勾配
    grad['b1'] = np.sum(delta1, axis=0) # delta1 の再利用
    x = x[np.newaxis, :]
    # W1の勾配
    grad['W1'] = np.dot(x.T, delta1) # delta1 の最入用
    # # (確認テスト:5-1) 微分値再利用 & (確認テスト:5-2) 各微分値の計算　該当箇所 ----- ここまで

    return grad

# サンプルデータを作成
data_sets_size = 100000
data_sets = [0 for i in range(data_sets_size)]

for i in range(data_sets_size):
    data_sets[i] = {}
    # ランダムな値を設定
    data_sets[i]['x'] = np.random.rand(2)
    
    ## 試してみよう_入力値の設定
    # data_sets[i]['x'] = np.random.rand(2) * 10 -5 # -5〜5のランダム数値
    
    # 目標出力を設定
    data_sets[i]['d'] = f(data_sets[i]['x'])
    
losses = []
# 学習率
learning_rate = 0.07

# 抽出数
epoch = 1000

# パラメータの初期化
network = init_network()
# データのランダム抽出
random_datasets = np.random.choice(data_sets, epoch)

# 勾配降下の繰り返し
for dataset in random_datasets:
    x, d = dataset['x'], dataset['d']
    z1, y = forward(network, x)
    grad = backward(x, d, z1, y)
    # パラメータに勾配適用
    # (確認テスト:4-1) 勾配降下法の更新式に対応するコード ここから
    for key in ('W1', 'W2', 'b1', 'b2'):
        network[key]  -= learning_rate * grad[key] 
    # (確認テスト:4-1) 勾配降下法の更新式に対応するコード ここまで

    # 誤差
    loss = functions.mean_squared_error(d, y)
    losses.append(loss)

print("##### 結果表示 #####")    
lists = range(epoch)

plt.plot(lists, losses) # 散布図より直線のほうが見やすいので変更
plt.show()
```

    ##### 結果表示 #####
![svg](/assets/1_3_stochastic_gradient_descent_2_1.svg)


### Tips：ディープラーニングの開発環境

「その前に開発環境を紹介しておきたいとおもいます」
という一言から録画が始まってるのだが、「その前に」って何の前だろう..

 - ローカルPC と クラウド環境
   - ローカルPC = 家や会社に置いてあるPC
   - クラウド = データセンター(に置いてあるPC)を間借り
     - 例: AWS, GCP
 - プロセッサ
   - 各種プロセッサ (一般的には下に行くほど早いが、用意するのにお金いる)
     - CPU : 汎用演算,  高クロックだが昨今頭打ち(数GHz), 並列度低め(数十コアくらいまで)
     - GPU : ゲーム(グラフィック)特化の演算がDLに流用可能、クロックはCPUよりは低いが、並列度が高い分演算が早い
     - FPGA : 論理回路をプログラム可能 (自分で適切な演算器を設計できる) なため、高速な演算ユニットをつくることが可能
     - ASIC : Google の TPUなどが具体例。完全ハードウェアロジック化したもので、FPGAよりさらに早い

### Tips: その他の一般的な機械学習の手法について

 - データ拡張(Data Augmentation)
    - データ拡張の適用先
      - 分類タスクに有効
        - 様々なバリエーションに対応する必要がある
        - 新しいデータを作ることが容易(例:回転させても同じ分類をしてほしいなど)
      - それ以外のタスクには向かないこともある
        - データの密度分布を推定する問題には使えない
          - 真の分布を知らずに、真の分布の推定の助けになるデータをでっち上げるのは不可能
    - データ拡張の方法
        - 入力データ改変: 問題の性質を考慮して行う
          - オフセット, 回転など
        - モデル内部での対応: 一般的に適用可能な方法を用いる
          - ドロップアウト
          - 入力層・中間層へのノイズ注入 (バリエーションの増加)  
    - 効果
      - 劇的に汎化性能が向上することがある
      - ランダム性のあるデータ拡張を行うときは再現性に注意

 - 転移学習
   - 学習済みモデルをベースに、タスク固有の処理に対応する"一部の層のみ"を再学習する手法
     - cf: ファインチューニング: 全パラメータを再学習する点が違う
     - 深層学習モデルでは 前半部が汎用的な特徴抽出部、後半部がタスク固有処理部である、というような理解に基づく

## Day 2 レポート

- 前半: 中間層をある程度増やした大きいネットワークを学習するときによく起こる問題
- 後半: 畳み込みニューラルネットワーク

### 1. 勾配消失問題
 
 - 誤差逆伝播法の復習
   - 割愛.
   - (確認テスト:1-1) 連鎖律を使ってdz/dx を求めよ。ただし $ z = t^2, t = x+y$
     - $ \frac{d z}{dx} = \frac{d z}{dt} \frac{dt}{dx} = 2t \dot 1 = 2(x+y) $ 
 - 勾配消失問題
   - 誤差逆伝播法で出力から入力に向かって計算をすすめていくにつれて勾配がどんどん緩やかになっていくことで、入力層に近い側の層のパラメータがほとんど変わらず、最適値に収束しなくなる
     - 微分値は 大きさ0~1の間をとるものが多い
       - (確認テスト:1-2) sigmoid 関数の 傾きは 0~0.25 (答えは (2) 0.25 )
         - sigmoid 関数 $\sigma$ の微分 $ = (1-\sigma) \sigma $ 
       - 中間層で sigmoid を使うと、1層遡るたびに 0.25よりも小さい値が掛け算されていく
 - 勾配消失問題の対策方法
   1. 活性化関数の選択
     - 勾配消失が起きづらい(微分値が小さくならない) 活性化関数を使う
     - 例: ReLU関数 $ f(x) = \mathrm{max}(0,x) $
      - 0 より大きいときは、微分値1  -> 勾配消失問題を回避
      - 0 より小さいときは、微分値0にする -> このノードを経由しての誤差は伝播しない -> スパース化
   2. 重みの初期化方法の工夫
     - 重みは適切に設定する必要がある
       - (確認テスト:1-3) たとえばすべての重みを0に設定すると、正しい学習が行えない
         - すべての重みの値が均一に更新され、多数の重みを有効に働くモデルが学習できない。(モデルが持つ自由度を発揮でない)
     - Xavier (ザビエル) の初期値設定法
       - sigmoid 等 S字カーブ系の活性化関数を使う時に有効
       - 標準正規分布(平均が0, 分散が1 )に従った乱数 を 前のレイヤーのノード数のルートで割る
         - 補足資料: 5つの中間層を持つNN の各層の出力のヒストグラムの観察
           - 標準正規分布で初期化した場合
             - 各層の出力のヒストグラムを見ると、0付近 と1付近に集中する -> sigmoid 使った場合勾配が非常に小さくなるので、勾配消失問題が起きやすい状態
           - 標準正規分布を小さな値で割った値(標準偏差を小さくする)で初期化した場合
             - 各中間層の出力は 0.5 付近に集中する -> 何を入れても出力が似通っている -> モデルが仕事してない状態
           - Xavier の初期化を使用した場合
             - 各層の出力は、適度に0から1の間でバラツキがあっていい感じの出力が得られていそう
     - Heの方法
       - ReLU 関数に対して使う
       - 正規分布を $\sqrt{\frac{2}{N}}$ 
         - 補足資料: 各層の出力
           - 標準偏差が1のとき -> ほとんど出力が0による
           - 標準偏差をもっと小さくした時 -> やはり出力が0による
           - He の初期化 -> いい感じ
   3. バッチ正規化 (batch normalization)
      - ミニバッチの単位で入力データの偏りを抑制する
        - バッチ内データを平均0分散1に正規化する($\hat{x}$とする)
        - $y=\gamma \hat{x} + \beta$ ($\gamma, \beta$ スケール・シフトの微調整を行うパラメータとして学習される)
      - (確認テスト:1-4) 期待される効用
        - 収束を早める
        - 過学習が起きづらい
      - 参考: バッチサイズ
        - 画像系のタスクのとき、GPUなら1~64枚, TPUなら1~256枚程度
        - 2のべき乗をよく使うことが多い

#### 実装演習

##### p.33 例題チャレンジ
 - Q. 特徴データ data_x, ラベルデータ data_t に対してミニバッチを学習を行うコード例 (き) に当てはまるのは?
 - A.  (1) ``` data_x[i:i_end], data_t[i:i_end] ``` 

 - こういうプログラムの虫食い問題はE資格でも結構あるらしい

##### 初期化による勾配消失問題の改善 
\textinput{2_2_2_vanishing_gradient_modified.md}

 - とにかく ReLU の効果が大きい
 - ReLu + Xavier より 気持ち ReLU + He のほうが早い? 

##### バッチ正規化
\textinput{2_3_batch_normalization.md}

 - バッチ正則化の効果で sigmoid + gauss 初期化でもなんとか更新は進んでいるようす (でもReLUにした方が効果大きい)
 - この実験のみで判断できないが、He + ReLU などで適切・十分に収束していそうなケースでは、BN入れない方が早いのかも?


### 2. 学習率最適化手法

 - Optimizer 動作概要
   - 学習の初期の段階では、学習率大きめに設定し、徐々に学習率を小さくしていく
   - パラメータ毎に学習率を可変にする
     - パラメータ毎に学習率が変わるということは、パラメータ更新のベクトルの方向自体が変わるということ. "学習率最適化"という言葉はしっくりこないなぁ..
 - 代表的な手法
    - モメンタム
      - 更新式
        - $ \bm{w}^{(t+1)} = \bm{w}^{(t)} + V_t $
          - パラメータ更新量は $V_t$
        - $ V_t = \mu V_{t-1} - \epsilon \nabla E$
          - $V_t$ は、勾配降下法のパラメータ更新量 $-\epsilon \nabla E$ を1次IIRをつかって Recursive average をとったもの
      - 効用
        - ささいな局所最適解につかまりにくい
        - 谷間についてから、最低値付近に収束するまでの時間が早い (しかし最終的にピタッととまりにくい)
    - AdaGrad
      - 更新式
        - $ \bm{w}^{(t+1)} = \bm{w}^{(t)} - \epsilon \frac{1}{\sqrt{h_t} + \theta }\nabla E $
          - $h_t$ が大きいほど遅くなるよね
        - $ h_t = h_{t-1} + (\nabla E)^2 , h_0 = \theta $
          - 最初の方は、$|\nabla E| $ で ノーマライズするような効果なので、斜面の緩急によらず速やかに収束していきそう
          - 毎回二乗で足していくので、$h_t$ は単調増加して、どんどんステップサイズは小さくなる。
      - 効用
        - 勾配の緩やかな斜面で最適値に近づいていきやすい(逆に急な斜面は不得意)
        - 鞍点問題 (SGDよりはマシなようだが..)
        - ハイパパラメータの調整が難しい
    - RMSProp
      - 鞍点問題対策を施したAdaGradの改良版
      - 更新式
        - $ \bm{w}^{(t+1)} = \bm{w}^{(t)} - \epsilon \frac{1}{\sqrt{h_t} + \theta }\nabla E $
          - AdaGrad と同じ
        - $ h_t = \alpha h_{t-1} + (1-\alpha) (\nabla E)^2$
          - $h_0$ の設定は AdaGrad と同じなのだろうか? 
          - AdaGradで 単調増加していた $h_t$ が、単純に一個前との按分をとる形になった
      - 効用
        - AdaGradと比較して より帯域的最適解を見つけやすく、ハイパラメータの調整の難易度も下がった
    - Adam 
      - モメンタムと、RMSProp のいいとこ取り版
      - 更新式 (なぜ資料にのっていないのか?!) [参考](https://qiita.com/omiita/items/1735c1d048fe5f611f80)
          - $ \bm{w}^{(t+1)} = \bm{w}^{(t)} - \epsilon \frac{V_t}{\sqrt{h_t} + \theta} $
          - $ V_t = \mu V_{t-1} + (1-\mu) \nabla E$
          - $ h_t = \alpha h_{t-1} + (1-\alpha) (\nabla E)^2$
#### 実装演習

 - Momentum をもとに AdaGrad の実装を行う
   - 講師の方、Momentum のコードが書かれているのに、AdaGrad の解説してて、コード見てないのがモロバレですよ。
 - Try パート
   - 学習率増やしたら、Momenutum, AdaGrad で収束するようになったが、RMSProp, AdamがNGに,,
   - Relu, He にすると各手法よくなる
   - Batch Normalization を入れても良くなるが 活性化関数/初期化手法の方がより安定的に収束してそう?
 
\textinput{2_4_optimizer.md}
 
### 3. 過学習
 - Day1 の 13_過学習 の動画の内容
   - 機械学習の項でやった内容なので、詳細割愛
   - 過学習はパラメータ数の大きい巨大なモデルだと容易に起こる
   - 
### 4. 畳み込みニューラルネットワークの概念

 - Day1 "15_CNNで扱えるデータの種類" 動画の内容
   - CNNで扱うと有効なデータ = 次元間でのつながりに意味があるデータ
     - 例: 
       - 単一チャンネル 音声(1次元)、スペクトログラム画像(2次元),　CTスキャン画像(3次元)
       - 複数チャンネル アニメのスケルトン(1次元), カラー画像(2次元),  動画(3次元) 

### 5. 最新のCNN



## 取り組み概要 & 感想

[過去の記事のまま]

### 取り組みの記録

今回は、実装演習にじっくり時間をかける感じでもなかったので、
動画みながら、少しずつ

- 5/7 : Day1 Section.1 までの動画、コードをざっと眺めて、レポートの要件について問い合わせ。1h
  - レポート提出方法の"実装演習"というのが何を指しているのか不明確であったため確認。
  - 事務局回答を踏まえて、本ページ冒頭のようなまとめ方をすることにした。
　- Stage.1,2 動画は基本座学だった(コードが動画でとりあげられなかった)ため、一通り動画を見てからまとめを行ったが、Stage.3 では動画講義中にガンガンコードを見る・動かす必要がありそうなので、動画を見ながら少しずつレポートをまとめるスタイルですすめることにした。

- 5/13: プロローグ を動画みながらざっくりまとめ  (0.5h)
- 5/14: Day1 Section 0, 1, 2 を動画みながらざっくりまとめ (1.5h)
- 5/15: Day1 Section 3 (1h)
- 5/16: Day1 Section 4 (1h)
- 5/17: Day1 Section 5 (1h)
- 5/20: Day1 のこり (1hくらい)
- 5/22: Day2 Section 1 (2h)
- 5/23: Day2 Section 
- 5/18: Section 6, 7, 8, 9
- 5/18: Section 10, テスト
 
### 感想ほか　

時々思うが、資料にしろ喋りにしろすこし言葉の使い方に違和感を感じる。

重み付き和のことを混ぜ合わせる=シャッフルと表現していたが、これ正しいノア?

無理やり直感的な説明をしようとするあまり逆によくわからない説明になる
 順を追って色々な用語を学習しているのだから、無理に専門用語をさけなくていいのに


- 講師、全然コードみてないよね。。二乗を使っていました、とモメンタムのコード見ていってるもん..

----

講義動画について。旧動画と新動画でけっこう趣が違う。時間に余裕があれば、両方見てもよいのではなかろうか。
 - 新動画
   - 「基本きっちり押さえれば、後の細かい所は頑張れるでしょ」的な解説。講師の男性はもともとは研究寄りの人な印象を受ける
   - 手計算してるところをみせてくれる (ロジスティック回帰の勾配計算とかやったことない人はやりましょう)
   - 実戦的な注意点などは、参考情報が旧動画より多め
   - その他資料に載っていない背景説明などもある。結果的に時間の都合で資料の説明を一部すっとばすことがある
   - 「お前らコレ難しいからわかってねーだろ」的な発言がチラホラ。人によってはイラッとするかも?
  
 - 旧動画のいいところ
   - 基本的に資料に沿った説明をたんたんとしてくれる感じで、 講師の女性はどちらかというとソフトウェア開発寄の人な印象
   - ハンズオンの説明は こちらの方が丁寧


ハンズオンは基本的に、資料/動画 で学習した手法を使ったデータ分析で、自分で編集しなくてもそのまま実行できてしまう。
コードから学ぶことに慣れている人は、買ってに勘所を見つけて得るべきものを得るが、
人によっては、「へーこんな結果になるんだぁ」で終わってしまうんでなかろうか? それだとちょっともったいない。
例えば、大事なところを数カ所穴埋めにして、考えるきっかけにしてもらう、などやっても良い気がした。


テストについての感想。(他の受講生もあとから受ける可能性があるため具体的な内容には触れない)
15問中13問はビデオ講義のどこかででやった内容そのままだった。
単純なミスや早とちりこそあれ、これらの問題に自信を持って回答できないのであれば、しっかり復習をした方がよさそうだ。
残り2問は、ラビットチャレンジで初めて勉強を始めた人にとっては、未知の手法が出題されたように感じるかもしれない。
しかし、この2問のうち先に出題される問題が、その未知の手法自体を問題文で説明してくれているので、問題文の意味をしっかり捉えれば正解は導ける。
また、それができれば、芋づる式に残り1問も計算問題として解けるので、難しすぎるとか、講義内容とのミスマッチなどを感じることは個人的にはなかった。

## 計画の見直し (2021/05/06 時点)

[過去の記事のまま]

4/18に終わらせるはずだったステージ2が半月遅れてしまった。
実際に学習に費やしている時間はそこまで長くないが、取り組む時間を確保するのが難しい。

これまで ステージテストはスムーズに合格しているので、各ステージうまく理解ができていると思いたい。
この調子ですすめれば、修了テストにかかる時間は少し縮めてもよいだろう。スケジュールを下記の様に修正する。

 - ~2021/2/15  : スタートテスト (2021/02/07完了)
 - ~2021/3/30  : ステージ1      (2021/03/30完了)
 - ~2021/5/6   : ステージ2      (2021/05/06完了)
 - ~2021/5/30  : ステージ3 
 - ~2021/6/27  : ステージ4 
 - ~2021/7/4   : 復習 -> 修了テスト 
 - ~2021/7/15  : Eもぎライト -> 今後の計画具体化 
 - ~2021/7/30  : シラバスの未習箇所の学習 
 - ~2021/8/26  : 全体の復習
 - 2021/8/27,28: E資格 受験 

だんだん 8月に万全を期しての受験が厳しくなってきた気もするが、、淡々と進めていこう。