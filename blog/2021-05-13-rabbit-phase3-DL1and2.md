@def title = "ラビットチャレンジ: Stage.3 深層学習 Day1,2"
@def author = "kyokke" 
@def tags = [ "Deep-Learning", "Rabbit-Challenge" ]


# ラビットチャレンジ: Stage.3 Day1, 2  

本ページはラビットチャレンジの、
Stage.3 "深層学習 Day1,2" のレポート提出を兼ねた受講記録です。
提出指示を満たすように、下記の方針でまとめました。(事務局にも問い合わせて問題ないことを確認した)

1. 動画講義の要点まとめ
   - 自分が講義の流れを思い出せるようなメモを残す。通常であれば要点として記載すべき図・数式などがあっても、それが自分にとって既知であれば、言葉の説明ですませることもある
2. 実装演習
   - 各Sectionで取り上げられた .ipynb or .py ファイルの内容を実行した結果を記載
   - ただし、初学者向けにやや冗長な内容がある場合、抜粋することもある
3. 確認テスト
   - 確認テストの解答に相当する内容は、個別の節をもうけず、要点まとめに含めたり、コードに対するコメントとして記載する
     - 確認テストは重要事項だから、出題されているのであって、まとめ/演習と内容がかぶるはず。
     - 事務局がレポートチェックをする時のために、(確認テスト:1-2) のようなタグを付す。１つ目の数字が section番号、２つ目の数字が 「何番目のテストか?」
4. 1~3 をまとめる上で思うことがあれば、考察やコメントも適宜残しておく。


## 目次
\toc

## レポート
### プロローグ　

 - 機械学習/DLモデルが解こうとしている問題
    - 識別 (discriminateve, backward)
      - データ $\bm{x}$ -> クラス $C_k$ 
        - $ p (C_k | \bm{x}) $ を計算する (確率最大となる $C_k$ を分類結果にする)
        - 例: 画像認識 ( 犬やネコの画像データを入力すると、犬なのか、ネコなのかを出力する) 
      - 高次元 -> 低次元
        - 学習データは比較的少なめ
      - モデルの例
        - 決定木, ロジスティック回帰, SVM, NN
      - 開発のアプローチ (Phase.2でやった話)
        - 生成的アプローチ 
          - $p(\bm{x} | C_k) , p(C_k)$ をモデル化・推定し、 ベイズの定理から $p(C_k| \bm{x}) $ を計算する
            - データの分布 $p(\bm{x}|C_k)$ は分類結果よりも複雑なことがある。
              - その分学習コスト大。
              - 単純に分類結果を得たいだけならば、識別的なアプローチを用いる。
              - 副産物として得られた生成モデルを活用したい場合は良い。
          - 確率的な識別ができる。(識別的アプローチと同様)
 - 識別的アプローチ
    - 直接　 $p(C_k| \bm{x}) $ をモデル化・推定する
    - 確率的な識別
      - 機械学習のモデルの出力は 事後確率 $p(C_k|\bm{x})$ であり、そこからどのように識別結果を得るかは、開発者の裁量。
      - 自信のある無しの度合いを測ることができる。
    - 学習コスト中
    - 確率的識別モデル
 - 識別関数
    - 入力 $ \bm{x} $ から 出力のクラスへの写像 $f(\bm{x})$ を直接推定
    - 学習データ・コストが少ない。決定的な識別
    - 決定的識別モデル
 - 生成 (generative, forward)
    - クラス $C_k$ ->  データ $\bm{x}$
      - $ p (\bm{x}|C_k ) $ の分布情報を持った上で、サンプリングする。
        - データの分布は複雑になりがち。実際に
      - 例: 犬の画像くれ、といったら 犬の画像っぽいものを出力してくれる, テキスト生成、画像の超解像など
    - 低次元 → 高次元
      - 学習データは大量に必要
    - モデルの例
      - HMM, ベイジアンネットワーク, VAE, GAN

 - 万能近似定理
   - 非線形の活性化関数を持つNeuralNetworkは任意の関数を近似できる

### 0. ニューラルネットワークの全体像
 
 - 入力層、中間層、出力層で構成される
   - 識別モデルの例では、入力層のノードにあたるのはデータ、出力層のノードはそれぞれのクラスに属する確率。 
   - (確認テスト:0-1) 
     - ディープラーニングがやろとしていること
       - 入力されたデータを変換して所望の出力に変換する数学モデルをつくる
       - ここで使用されるのはパラメータを持つ中間層を複数使用して高い自由度が得られるようなモデル
     - 学習によって最適化するのは
       - パラメータ　(3. 重み と 4. バイアス)
 - 数式も用いて説明
   - (確認テスト:0-2) 入力層2ノード, 中間層3ノード 出力層1ノード のネットワーク
~~~
<figure style="text-align:center;">
<img src="/assets/20210514-nn.svg" style="padding:0;width:150%;" alt="#1"/>
<figcaption></figcaption>
</figure>
~~~

 - ニューラルネットワークでできること
   - 回帰 : (主に) 連続的な実数値を取る関数の近似
     - 例
       - 結果の予想 (売上、株価など)
       - ランキング (競馬順位, 人気順位) 
     - NN以外の手法
       - 線形回帰, ランダムフォレスト、回帰木
   - 分類 : 数値の大小関係に意味のない、離散的な結果を予想するための分析
     - 例
       - ネコ写真判別
       - 手書き文字認識
       - 花の種類分類
     - NN以外の手法
       - ベイズ分類, ロジスティック回帰, 決定木, ランダムフォレスト
 - 深層学習の実用例
   - 深層学習モデルはとても表現力が高いため、入力と出力を数値・ベクトルに変換してしまいさえすれば、あらゆる問題に応用できる可能性がある
   - 例えば、自動売買、チャットボット、翻訳、音声解釈、囲碁/将棋AI など

### 1. 入力層-中間層

  - (確認テスト:1-1) 入力層~中間層の図に Section.0で取り上げた動物分類の実例を書き入れてみると以下のようになる 
    -  ![確認テスト:1-1](/assets/確認テスト1-1.png)
    - 愚痴コメント: "この図式に"実例を入れると書いてあるのだから、入力4次元のNNに合わせて実例の方を変えるべきですよね..  (模範解答のような回答を期待しているのならば、「実例の図から、この図式のような形で入力層~中間層までに相当する部分を抜き出せ」というような問題文になるはず)
   - $u=\sum_{i=1}^4 w_i x_i + b$ の変換を行い、それが次の中間層の入力となり、活性化関数がかかって中間層の出力となる
     - 学習に置いては、この $w_i, b$ が最適化される
     - 各計算式に対応する箇所は、実装演習のコードにコメントした。


#### 実装演習抜粋 ( 順伝播（3層・複数ユニット）)

数式では、$Wx+b$ なのに python では np.dot(x,W) となる?
np.dot() 関数の挙動をしっかり抑えておく必要がある。

  - 基本の動作は行列積(のようなもの)
    - 要素数が一致する次元: a の最後の axis と b の 最後から2番目のaxis 
    - 通常の行列積も包含
  - 下記が適応不可能な自体では、よしなにそれっぽい計算が行われる
    - 例:
      - if a,bの両方orどちらかがスカラー: 普通の積. (スカラーをもう一方の全要素にかける) 
      - elif b がベクトル: a の最後の axis と b の長さが一致
        - ベクトルの内積のケースも包含



```python
# 順伝播（3層・複数ユニット）

# ウェイトとバイアスを設定
# ネートワークを作成
def init_network():
    print("##### ネットワークの初期化 #####")
    network = {}
    
    # 試してみよう
    #_ネットワークの初期値ランダム生成
    network['W1'] = np.random.rand(2,3)
    network['W2'] = np.random.rand(3,2)
    network['W3'] = np.random.rand(2,2)
    network['b1'] = np.random.rand(3)
    network['b2'] = np.random.rand(2)
    network['b3'] = np.random.rand(2)

    # 試してみよう
    #_各パラメータのshapeを表示
    print("*各パラメータのサイズ情報")
    for k,v in network.items():
        print("network[%s].shape=" % k, end='')
        print(v.shape)
    print()

    # 変更前
    # network['W1'] = np.array([
    #     [0.1, 0.3, 0.5],
    #     [0.2, 0.4, 0.6]
    # ])
    # network['W2'] = np.array([
    #     [0.1, 0.4],
    #     [0.2, 0.5],
    #     [0.3, 0.6]
    # ])
    # network['W3'] = np.array([
    #     [0.1, 0.3],
    #     [0.2, 0.4]
    # ])
    # network['b1'] = np.array([0.1, 0.2, 0.3])
    # network['b2'] = np.array([0.1, 0.2])
    # network['b3'] = np.array([1, 2])

    print_vec("重み1", network['W1'] )
    print_vec("重み2", network['W2'] )
    print_vec("重み3", network['W3'] )
    print_vec("バイアス1", network['b1'] )
    print_vec("バイアス2", network['b2'] )
    print_vec("バイアス3", network['b3'] )

    

    return network

# プロセスを作成
# x：入力値
def forward(network, x):
    
    print("##### 順伝播開始 #####")

    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    # 1層の総入力
    u1 = np.dot(x, W1) + b1  # (確認テスト:1-2) 入力層の計算に該当
    
    # 1層の総出力
    z1 = functions.relu(u1) 
    
    # 2層の総入力
    u2 = np.dot(z1, W2) + b2
    
    # 2層の総出力
    z2 = functions.relu(u2) # (確認テスト:1-3) 中間層の出力に該当

    # 出力層の総入力
    u3 = np.dot(z2, W3) + b3
    
    # 出力層の総出力
    y = u3
    
    print_vec("総入力1", u1)
    print_vec("中間層出力1", z1)
    print_vec("総入力2", u2)
    print_vec("出力1", z1)
    print("出力合計: " + str(np.sum(z1)))

    return y, z1, z2

# 入力値
x = np.array([1., 2.])
print_vec("入力", x)

# ネットワークの初期化
network =  init_network()

y, z1, z2 = forward(network, x)
```

```
    *** 入力 ***
    [1. 2.]
    
    ##### ネットワークの初期化 #####
    *各パラメータのサイズ情報
    network[W1].shape=(2, 3)
    network[W2].shape=(3, 2)
    network[W3].shape=(2, 2)
    network[b1].shape=(3,)
    network[b2].shape=(2,)
    network[b3].shape=(2,)
    
    *** 重み1 ***
    [[0.46325703 0.4515028  0.9015613 ]
     [0.63300315 0.09960203 0.35035177]]
    
    *** 重み2 ***
    [[0.85561969 0.64270626]
     [0.71017196 0.35407221]
     [0.21520079 0.12526929]]
    
    *** 重み3 ***
    [[0.04970514 0.3372038 ]
     [0.80197002 0.34030274]]
    
    *** バイアス1 ***
    [0.14282813 0.02490021 0.42369957]
    
    *** バイアス2 ***
    [0.16724682 0.77646073]
    
    *** バイアス3 ***
    [0.97460445 0.08008213]
    
    ##### 順伝播開始 #####
    *** 総入力1 ***
    [1.87209147 0.67560708 2.02596442]
    
    *** 中間層出力1 ***
    [1.87209147 0.67560708 2.02596442]
    
    *** 総入力2 ***
    [2.6848315  2.47267045]
    
    *** 出力1 ***
    [1.87209147 0.67560708 2.02596442]
    
    出力合計: 4.573662972619907
```

### 2. 活性化関数

### 3. 出力層

### 4. 勾配降下法

### 5. 誤差逆伝播法


## 取り組み概要 & 感想

[過去の記事のまま]

### 取り組みの記録

今回は、実装演習にじっくり時間をかける感じでもなかったので、
動画みながら、少しずつ

- 5/7 : Day1 Section.1 までの動画、コードをざっと眺めて、レポートの要件について問い合わせ。1h
  - レポート提出方法の"実装演習"というのが何を指しているのか不明確であったため確認。
  - 事務局回答を踏まえて、本ページ冒頭のようなまとめ方をすることにした。
　- Stage.1,2 動画は基本座学だった(コードが動画でとりあげられなかった)ため、一通り動画を見てからまとめを行ったが、Stage.3 では動画講義中にガンガンコードを見る・動かす必要がありそうなので、動画を見ながら少しずつレポートをまとめるスタイルですすめることにした。

Stage.3 では、

- 5/13: プロローグ を動画みながらざっくりまとめ  (0.5h)
- 5/14: Section 0, 1 を動画みながらざっくりまとめ (1h)

予定
- 5/15: Section 2, 3
- 5/16: Section 4, 5 
- 5/17: Section 6, 7
- 5/18: Section 8, 9
- 5/18: Section 10, テスト
 
### 感想ほか　

時々思うが、資料にしろ喋りにしろすこし言葉の使い方に違和感を感じる。

重み付き和のことを混ぜ合わせる=シャッフルと表現していたが、これ正しいノア?

----

講義動画について。旧動画と新動画でけっこう趣が違う。時間に余裕があれば、両方見てもよいのではなかろうか。
 - 新動画
   - 「基本きっちり押さえれば、後の細かい所は頑張れるでしょ」的な解説。講師の男性はもともとは研究寄りの人な印象を受ける
   - 手計算してるところをみせてくれる (ロジスティック回帰の勾配計算とかやったことない人はやりましょう)
   - 実戦的な注意点などは、参考情報が旧動画より多め
   - その他資料に載っていない背景説明などもある。結果的に時間の都合で資料の説明を一部すっとばすことがある
   - 「お前らコレ難しいからわかってねーだろ」的な発言がチラホラ。人によってはイラッとするかも?
  
 - 旧動画のいいところ
   - 基本的に資料に沿った説明をたんたんとしてくれる感じで、 講師の女性はどちらかというとソフトウェア開発寄の人な印象
   - ハンズオンの説明は こちらの方が丁寧


ハンズオンは基本的に、資料/動画 で学習した手法を使ったデータ分析で、自分で編集しなくてもそのまま実行できてしまう。
コードから学ぶことに慣れている人は、買ってに勘所を見つけて得るべきものを得るが、
人によっては、「へーこんな結果になるんだぁ」で終わってしまうんでなかろうか? それだとちょっともったいない。
例えば、大事なところを数カ所穴埋めにして、考えるきっかけにしてもらう、などやっても良い気がした。


テストについての感想。(他の受講生もあとから受ける可能性があるため具体的な内容には触れない)
15問中13問はビデオ講義のどこかででやった内容そのままだった。
単純なミスや早とちりこそあれ、これらの問題に自信を持って回答できないのであれば、しっかり復習をした方がよさそうだ。
残り2問は、ラビットチャレンジで初めて勉強を始めた人にとっては、未知の手法が出題されたように感じるかもしれない。
しかし、この2問のうち先に出題される問題が、その未知の手法自体を問題文で説明してくれているので、問題文の意味をしっかり捉えれば正解は導ける。
また、それができれば、芋づる式に残り1問も計算問題として解けるので、難しすぎるとか、講義内容とのミスマッチなどを感じることは個人的にはなかった。

## 計画の見直し (2021/05/06 時点)

[過去の記事のまま]

4/18に終わらせるはずだったステージ2が半月遅れてしまった。
実際に学習に費やしている時間はそこまで長くないが、取り組む時間を確保するのが難しい。

これまで ステージテストはスムーズに合格しているので、各ステージうまく理解ができていると思いたい。
この調子ですすめれば、修了テストにかかる時間は少し縮めてもよいだろう。スケジュールを下記の様に修正する。

 - ~2021/2/15  : スタートテスト (2021/02/07完了)
 - ~2021/3/30  : ステージ1      (2021/03/30完了)
 - ~2021/5/6   : ステージ2      (2021/05/06完了)
 - ~2021/5/30  : ステージ3 
 - ~2021/6/27  : ステージ4 
 - ~2021/7/4   : 復習 -> 修了テスト 
 - ~2021/7/15  : Eもぎライト -> 今後の計画具体化 
 - ~2021/7/30  : シラバスの未習箇所の学習 
 - ~2021/8/26  : 全体の復習
 - 2021/8/27,28: E資格 受験 

だんだん 8月に万全を期しての受験が厳しくなってきた気もするが、、淡々と進めていこう。