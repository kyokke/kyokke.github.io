<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/poole_hyde.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <title>ラビットチャレンジ: Stage.3 深層学習 Day1,2</title> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <h1><a href="/">kyokke's blog</a></h1> <p class=lead > julialang や　機械学習関連の作業/Tips 記録など</p> </div> <nav class=sidebar-nav > <!-- <a class="sidebar-nav-item {{ispage index.html}}active{{end}}" href="/">Home</a> <a class="sidebar-nav-item {{ispage /blog/*}}active{{end}}" href="/blog/">Blog</a> --> <!-- <a class="sidebar-nav-item {{ispage menu1/*}}active{{end}}" href="/menu1/">Code blocks</a> <a class="sidebar-nav-item {{ispage menu2/*}}active{{end}}" href="/menu2/">More goodies</a> <a class="sidebar-nav-item {{ispage menu3/*}}active{{end}}" href="/menu3/">Tags</a> --> </nav> <p>&copy; kyokke.</p> </div> </div> <div class="content container"> <div class=franklin-content ><h1 id="ラビットチャレンジ_stage3_day1_2"><a href="#ラビットチャレンジ_stage3_day1_2" class=header-anchor >ラビットチャレンジ: Stage.3 Day1, 2 </a></h1> <p>本ページはラビットチャレンジの、 Stage.3 &quot;深層学習 Day1,2&quot; のレポート提出を兼ねた受講記録です。 提出指示を満たすように、下記の方針でまとめました。&#40;事務局にも問い合わせて問題ないことを確認した&#41;</p> <ol> <li><p>動画講義の要点まとめ</p> <ul> <li><p>自分が講義の流れを思い出せるようなメモを残す。通常であれば要点として記載すべき図・数式などがあっても、それが自分にとって既知であれば、言葉の説明ですませることもある</p> </ul> <li><p>実装演習</p> <ul> <li><p>各Sectionで取り上げられた .ipynb or .py ファイルの内容を実行した結果を記載</p> <li><p>ただし、初学者向けにやや冗長な内容がある場合、抜粋することもある</p> </ul> <li><p>確認テスト</p> <ul> <li><p>確認テストの解答に相当する内容は、個別の節をもうけず、要点まとめに含めたり、コードに対するコメントとして記載する</p> <ul> <li><p>確認テストは重要事項だから、出題されているのであって、まとめ/演習と内容がかぶるはず。</p> <li><p>事務局がレポートチェックをする時のために、&#40;確認テスト:1-2&#41; のようなタグを付す。１つ目の数字が section番号、２つ目の数字が 「何番目のテストか?」</p> </ul> </ul> <li><p>1~3 をまとめる上で思うことがあれば、考察やコメントも適宜残しておく。</p> </ol> <h2 id="目次"><a href="#目次" class=header-anchor >目次</a></h2> <div class=franklin-toc ><ol><li><a href="#目次">目次</a><li><a href="#day1">Day1 </a><ol><li><a href="#プロローグ">プロローグ　</a><li><a href="#ニューラルネットワークの全体像"><ol> <li><p>ニューラルネットワークの全体像</p> </ol> </a><li><a href="#入力層-中間層"><ol> <li><p>入力層-中間層</p> </ol> </a><ol><li><a href="#実装演習抜粋_順伝播3層複数ユニット">実装演習抜粋 &#40; 順伝播（3層・複数ユニット）&#41;</a></ol><li><a href="#ol_start2_活性化関数"><ol start=2 > <li><p>活性化関数</p> </ol> </a><ol><li><ol><li><a href="#実装演習抜粋_順伝播_単層複数ユニット">実装演習抜粋 &#40;順伝播 単層・複数ユニット）</a></ol></ol><li><a href="#ol_start3_出力層"><ol start=3 > <li><p>出力層</p> </ol> </a><ol><li><a href="#実装演習_誤差関数定義">実装演習 &#40;誤差関数定義&#41;</a></ol><li><a href="#ol_start4_勾配降下法"><ol start=4 > <li><p>勾配降下法</p> </ol> </a><ol><li><a href="#実装演習_更新式定義">実装演習 &#40;更新式定義&#41;</a></ol><li><a href="#ol_start5_誤差逆伝播法"><ol start=5 > <li><p>誤差逆伝播法</p> </ol> </a><ol><li><a href="#実装演習_確率的勾配法のコードを用いて誤差逆伝播法を理解">実装演習 &#40;確率的勾配法のコードを用いて誤差逆伝播法を理解&#41;</a></ol><li><a href="#tipsディープラーニングの開発環境">Tips：ディープラーニングの開発環境</a><li><a href="#tips_その他の一般的な機械学習の手法について">Tips: その他の一般的な機械学習の手法について</a></ol><li><a href="#day_2_レポート">Day 2 レポート</a><ol><li><a href="#勾配消失問題"><ol> <li><p>勾配消失問題</p> </ol> </a><ol><li><a href="#実装演習">実装演習</a><ol><li><a href="#p33_例題チャレンジ">p.33 例題チャレンジ</a><li><a href="#初期化による勾配消失問題の改善">初期化による勾配消失問題の改善 </a><li><a href="#バッチ正規化">バッチ正規化</a></ol></ol><li><a href="#ol_start2_学習率最適化手法"><ol start=2 > <li><p>学習率最適化手法</p> </ol> </a><ol><li><a href="#実装演習__2">実装演習</a></ol><li><a href="#ol_start3_過学習"><ol start=3 > <li><p>過学習</p> </ol> </a><ol><li><a href="#実装演習__3">実装演習</a><ol><li><a href="#例題チャレンジ">例題チャレンジ</a><li><a href="#overfitting">overfitting </a></ol></ol><li><a href="#ol_start4_畳み込みニューラルネットワークcnnの概念"><ol start=4 > <li><p>畳み込みニューラルネットワーク&#40;CNN&#41;の概念</p> </ol> </a><ol><li><a href="#実装演習_畳み込みレイヤーの実装例">実装演習: 畳み込みレイヤーの実装例</a></ol><li><a href="#ol_start5_最新のcnn"><ol start=5 > <li><p>最新のCNN</p> </ol> </a></ol><li><a href="#取り組み概要_感想">取り組み概要 &amp; 感想</a><ol><li><a href="#取り組みの記録">取り組みの記録</a><li><a href="#感想ほか">感想ほか　</a></ol><li><a href="#計画_前回から変更無し">計画 &#40;前回から変更無し&#41;</a></ol></div> <h2 id=day1 ><a href="#day1" class=header-anchor >Day1 </a></h2> <h3 id="プロローグ"><a href="#プロローグ" class=header-anchor >プロローグ　</a></h3> <ul> <li><p>機械学習/DLモデルが解こうとしている問題</p> <ul> <li><p>識別 &#40;discriminateve, backward&#41;</p> <ul> <li><p>データ <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >x</mi></mrow><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44444em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span></span></span></span> -&gt; クラス <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_k</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> </p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant=normal >∣</mi><mi mathvariant=bold-italic >x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> p (C_k | \bm{x}) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mclose >)</span></span></span></span> を計算する &#40;確率最大となる <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_k</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> を分類結果にする&#41;</p> <li><p>例: 画像認識 &#40; 犬やネコの画像データを入力すると、犬なのか、ネコなのかを出力する&#41; </p> </ul> <li><p>高次元 -&gt; 低次元</p> <ul> <li><p>学習データは比較的少なめ</p> </ul> <li><p>モデルの例</p> <ul> <li><p>決定木, ロジスティック回帰, SVM, NN</p> </ul> <li><p>開発のアプローチ &#40;Phase.2でやった話&#41;</p> <ul> <li><p>生成的アプローチ </p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi mathvariant=bold-italic >x</mi><mi mathvariant=normal >∣</mi><msub><mi>C</mi><mi>k</mi></msub><mo stretchy=false >)</mo><mo separator=true >,</mo><mi>p</mi><mo stretchy=false >(</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(\bm{x} | C_k) , p(C_k)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mord >∣</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> をモデル化・推定し、 ベイズの定理から <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant=normal >∣</mi><mi mathvariant=bold-italic >x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(C_k| \bm{x}) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mclose >)</span></span></span></span> を計算する</p> <ul> <li><p>データの分布 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi mathvariant=bold-italic >x</mi><mi mathvariant=normal >∣</mi><msub><mi>C</mi><mi>k</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(\bm{x}|C_k)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mord >∣</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> は分類結果よりも複雑なことがある。</p> <ul> <li><p>その分学習コスト大。</p> <li><p>単純に分類結果を得たいだけならば、識別的なアプローチを用いる。</p> <li><p>副産物として得られた生成モデルを活用したい場合は良い。</p> </ul> </ul> <li><p>確率的な識別ができる。&#40;識別的アプローチと同様&#41;</p> </ul> </ul> </ul> </ul> <li><p>識別的アプローチ</p> <ul> <li><p>直接 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant=normal >∣</mi><mi mathvariant=bold-italic >x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(C_k| \bm{x}) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mclose >)</span></span></span></span> をモデル化・推定する</p> <li><p>確率的な識別</p> <ul> <li><p>機械学習のモデルの出力は 事後確率 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant=normal >∣</mi><mi mathvariant=bold-italic >x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">p(C_k|\bm{x})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord >∣</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mclose >)</span></span></span></span> であり、そこからどのように識別結果を得るかは、開発者の裁量。</p> <li><p>自信のある無しの度合いを測ることができる。</p> </ul> <li><p>学習コスト中</p> <li><p>確率的識別モデル</p> </ul> <li><p>識別関数</p> <ul> <li><p>入力 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >x</mi></mrow><annotation encoding="application/x-tex"> \bm{x} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44444em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span></span></span></span> から 出力のクラスへの写像 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy=false >(</mo><mi mathvariant=bold-italic >x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">f(\bm{x})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mclose >)</span></span></span></span> を直接推定</p> <li><p>学習データ・コストが少ない。決定的な識別</p> <li><p>決定的識別モデル</p> </ul> <li><p>生成 &#40;generative, forward&#41;</p> <ul> <li><p>クラス <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">C_k</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> -&gt; データ <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >x</mi></mrow><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44444em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span></span></span></span></p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy=false >(</mo><mi mathvariant=bold-italic >x</mi><mi mathvariant=normal >∣</mi><msub><mi>C</mi><mi>k</mi></msub><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> p (\bm{x}|C_k ) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class=mopen >(</span><span class=mord ><span class=mord ><span class="mord boldsymbol">x</span></span></span><span class=mord >∣</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> の分布情報を持った上で、サンプリングする。</p> <ul> <li><p>データの分布は複雑になりがち。実際に</p> </ul> <li><p>例: 犬の画像くれ、といったら 犬の画像っぽいものを出力してくれる, テキスト生成、画像の超解像など</p> </ul> <li><p>低次元 → 高次元</p> <ul> <li><p>学習データは大量に必要</p> </ul> <li><p>モデルの例</p> <ul> <li><p>HMM, ベイジアンネットワーク, VAE, GAN</p> </ul> </ul> </ul> <ul> <li><p>万能近似定理</p> <ul> <li><p>非線形の活性化関数を持つNeuralNetworkは任意の関数を近似できる</p> </ul> </ul> <h3 id="ニューラルネットワークの全体像"><a href="#ニューラルネットワークの全体像" class=header-anchor ><ol> <li><p>ニューラルネットワークの全体像</p> </ol> </a></h3> <ul> <li><p>入力層、中間層、出力層で構成される</p> <ul> <li><p>識別モデルの例では、入力層のノードにあたるのはデータ、出力層のノードはそれぞれのクラスに属する確率。 </p> <li><p>&#40;確認テスト:0-1&#41; </p> <ul> <li><p>ディープラーニングがやろとしていること</p> <ul> <li><p>入力されたデータを変換して所望の出力に変換する数学モデルをつくる</p> <li><p>ここで使用されるのはパラメータを持つ中間層を複数使用して高い自由度が得られるようなモデル</p> </ul> <li><p>学習によって最適化するのは</p> <ul> <li><p>パラメータ　&#40;3. 重み と 4. バイアス&#41;</p> </ul> </ul> </ul> <li><p>数式も用いて説明</p> <ul> <li><p>&#40;確認テスト:0-2&#41; 入力層2ノード, 中間層3ノード 出力層1ノード のネットワーク</p> </ul> </ul> <figure style="text-align:center;"> <img src="/assets/20210514-nn.svg" style="padding:0;width:150%;" alt="#1"/> <figcaption></figcaption> </figure> <ul> <li><p>ニューラルネットワークでできること</p> <ul> <li><p>回帰 : &#40;主に&#41; 連続的な実数値を取る関数の近似</p> <ul> <li><p>例</p> <ul> <li><p>結果の予想 &#40;売上、株価など&#41;</p> <li><p>ランキング &#40;競馬順位, 人気順位&#41; </p> </ul> <li><p>NN以外の手法</p> <ul> <li><p>線形回帰, ランダムフォレスト、回帰木</p> </ul> </ul> <li><p>分類 : 数値の大小関係に意味のない、離散的な結果を予想するための分析</p> <ul> <li><p>例</p> <ul> <li><p>ネコ写真判別</p> <li><p>手書き文字認識</p> <li><p>花の種類分類</p> </ul> <li><p>NN以外の手法</p> <ul> <li><p>ベイズ分類, ロジスティック回帰, 決定木, ランダムフォレスト</p> </ul> </ul> </ul> <li><p>深層学習の実用例</p> <ul> <li><p>深層学習モデルはとても表現力が高いため、入力と出力を数値・ベクトルに変換してしまいさえすれば、あらゆる問題に応用できる可能性がある</p> <li><p>例えば、自動売買、チャットボット、翻訳、音声解釈、囲碁/将棋AI など</p> </ul> </ul> <h3 id="入力層-中間層"><a href="#入力層-中間層" class=header-anchor ><ol> <li><p>入力層-中間層</p> </ol> </a></h3> <ul> <li><p>&#40;確認テスト:1-1&#41; 入力層~中間層の図に Section.0で取り上げた動物分類の実例を書き入れてみると以下のようになる </p> <ul> <li><p><img src="/assets/確認テスト1-1.png" alt="確認テスト:1-1" /></p> <li><p>愚痴コメント: &quot;この図式に&quot;実例を入れると書いてあるのだから、入力4次元のNNに合わせて実例の方を変えるべきですよね.. &#40;模範解答のような回答を期待しているのならば、「実例の図から、この図式のような形で入力層~中間層までに相当する部分を抜き出せ」というような問題文になるはず&#41;</p> </ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></msubsup><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">u=\sum_{i=1}^4 w_i x_i + b</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">u</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.253718em;vertical-align:-0.29971000000000003em;"></span><span class=mop ><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.954008em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mord ><span class="mord mathnormal">x</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span> の変換を行い、それが次の中間層の入力となり、活性化関数がかかって中間層の出力となる</p> <ul> <li><p>学習に置いては、この <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo separator=true >,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">w_i, b</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">b</span></span></span></span> が最適化される</p> <li><p>各計算式に対応する箇所は、実装演習のコードにコメントした。</p> </ul> </ul> <ul> <li><p>&quot;12_入力層の設計&quot; の動画の内容</p> <ul> <li><p>深層学習モデルに入力するには、とにかく対象を数値ベクトルにすればよい</p> <ul> <li><p>例: カテゴリ変数のようなものも フラグ値として定義可能</p> <ul> <li><p>犬・ネコ・ネズミの三種のいずれかを表したい場合、犬&#61;&#91;1,0,0&#93;, ネコ&#61;&#91;0,1,0&#93;, ネズミ&#61;&#91;0,0,1&#93; などとする &#40;ワンホットラベル,ワンホットベクトル&#41;</p> </ul> </ul> <li><p>入力層に入れるデータの選定</p> <ul> <li><p>欠損値が多いデータ、誤差の大きいデータは学習データからは取り除く</p> <li><p>解きたい問題の&quot;生の入力&quot; を入れて問題が解けるのが理想</p> <ul> <li><p>end2end のモデルを作るほうが全体最適なモデルができあがるはずで&#40;理屈上は&#41;ベター.</p> </ul> <li><p>意味なく振られた数値は使えない</p> <ul> <li><p>背番号などは必ずしも人と一対一対応しない</p> <li><p>誤差計算をするのに適切でない場合 &#40;Yes:1 No:0, どちらでもない:-1, 無回答:-1&#41; </p> <ul> <li><p>別の意味を持つ回答が同じ数値に割り当てられている</p> <li><p>Yes - No &#61; 1, No - 無回答&#61; 1 両者の違いが等しいわけではない</p> <li><p>-&gt; これらの問題を解決するには、ワンホットベクトルを使う</p> </ul> </ul> </ul> <li><p>入力データの加工例</p> <ul> <li><p>欠損値の扱い</p> <ul> <li><p>ゼロで埋める</p> <li><p>欠損値がやたらと多い説明変数はそもそも使わない</p> <li><p>色々な次元で欠損値があるサンプルはそもそも使わない</p> </ul> <li><p>数値の正規化・正則化</p> <ul> <li><p>正規化: &#40;絶対値の&#41;最大値で割って -1 ~ 1 とか、0~1 の範囲におさめる</p> <li><p>正則化: 平均0, 分散を1にする</p> </ul> <li><p>データの結合</p> <ul> <li><p>読んで時の通り。中には、数量的なデータとカテゴリ変数だったものを&#40;ワンホットエンコードして&#41;結合するようなこともある</p> </ul> </ul> </ul> </ul> <h4 id="実装演習抜粋_順伝播3層複数ユニット"><a href="#実装演習抜粋_順伝播3層複数ユニット" class=header-anchor >実装演習抜粋 &#40; 順伝播（3層・複数ユニット）&#41;</a></h4> <p>数式では、<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Wx+b</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span> なのに python では np.dot&#40;x,W&#41; となる? np.dot&#40;&#41; 関数の挙動をしっかり抑えておく必要がある。</p> <ul> <li><p>基本の動作は行列積&#40;のようなもの&#41;</p> <ul> <li><p>要素数が一致する次元: a の最後の axis と b の 最後から2番目のaxis </p> <li><p>通常の行列積も包含</p> </ul> <li><p>下記が適応不可能な自体では、よしなにそれっぽい計算が行われる</p> <ul> <li><p>例:</p> <ul> <li><p>if a,bの両方orどちらかがスカラー: 普通の積. &#40;スカラーをもう一方の全要素にかける&#41; </p> <li><p>elif b がベクトル: a の最後の axis と b の長さが一致</p> <ul> <li><p>ベクトルの内積のケースも包含</p> </ul> </ul> </ul> </ul> <pre><code class="python hljs"><span class=hljs-comment ># 順伝播（3層・複数ユニット）</span>

<span class=hljs-comment ># ウェイトとバイアスを設定</span>
<span class=hljs-comment ># ネートワークを作成</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">init_network</span>():
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;##### ネットワークの初期化 #####&quot;</span>)
    network = {}
    
    <span class=hljs-comment ># 試してみよう</span>
    <span class=hljs-comment >#_ネットワークの初期値ランダム生成</span>
    network[<span class=hljs-string >&#x27;W1&#x27;</span>] = np.random.rand(<span class=hljs-number >2</span>,<span class=hljs-number >3</span>)
    network[<span class=hljs-string >&#x27;W2&#x27;</span>] = np.random.rand(<span class=hljs-number >3</span>,<span class=hljs-number >2</span>)
    network[<span class=hljs-string >&#x27;W3&#x27;</span>] = np.random.rand(<span class=hljs-number >2</span>,<span class=hljs-number >2</span>)
    network[<span class=hljs-string >&#x27;b1&#x27;</span>] = np.random.rand(<span class=hljs-number >3</span>)
    network[<span class=hljs-string >&#x27;b2&#x27;</span>] = np.random.rand(<span class=hljs-number >2</span>)
    network[<span class=hljs-string >&#x27;b3&#x27;</span>] = np.random.rand(<span class=hljs-number >2</span>)

    <span class=hljs-comment ># 試してみよう</span>
    <span class=hljs-comment >#_各パラメータのshapeを表示</span>
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;*各パラメータのサイズ情報&quot;</span>)
    <span class=hljs-keyword >for</span> k,v <span class=hljs-keyword >in</span> network.items():
        <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;network[%s].shape=&quot;</span> % k, end=<span class=hljs-string >&#x27;&#x27;</span>)
        <span class=hljs-built_in >print</span>(v.shape)
    <span class=hljs-built_in >print</span>()

    <span class=hljs-comment ># 変更前</span>
    <span class=hljs-comment ># network[&#x27;W1&#x27;] = np.array([</span>
    <span class=hljs-comment >#     [0.1, 0.3, 0.5],</span>
    <span class=hljs-comment >#     [0.2, 0.4, 0.6]</span>
    <span class=hljs-comment ># ])</span>
    <span class=hljs-comment ># network[&#x27;W2&#x27;] = np.array([</span>
    <span class=hljs-comment >#     [0.1, 0.4],</span>
    <span class=hljs-comment >#     [0.2, 0.5],</span>
    <span class=hljs-comment >#     [0.3, 0.6]</span>
    <span class=hljs-comment ># ])</span>
    <span class=hljs-comment ># network[&#x27;W3&#x27;] = np.array([</span>
    <span class=hljs-comment >#     [0.1, 0.3],</span>
    <span class=hljs-comment >#     [0.2, 0.4]</span>
    <span class=hljs-comment ># ])</span>
    <span class=hljs-comment ># network[&#x27;b1&#x27;] = np.array([0.1, 0.2, 0.3])</span>
    <span class=hljs-comment ># network[&#x27;b2&#x27;] = np.array([0.1, 0.2])</span>
    <span class=hljs-comment ># network[&#x27;b3&#x27;] = np.array([1, 2])</span>

    print_vec(<span class=hljs-string >&quot;重み1&quot;</span>, network[<span class=hljs-string >&#x27;W1&#x27;</span>] )
    print_vec(<span class=hljs-string >&quot;重み2&quot;</span>, network[<span class=hljs-string >&#x27;W2&#x27;</span>] )
    print_vec(<span class=hljs-string >&quot;重み3&quot;</span>, network[<span class=hljs-string >&#x27;W3&#x27;</span>] )
    print_vec(<span class=hljs-string >&quot;バイアス1&quot;</span>, network[<span class=hljs-string >&#x27;b1&#x27;</span>] )
    print_vec(<span class=hljs-string >&quot;バイアス2&quot;</span>, network[<span class=hljs-string >&#x27;b2&#x27;</span>] )
    print_vec(<span class=hljs-string >&quot;バイアス3&quot;</span>, network[<span class=hljs-string >&#x27;b3&#x27;</span>] )

    

    <span class=hljs-keyword >return</span> network

<span class=hljs-comment ># プロセスを作成</span>
<span class=hljs-comment ># x：入力値</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >network, x</span>):
    
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;##### 順伝播開始 #####&quot;</span>)

    W1, W2, W3 = network[<span class=hljs-string >&#x27;W1&#x27;</span>], network[<span class=hljs-string >&#x27;W2&#x27;</span>], network[<span class=hljs-string >&#x27;W3&#x27;</span>]
    b1, b2, b3 = network[<span class=hljs-string >&#x27;b1&#x27;</span>], network[<span class=hljs-string >&#x27;b2&#x27;</span>], network[<span class=hljs-string >&#x27;b3&#x27;</span>]
    
    <span class=hljs-comment ># 1層の総入力</span>
    u1 = np.dot(x, W1) + b1  <span class=hljs-comment ># (確認テスト:1-2) 入力層の計算に該当</span>
    
    <span class=hljs-comment ># 1層の総出力</span>
    z1 = functions.relu(u1)  
    
    <span class=hljs-comment ># 2層の総入力</span>
    u2 = np.dot(z1, W2) + b2
    
    <span class=hljs-comment ># 2層の総出力</span>
    z2 = functions.relu(u2) <span class=hljs-comment ># (確認テスト:1-3) 中間層の出力に該当</span>

    <span class=hljs-comment ># 出力層の総入力</span>
    u3 = np.dot(z2, W3) + b3
    
    <span class=hljs-comment ># 出力層の総出力</span>
    y = u3
    
    print_vec(<span class=hljs-string >&quot;総入力1&quot;</span>, u1)
    print_vec(<span class=hljs-string >&quot;中間層出力1&quot;</span>, z1)
    print_vec(<span class=hljs-string >&quot;総入力2&quot;</span>, u2)
    print_vec(<span class=hljs-string >&quot;出力1&quot;</span>, z1)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;出力合計: &quot;</span> + <span class=hljs-built_in >str</span>(np.<span class=hljs-built_in >sum</span>(z1)))

    <span class=hljs-keyword >return</span> y, z1, z2

<span class=hljs-comment ># 入力値</span>
x = np.array([<span class=hljs-number >1.</span>, <span class=hljs-number >2.</span>])
print_vec(<span class=hljs-string >&quot;入力&quot;</span>, x)

<span class=hljs-comment ># ネットワークの初期化</span>
network =  init_network()

y, z1, z2 = forward(network, x)</code></pre> <pre><code class="julia hljs">*** 入力 ***
    [<span class=hljs-number >1.</span> <span class=hljs-number >2.</span>]
    
    <span class=hljs-comment >##### ネットワークの初期化 #####</span>
    *各パラメータのサイズ情報
    network[W1].shape=(<span class=hljs-number >2</span>, <span class=hljs-number >3</span>)
    network[W2].shape=(<span class=hljs-number >3</span>, <span class=hljs-number >2</span>)
    network[W3].shape=(<span class=hljs-number >2</span>, <span class=hljs-number >2</span>)
    network[b1].shape=(<span class=hljs-number >3</span>,)
    network[b2].shape=(<span class=hljs-number >2</span>,)
    network[b3].shape=(<span class=hljs-number >2</span>,)
    
    *** 重み<span class=hljs-number >1</span> ***
    [[<span class=hljs-number >0.46325703</span> <span class=hljs-number >0.4515028</span>  <span class=hljs-number >0.9015613</span> ]
     [<span class=hljs-number >0.63300315</span> <span class=hljs-number >0.09960203</span> <span class=hljs-number >0.35035177</span>]]
    
    *** 重み<span class=hljs-number >2</span> ***
    [[<span class=hljs-number >0.85561969</span> <span class=hljs-number >0.64270626</span>]
     [<span class=hljs-number >0.71017196</span> <span class=hljs-number >0.35407221</span>]
     [<span class=hljs-number >0.21520079</span> <span class=hljs-number >0.12526929</span>]]
    
    *** 重み<span class=hljs-number >3</span> ***
    [[<span class=hljs-number >0.04970514</span> <span class=hljs-number >0.3372038</span> ]
     [<span class=hljs-number >0.80197002</span> <span class=hljs-number >0.34030274</span>]]
    
    *** バイアス<span class=hljs-number >1</span> ***
    [<span class=hljs-number >0.14282813</span> <span class=hljs-number >0.02490021</span> <span class=hljs-number >0.42369957</span>]
    
    *** バイアス<span class=hljs-number >2</span> ***
    [<span class=hljs-number >0.16724682</span> <span class=hljs-number >0.77646073</span>]
    
    *** バイアス<span class=hljs-number >3</span> ***
    [<span class=hljs-number >0.97460445</span> <span class=hljs-number >0.08008213</span>]
    
    <span class=hljs-comment >##### 順伝播開始 #####</span>
    *** 総入力<span class=hljs-number >1</span> ***
    [<span class=hljs-number >1.87209147</span> <span class=hljs-number >0.67560708</span> <span class=hljs-number >2.02596442</span>]
    
    *** 中間層出力<span class=hljs-number >1</span> ***
    [<span class=hljs-number >1.87209147</span> <span class=hljs-number >0.67560708</span> <span class=hljs-number >2.02596442</span>]
    
    *** 総入力<span class=hljs-number >2</span> ***
    [<span class=hljs-number >2.6848315</span>  <span class=hljs-number >2.47267045</span>]
    
    *** 出力<span class=hljs-number >1</span> ***
    [<span class=hljs-number >1.87209147</span> <span class=hljs-number >0.67560708</span> <span class=hljs-number >2.02596442</span>]
    
    出力合計: <span class=hljs-number >4.573662972619907</span></code></pre> <h3 id="ol_start2_活性化関数"><a href="#ol_start2_活性化関数" class=header-anchor ><ol start=2 > <li><p>活性化関数</p> </ol> </a></h3> <ul> <li><p>活性化関数</p> <ul> <li><p>次の層への出力を決める &#40;一般的には&#41;非線形の関数 </p> <ul> <li><p>Section.0 の図中の関数 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold >f</mi></mrow><annotation encoding="application/x-tex">\mathbf{f}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class=mord ><span class="mord mathbf" style="margin-right:0.10903em;">f</span></span></span></span></span> に相当</p> </ul> <li><p>&#40;確認テスト:2-1&#41; 線形と非線形</p> <ul> <li><p>図にすると</p> <ul> <li><p>線形 <img src="/assets/1_1_forward_propagation_16_0.svg" alt=svg  /> </p> <li><p>非線形 <img src="/assets/1_1_forward_propagation_16_1.svg" alt=svg  /></p> </ul> <li><p>線形な関数は、加法性 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy=false >(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo stretchy=false >)</mo><mo>=</mo><mi>f</mi><mo stretchy=false >(</mo><mi>a</mi><mo stretchy=false >)</mo><mo>+</mo><mi>f</mi><mo stretchy=false >(</mo><mi>b</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> f(a+b) = f(a) + f(b) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">a</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">a</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">b</span><span class=mclose >)</span></span></span></span> と 斉次性&#40;作用素との可換性&#41; <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy=false >(</mo><mi>a</mi><mi>b</mi><mo stretchy=false >)</mo><mo>=</mo><mi>a</mi><mi>f</mi><mo stretchy=false >(</mo><mi>b</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> f(ab) = a f(b) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">a</span><span class="mord mathnormal">b</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">b</span><span class=mclose >)</span></span></span></span> を満たす</p> <li><p>非線形な関数は、その名の通り、線形な関数 &quot;以外&quot; </p> </ul> <li><p>活性化関数の例</p> <ul> <li><p>中間層で用いられる代表例</p> <ul> <li><p>ステップ関数 </p> <ul> <li><p>パーセプトロンで使われていた経緯があるが 0 or 1 しか表せないので 線形分離可能なものしか識別できないということで最近は使われない</p> </ul> <li><p>シグモイド関数</p> <ul> <li><p>すべての点で微分可能で扱いやすいが、勾配消失問題を引き起こしやすい</p> </ul> <li><p>ReLU関数</p> <ul> <li><p>シグモイドに比べれば勾配消失問題を起こしにくい</p> <li><p>スパース化に貢献</p> </ul> </ul> <li><p>出力層でもちいられる代表例</p> <ul> <li><p>ソフトマックス関数</p> <li><p>恒等写像 &#40;これは非線形関数じゃないから&#41;</p> </ul> </ul> </ul> </ul> <h5 id="実装演習抜粋_順伝播_単層複数ユニット"><a href="#実装演習抜粋_順伝播_単層複数ユニット" class=header-anchor >実装演習抜粋 &#40;順伝播 単層・複数ユニット）</a></h5> <pre><code class="python hljs"><span class=hljs-comment ># 順伝播（単層・複数ユニット）</span>

<span class=hljs-comment ># 重み</span>
W = np.array([
    [<span class=hljs-number >0.1</span>, <span class=hljs-number >0.2</span>, <span class=hljs-number >0.3</span>], 
    [<span class=hljs-number >0.2</span>, <span class=hljs-number >0.3</span>, <span class=hljs-number >0.4</span>], 
    [<span class=hljs-number >0.3</span>, <span class=hljs-number >0.4</span>, <span class=hljs-number >0.5</span>],
    [<span class=hljs-number >0.4</span>, <span class=hljs-number >0.5</span>, <span class=hljs-number >0.6</span>]
])

<span class=hljs-comment >## 試してみよう_配列の初期化</span>
<span class=hljs-comment >#W = np.zeros((4,3))</span>
<span class=hljs-comment >#W = np.ones((4,3))</span>
<span class=hljs-comment >#W = np.random.rand(4,3)</span>
W = np.random.randint(<span class=hljs-number >5</span>, size=(<span class=hljs-number >4</span>,<span class=hljs-number >3</span>)) <span class=hljs-comment ># randint を試してみた例</span>

print_vec(<span class=hljs-string >&quot;重み&quot;</span>, W)

<span class=hljs-comment ># バイアス</span>
b = np.array([<span class=hljs-number >0.1</span>, <span class=hljs-number >0.2</span>, <span class=hljs-number >0.3</span>])
print_vec(<span class=hljs-string >&quot;バイアス&quot;</span>, b)

<span class=hljs-comment ># 入力値</span>
x = np.array([<span class=hljs-number >1.0</span>, <span class=hljs-number >5.0</span>, <span class=hljs-number >2.0</span>, -<span class=hljs-number >1.0</span>])
print_vec(<span class=hljs-string >&quot;入力&quot;</span>, x)


<span class=hljs-comment >#  総入力</span>
u = np.dot(x, W) + b
print_vec(<span class=hljs-string >&quot;総入力&quot;</span>, u)

<span class=hljs-comment ># 中間層出力</span>
z = functions.sigmoid(u)  <span class=hljs-comment ># (確認テスト:2-2) </span>
print_vec(<span class=hljs-string >&quot;中間層出力&quot;</span>, z)</code></pre> <pre><code class="julia hljs">*** 重み ***
    [[<span class=hljs-number >3</span> <span class=hljs-number >1</span> <span class=hljs-number >4</span>]
     [<span class=hljs-number >3</span> <span class=hljs-number >1</span> <span class=hljs-number >0</span>]
     [<span class=hljs-number >4</span> <span class=hljs-number >2</span> <span class=hljs-number >3</span>]
     [<span class=hljs-number >0</span> <span class=hljs-number >3</span> <span class=hljs-number >4</span>]]
    
    *** バイアス ***
    [<span class=hljs-number >0.1</span> <span class=hljs-number >0.2</span> <span class=hljs-number >0.3</span>]
    
    *** 入力 ***
    [ <span class=hljs-number >1.</span>  <span class=hljs-number >5.</span>  <span class=hljs-number >2.</span> -<span class=hljs-number >1.</span>]
    
    *** 総入力 ***
    [<span class=hljs-number >26.1</span>  <span class=hljs-number >7.2</span>  <span class=hljs-number >6.3</span>]
    
    *** 中間層出力 ***
    [<span class=hljs-number >1.</span>         <span class=hljs-number >0.99925397</span> <span class=hljs-number >0.99816706</span>]</code></pre> <h3 id="ol_start3_出力層"><a href="#ol_start3_出力層" class=header-anchor ><ol start=3 > <li><p>出力層</p> </ol> </a></h3> <ul> <li><p>誤差関数</p> <ul> <li><p>その時々のパラメータを用いて、入力データから計算したモデルの出力が、どれくらい間違っているかを計算するもの</p> <ul> <li><p>予め 入力データ&#40;NNモデルの入力&#41;　と 訓練データ&#40;入力データに対応する正解値&#41;を用意しておく</p> </ul> <li><p>例: 二乗誤差和 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></msubsup><mo stretchy=false >(</mo><msub><mi>y</mi><mi>j</mi></msub><mo>−</mo><msub><mi>d</mi><mi>j</mi></msub><msup><mo stretchy=false >)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> \frac{1}{2} \sum_{j=1}^{J} (y_j-d_j)^2 </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.417049em;vertical-align:-0.43581800000000004em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mop ><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em;">J</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class=mopen >(</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.1002159999999999em;vertical-align:-0.286108em;"></span><span class=mord ><span class="mord mathnormal">d</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span><span class=mclose ><span class=mclose >)</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> </p> <ul> <li><p>&#40;確認テスト:3-1&#41; </p> <ul> <li><p>差を二乗する理由: 各要素毎の誤差を非負値にすることで、完全にベクトルが一致するとき以外誤差が0にならないようにするための一方法</p> <li><p>1/2 している理由: 最適化計算のための微分計算をすることになるのだが、二乗を微分してでてくる 2 と打ち消し合って式がよりシンプルになることを狙っている。</p> </ul> <li><p>通常分類問題では誤差関数にクロスエントロピー誤差を用いることが多いが、本講義では説明の便宜上平均二乗誤差が用いられる箇所がある</p> </ul> </ul> <li><p>出力層の活性化関数</p> <ul> <li><p>中間層とは役割が異なるため、利用される活性化関数も異なる</p> <li><p>モデル出力を解いている問題にあった使いやすい形に変換するために用いる</p> <ul> <li><p>分類問題では、入力データがどの分類クラスのデータであるかを表す確率を出力させる</p> <ul> <li><p>出力ベクトルの各要素は 0~1の範囲で、全要素の和が1</p> </ul> </ul> <li><p>例</p> <ul> <li><p>回帰問題</p> <ul> <li><p>活性化関数 : 恒等写像</p> <li><p>誤差関数 : &#40;平均&#41;二乗誤差</p> </ul> <li><p>二値分類</p> <ul> <li><p>活性化関数 : シグモイド関数</p> <li><p>誤差関数: 交差エントロピー</p> </ul> <li><p>多クラス分類</p> <ul> <li><p>活性化関数 : ソフトマックス関数 </p> <li><p>誤差関数: 交差エントロピー</p> </ul> </ul> </ul> </ul> <h4 id="実装演習_誤差関数定義"><a href="#実装演習_誤差関数定義" class=header-anchor >実装演習 &#40;誤差関数定義&#41;</a></h4> <p>厳密には、実装演習は無し。確認テストのために関数定義を確認しただけ</p> <pre><code class="python hljs"><span class=hljs-comment ># ソフトマックス関数　(確認テスト:3-2)</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">softmax</span>(<span class=hljs-params >x</span>):
    <span class=hljs-keyword >if</span> x.ndim == <span class=hljs-number >2</span>: <span class=hljs-comment ># 複数データが入力された場合のための場合分け</span>
        x = x.T
        x = x - np.<span class=hljs-built_in >max</span>(x, axis=<span class=hljs-number >0</span>)
        y = np.exp(x) / np.<span class=hljs-built_in >sum</span>(np.exp(x), axis=<span class=hljs-number >0</span>)
        <span class=hljs-keyword >return</span> y.T

    x = x - np.<span class=hljs-built_in >max</span>(x) <span class=hljs-comment ># オーバーフロー対策 (プログラム動作を安定化させるためのもの)</span>
    <span class=hljs-keyword >return</span> np.exp(x) / np.<span class=hljs-built_in >sum</span>(np.exp(x)) <span class=hljs-comment ># この一行がソフトマックス関数の本質的な実装</span>
    <span class=hljs-comment ># (1) はあえていうなら return</span>
    <span class=hljs-comment ># (2) は分子の np.exp(x) </span>
    <span class=hljs-comment ># (3) は分母 の np.sum(np.exp(x))</span></code></pre> <pre><code class="python hljs"><span class=hljs-comment ># クロスエントロピー (確認テスト:3-3)</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class=hljs-params >d, y</span>):
    <span class=hljs-keyword >if</span> y.ndim == <span class=hljs-number >1</span>: <span class=hljs-comment ># 次元を一つ増やして、この後の処理と整合性をとる</span>
        d = d.reshape(<span class=hljs-number >1</span>, d.size)
        y = y.reshape(<span class=hljs-number >1</span>, y.size)
        
    <span class=hljs-comment ># 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換</span>
    <span class=hljs-keyword >if</span> d.size == y.size:
        d = d.argmax(axis=<span class=hljs-number >1</span>)
             
    batch_size = y.shape[<span class=hljs-number >0</span>]
    <span class=hljs-keyword >return</span> -np.<span class=hljs-built_in >sum</span>(np.log(y[np.arange(batch_size), d] + <span class=hljs-number >1e-7</span>)) / batch_size
    <span class=hljs-comment ># (1) はあえていうなら return</span>
    <span class=hljs-comment ># (2) は -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) が本質的な部分</span>
    <span class=hljs-comment >#    y[np.arange(batch_size), d] は 少しトリッキーだが、</span>
    <span class=hljs-comment >#    この関数上で yは確率ベクトルであるが、dはワンホットエンコードされたものではなく、正解ラベルである前提で</span>
    <span class=hljs-comment >#    np.sum() は複数データに関する足し算であって、数式上の 和とはことなる</span>
    <span class=hljs-comment ># + 1e-7 は logの真数を確実に正の値とするための epsilon</span></code></pre> <h3 id="ol_start4_勾配降下法"><a href="#ol_start4_勾配降下法" class=header-anchor ><ol start=4 > <li><p>勾配降下法</p> </ol> </a></h3> <ul> <li><p>勾配降下法の考え方</p> <ul> <li><p>深層学習モデルを構築するための最適化手法として用いる</p> <ul> <li><p>学習 &#61; 誤差関数 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy=false >(</mo><mi mathvariant=bold >w</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">E(\mathbf{w})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mopen >(</span><span class=mord ><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span><span class=mclose >)</span></span></span></span> を最小化するパラメータ <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold >w</mi></mrow><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44444em;vertical-align:0em;"></span><span class=mord ><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span></span></span></span> を見つける</p> <li><p>最適化手法には 通常 &quot;勾配降下法&quot; が用いられる</p> </ul> <li><p>パラメータ更新式のイメージと学習率</p> <ul> <li><p>&#40;確認テスト:4-1&#41; -&gt; Section.5 勾配降下法の実装演習コード上にコメント付記</p> <li><p>その時々のパラメータにおける勾配と逆方向にパラメータを更新する &#61; 極小値に向かう変更</p> <li><p>学習率 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> はその歩幅を決める</p> <ul> <li><p>学習率が大きすぎると、最適解&#40;&#61;大域的極小値&#41;を飛び越えて最悪発散する可能性がある</p> <li><p>学習率が小さすぎると、最適解にたどり着くまでの更新回数が増える&#40;時間がかかる&#41;</p> <ul> <li><p>小さすぎると局所解につかまるが、適切な量に設定すると飛び越えられるという説明の図&#40;下図&#41;は、不適切&#40;もしくは言葉足らず&#41;なように思う </p> <ul> <li><p><img src="/assets/2021-05-16_142224.jpg" alt="" /></p> <li><p>説明では強引に最適解まで更新し続けるような矢印が書かれているが、、一回目の更新後の勾配は正値のためパラメータは局所解の方向に向かう。勾配の絶対値も小さいのでちょうど局所最適にハマるような絵になっている。</p> </ul> </ul> </ul> <li><p>勾配降下法ベースのパラメータ更新アルゴリズム &#40;詳細はDay2&#41;</p> <ul> <li><p>Momentum, AdaGrad, Adadelta, Adam など。</p> <li><p>Adam がよく使われる</p> </ul> </ul> </ul> <li><p>&#40;バッチ&#41;勾配降下法</p> <ul> <li><p>与えられたデータ&#40;全部&#41;に対する出力からエラーを計算する</p> <li><p>そこから得られた勾配を使って、パラメータ&#40;重み <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span> と バイアス <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>&#41;を更新</p> <li><p>更新されたパラメータを用いて、次の週&#40;エポック&#41;へ</p> </ul> </ul> <ul> <li><p>確率的勾配降下法 &#40;SGD&#41;</p> <ul> <li><p>パラメータを更新する毎にランダムに抽出したサンプルの誤差を計算</p> <ul> <li><p>1サンプルで1回パラメータ更新</p> </ul> <li><p>メリット</p> <ul> <li><p>データが冗長な場合計算コストを軽減できる</p> <li><p>パラメータ更新方向にランダム性をもたせることで、望まない局所解に収束するリスクを軽減する</p> <li><p>オンライン学習ができる</p> <ul> <li><p>&#40;確認テスト:4-2&#41; オンライン学習とは</p> <ul> <li><p>新しい学習データが入ってくるたびに都度パラメータを更新する学習方法。バッチ学習は一度にすべての学習データを使ってパラメータ更新を行う。</p> <ul> <li><p>大規模な機械学習・深層学習では計算機のRAM制約でバッチ学習を実施することが非現実になることがある</p> </ul> </ul> </ul> </ul> </ul> </ul> <ul> <li><p>ミニバッチ勾配降下法 &#40;深層学習の場合は基本的にこのアプローチをとる&#41;</p> <ul> <li><p>ランダムに分割したデータの集合&#40;ミニバッチ&#41; <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">D_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> に属するサンプルの誤差を使って勾配を計算する方法</p> <ul> <li><p>例: 全データ10万 &#61; 500データ x 2000 バッチ</p> <li><p>&#40;要確認&#41;: 結局、1エポックに1回更新するのか、1ミニバッチ毎に1回更新するのか? </p> </ul> <li><p>メリット</p> <ul> <li><p>確率的勾配降下法のメリットを維持しつつ、計算機の計算資源を有効利用できる</p> <ul> <li><p>CPUを利用したスレッド並列化や、GPUを利用したSIMD並列化など</p> <li><p>&#40;要確認&#41;: 講師の説明では、複数のバッチを同時並行的に計算すると言っているが、私の理解では、バッチ内の複数データを並列計算するというのが、通常のミニバッチ勾配降下法の実装のされ方だと思っていた</p> <ul> <li><p>分散学習などするときには、各分散ノードで異なるミニバッチの処理を同時に行うことがあるのは知っているが、現時点でその話を想定して話をしていると考えるは無理がある.. </p> </ul> </ul> <li><p>&#40;確認テスト:4-3&#41; <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup><mo>−</mo><mi>ϵ</mi><mi mathvariant=normal >∇</mi><msub><mi>E</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w^{(t+1)}=w^{(t)}-\epsilon \nabla E_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal">ϵ</span><span class=mord >∇</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> のイメージ図</p> <ul> <li><p><img src="/assets/210517_gradient.png" alt="パラメータ更新イメージ" /></p> <li><p>ミニバッチ勾配降下法の動画の中で取り上げられた文脈なので、<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span></span></span></span> は ミニバッチのインデックスかと思っていたが、動画では エポックのインデックスだという。。理解が間違っているのか? その辺りはぼかした図にしておく.</p> </ul> </ul> </ul> </ul> <h4 id="実装演習_更新式定義"><a href="#実装演習_更新式定義" class=header-anchor >実装演習 &#40;更新式定義&#41;</a></h4> <p>厳密には、実装演習は無し? -&gt; 誤差逆伝播法の実装演習と兼ねる。</p> <h3 id="ol_start5_誤差逆伝播法"><a href="#ol_start5_誤差逆伝播法" class=header-anchor ><ol start=5 > <li><p>誤差逆伝播法</p> </ol> </a></h3> <ul> <li><p>誤差逆伝播法をなぜ使うか?</p> <ul> <li><p>勾配降下法を行うためには、各々のパラメータに関する誤差関数の微分値を求める必要がある</p> <li><p>数値微分では、演算負荷が大きい。</p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant=normal >∂</mi><mi>E</mi></mrow><mrow><mi mathvariant=normal >∂</mi><msub><mi>w</mi><mi>m</mi></msub></mrow></mfrac><mo>≃</mo><mfrac><mrow><mi>E</mi><mo stretchy=false >(</mo><msub><mi>w</mi><mi>m</mi></msub><mo>+</mo><mi>h</mi><mo stretchy=false >)</mo><mo>−</mo><mi>E</mi><mo stretchy=false >(</mo><msub><mi>w</mi><mi>m</mi></msub><mo>−</mo><mi>h</mi><mo stretchy=false >)</mo></mrow><mrow><mn>2</mn><mi>h</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial E}{\partial w_m} \simeq \frac{E(w_m+h) - E(w_m-h)}{2h}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.325208em;vertical-align:-0.44509999999999994em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.655em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >≃</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.355em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">h</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p> <li><p>各パラメータに対して、現在の値を中心に <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo><mi>h</mi></mrow><annotation encoding="application/x-tex">\pm h</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.77777em;vertical-align:-0.08333em;"></span><span class=mord >±</span><span class="mord mathnormal">h</span></span></span></span> ずらした誤差関数を計算する必要があり、大量パラメータの反復更新を膨大な回数行う深層学習では、効率が悪い</p> </ul> <li><p>誤差逆伝播法は、上記と比較して効率がよい</p> </ul> </ul> <ul> <li><p>誤差逆伝播法の方法</p> <ul> <li><p>合成関数の微分&#40;連鎖律&#41;を用いて、出力層側から順番に微分値を計算していく</p> <ul> <li><p>コメント: 図を使った講師の表現が気になる..</p> <ul> <li><p>講師の表現 : &quot;yはuになって, u は w になって&quot; </p> <li><p>自分的にしっくりくる表現: &quot;yはuの関数で、uはwの関数で&quot; -&gt; 後に講師もこの表現で説明してた.安心。</p> </ul> <li><p>例: 3層のNNの中間層の重み <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{w}^{(2)}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span> の 1要素 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>w</mi><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">w^{(2)}_{ji}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.4577719999999998em;vertical-align:-0.4129719999999999em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.4129719999999999em;"><span></span></span></span></span></span></span></span></span></span> に関する勾配を求めるための計算</p> <ul> <li><p>連鎖律: <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant=normal >∂</mi><mi>E</mi></mrow><mrow><mi mathvariant=normal >∂</mi><msubsup><mi>w</mi><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msubsup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant=normal >∂</mi><mi>E</mi></mrow><mrow><mi mathvariant=normal >∂</mi><mi mathvariant=bold-italic >y</mi></mrow></mfrac><mfrac><mrow><mi mathvariant=normal >∂</mi><mi mathvariant=bold-italic >y</mi></mrow><mrow><mi mathvariant=normal >∂</mi><msup><mi mathvariant=bold-italic >u</mi><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mi mathvariant=normal >∂</mi><msup><mi mathvariant=bold-italic >u</mi><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msup></mrow><mrow><mi mathvariant=normal >∂</mi><msubsup><mi>w</mi><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msubsup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial E}{\partial w^{(2)}_{ji}} = \frac{\partial E}{\partial \bm{y}} \frac{\partial \bm{y}}{\partial \bm{u}^{(2)}} \frac{\partial \bm{u}^{(2)}}{\partial w^{(2)}_{ji}}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.7542879999999998em;vertical-align:-0.8741799999999998em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.44864em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0590857142857142em;"><span style="top:-2.2134285714285715em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.059085714285714em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.46117142857142857em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8741799999999998em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.94488em;vertical-align:-0.8741799999999998em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">y</span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.9322159999999999em;"><span style="top:-2.614575em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight">u</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8220357142857143em;"><span style="top:-2.8220357142857138em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">y</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.38542499999999996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0707em;"><span style="top:-2.44864em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0590857142857142em;"><span style="top:-2.2134285714285715em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.059085714285714em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.46117142857142857em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight">u</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.9667142857142857em;"><span style="top:-2.966714285714285em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8741799999999998em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant=normal >∂</mi><mi>E</mi></mrow><mrow><mi mathvariant=normal >∂</mi><mi>y</mi></mrow></mfrac><mo>=</mo><mi mathvariant=bold-italic >y</mi><mo>−</mo><mi mathvariant=bold-italic >d</mi></mrow><annotation encoding="application/x-tex"> \frac{\partial E}{\partial y} = \bm{y}-\bm{d}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.3612159999999998em;vertical-align:-0.481108em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.03704em;">y</span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">d</span></span></span></span></span></span> </p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant=normal >∂</mi><mi mathvariant=bold-italic >y</mi></mrow><mrow><mi mathvariant=normal >∂</mi><msup><mi mathvariant=bold-italic >u</mi><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex"> \frac{\partial \bm{y}}{\partial \bm{u}^{(2)}} = 1</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.3176409999999998em;vertical-align:-0.38542499999999996em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.9322159999999999em;"><span style="top:-2.614575em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight">u</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8220357142857143em;"><span style="top:-2.8220357142857138em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight" style="margin-right:0.03704em;">y</span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.38542499999999996em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.64444em;vertical-align:0em;"></span><span class=mord >1</span></span></span></span></p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant=normal >∂</mi><msup><mi>u</mi><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msup></mrow><mrow><mi mathvariant=normal >∂</mi><msubsup><mi>w</mi><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy=false >(</mo><mn>2</mn><mo stretchy=false >)</mo></mrow></msubsup></mrow></mfrac><mo>=</mo><mo stretchy=false >[</mo><mn>0</mn><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><msub><mi>z</mi><mi>i</mi></msub><mo separator=true >,</mo><mo>…</mo><mo separator=true >,</mo><mn>0</mn><msup><mo stretchy=false >]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\frac{\partial u^{(2)}}{\partial w^{(2)}_{ji}} = [0,\ldots,z_i,\ldots,0]^T</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.94488em;vertical-align:-0.8741799999999998em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0707em;"><span style="top:-2.44864em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.0590857142857142em;"><span style="top:-2.2134285714285715em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.059085714285714em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.46117142857142857em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.9667142857142857em;"><span style="top:-2.966714285714285em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5357142857142856em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.8741799999999998em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class=mopen >[</span><span class=mord >0</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=minner >…</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord >0</span><span class=mclose ><span class=mclose >]</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> </p> </ul> </ul> <li><p>&#40;確認テスト:5-1&#41; -&gt; 実装演習コードにコメント</p> <li><p>&#40;確認テスト:5-2&#41; -&gt; 実装演習コードにコメント &#40;動画で説明された模範解答は間違っていると思う..&#41;</p> </ul> </ul> <h4 id="実装演習_確率的勾配法のコードを用いて誤差逆伝播法を理解"><a href="#実装演習_確率的勾配法のコードを用いて誤差逆伝播法を理解" class=header-anchor >実装演習 &#40;確率的勾配法のコードを用いて誤差逆伝播法を理解&#41;</a></h4> <p>ちなみに、ファイルをダウンロードした状態では、 forward/backward で活性化関数の不一致があったため、reluに統一した &#40;sigmoidでも同様に収束していそうなことは確認&#41;</p> <pre><code class="python hljs"><span class=hljs-keyword >import</span> sys
sys.path.append(<span class=hljs-string >&#x27;.&#x27;</span>)

<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> functions
<span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt

<span class=hljs-keyword >def</span> <span class="hljs-title function_">print_vec</span>(<span class=hljs-params >text, vec</span>):
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;*** &quot;</span> + text + <span class=hljs-string >&quot; ***&quot;</span>)
    <span class=hljs-built_in >print</span>(vec)
    <span class=hljs-comment >#print(&quot;shape: &quot; + str(x.shape))</span>
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;&quot;</span>)</code></pre> <p>確率勾配降下法 </p> <pre><code class="python hljs"><span class=hljs-comment ># サンプルとする関数</span>
<span class=hljs-comment >#yの値を予想するAI</span>

<span class=hljs-keyword >def</span> <span class="hljs-title function_">f</span>(<span class=hljs-params >x</span>):
    y = <span class=hljs-number >3</span> * x[<span class=hljs-number >0</span>] + <span class=hljs-number >2</span> * x[<span class=hljs-number >1</span>]
    <span class=hljs-keyword >return</span> y

<span class=hljs-comment ># 初期設定</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">init_network</span>():
    <span class=hljs-comment ># print(&quot;##### ネットワークの初期化 #####&quot;)</span>
    network = {}
    nodesNum = <span class=hljs-number >10</span>
    network[<span class=hljs-string >&#x27;W1&#x27;</span>] = np.random.randn(<span class=hljs-number >2</span>, nodesNum)
    network[<span class=hljs-string >&#x27;W2&#x27;</span>] = np.random.randn(nodesNum)
    network[<span class=hljs-string >&#x27;b1&#x27;</span>] = np.random.randn(nodesNum)
    network[<span class=hljs-string >&#x27;b2&#x27;</span>] = np.random.randn()

    <span class=hljs-comment ># print_vec(&quot;重み1&quot;, network[&#x27;W1&#x27;])</span>
    <span class=hljs-comment ># print_vec(&quot;重み2&quot;, network[&#x27;W2&#x27;])</span>
    <span class=hljs-comment ># print_vec(&quot;バイアス1&quot;, network[&#x27;b1&#x27;])</span>
    <span class=hljs-comment ># print_vec(&quot;バイアス2&quot;, network[&#x27;b2&#x27;])</span>

    <span class=hljs-keyword >return</span> network

<span class=hljs-comment ># 順伝播</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >network, x</span>):
    <span class=hljs-comment ># print(&quot;##### 順伝播開始 #####&quot;)</span>
    
    W1, W2 = network[<span class=hljs-string >&#x27;W1&#x27;</span>], network[<span class=hljs-string >&#x27;W2&#x27;</span>]
    b1, b2 = network[<span class=hljs-string >&#x27;b1&#x27;</span>], network[<span class=hljs-string >&#x27;b2&#x27;</span>]
    u1 = np.dot(x, W1) + b1
    z1 = functions.relu(u1)    
    <span class=hljs-comment >#z1 = functions.sigmoid(u1)</span>
    
    u2 = np.dot(z1, W2) + b2
    y = u2

    <span class=hljs-keyword >return</span> z1, y

<span class=hljs-comment ># 誤差逆伝播</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">backward</span>(<span class=hljs-params >x, d, z1, y</span>):
    <span class=hljs-comment ># print(&quot;\n##### 誤差逆伝播開始 #####&quot;)    </span>

    grad = {}
    
    W1, W2 = network[<span class=hljs-string >&#x27;W1&#x27;</span>], network[<span class=hljs-string >&#x27;W2&#x27;</span>]
    b1, b2 = network[<span class=hljs-string >&#x27;b1&#x27;</span>], network[<span class=hljs-string >&#x27;b2&#x27;</span>]
    <span class=hljs-comment ># (確認テスト:5-1) 微分値再利用 &amp; (確認テスト:5-2) 各微分値の計算 -------------ここから</span>
    <span class=hljs-comment ># 出力層でのデルタ</span>
    <span class=hljs-comment >#  dE/dy = dE/dy ・ dy/du2  の計算に該当. (yとu2が恒等写像なので) </span>
    <span class=hljs-comment >#  これを保存して 後に再利用していることで演算効率をあげている</span>
    delta2 = functions.d_mean_squared_error(d, y)

    <span class=hljs-comment ># b2の勾配</span>
    grad[<span class=hljs-string >&#x27;b2&#x27;</span>] = np.<span class=hljs-built_in >sum</span>(delta2, axis=<span class=hljs-number >0</span>) <span class=hljs-comment ># delta2 の再利用 </span>
    <span class=hljs-comment ># W2の勾配</span>
    grad[<span class=hljs-string >&#x27;W2&#x27;</span>] = np.dot(z1.T, delta2) <span class=hljs-comment ># delta 2 の再利用, dE/dw2 = dE/dy ・ dy/du2 ・ du2/dw2</span>

    <span class=hljs-comment ># 中間層でのデルタ</span>
    <span class=hljs-comment ># dE/du1 = dE/dy・dy/du2・ du2/dz ・dz/du1 の保存</span>
    <span class=hljs-comment ># これを保存して 後に再利用していることで演算効率をあげている</span>
    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)   
    <span class=hljs-comment >#delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1) </span>

    delta1 = delta1[np.newaxis, :]
    <span class=hljs-comment ># b1の勾配</span>
    grad[<span class=hljs-string >&#x27;b1&#x27;</span>] = np.<span class=hljs-built_in >sum</span>(delta1, axis=<span class=hljs-number >0</span>) <span class=hljs-comment ># delta1 の再利用</span>
    x = x[np.newaxis, :]
    <span class=hljs-comment ># W1の勾配</span>
    grad[<span class=hljs-string >&#x27;W1&#x27;</span>] = np.dot(x.T, delta1) <span class=hljs-comment ># delta1 の最入用</span>
    <span class=hljs-comment ># # (確認テスト:5-1) 微分値再利用 &amp; (確認テスト:5-2) 各微分値の計算　該当箇所 ----- ここまで</span>

    <span class=hljs-keyword >return</span> grad

<span class=hljs-comment ># サンプルデータを作成</span>
data_sets_size = <span class=hljs-number >100000</span>
data_sets = [<span class=hljs-number >0</span> <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(data_sets_size)]

<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(data_sets_size):
    data_sets[i] = {}
    <span class=hljs-comment ># ランダムな値を設定</span>
    data_sets[i][<span class=hljs-string >&#x27;x&#x27;</span>] = np.random.rand(<span class=hljs-number >2</span>)
    
    <span class=hljs-comment >## 試してみよう_入力値の設定</span>
    <span class=hljs-comment ># data_sets[i][&#x27;x&#x27;] = np.random.rand(2) * 10 -5 # -5〜5のランダム数値</span>
    
    <span class=hljs-comment ># 目標出力を設定</span>
    data_sets[i][<span class=hljs-string >&#x27;d&#x27;</span>] = f(data_sets[i][<span class=hljs-string >&#x27;x&#x27;</span>])
    
losses = []
<span class=hljs-comment ># 学習率</span>
learning_rate = <span class=hljs-number >0.07</span>

<span class=hljs-comment ># 抽出数</span>
epoch = <span class=hljs-number >1000</span>

<span class=hljs-comment ># パラメータの初期化</span>
network = init_network()
<span class=hljs-comment ># データのランダム抽出</span>
random_datasets = np.random.choice(data_sets, epoch)

<span class=hljs-comment ># 勾配降下の繰り返し</span>
<span class=hljs-keyword >for</span> dataset <span class=hljs-keyword >in</span> random_datasets:
    x, d = dataset[<span class=hljs-string >&#x27;x&#x27;</span>], dataset[<span class=hljs-string >&#x27;d&#x27;</span>]
    z1, y = forward(network, x)
    grad = backward(x, d, z1, y)
    <span class=hljs-comment ># パラメータに勾配適用</span>
    <span class=hljs-comment ># (確認テスト:4-1) 勾配降下法の更新式に対応するコード ここから</span>
    <span class=hljs-keyword >for</span> key <span class=hljs-keyword >in</span> (<span class=hljs-string >&#x27;W1&#x27;</span>, <span class=hljs-string >&#x27;W2&#x27;</span>, <span class=hljs-string >&#x27;b1&#x27;</span>, <span class=hljs-string >&#x27;b2&#x27;</span>):
        network[key]  -= learning_rate * grad[key] 
    <span class=hljs-comment ># (確認テスト:4-1) 勾配降下法の更新式に対応するコード ここまで</span>

    <span class=hljs-comment ># 誤差</span>
    loss = functions.mean_squared_error(d, y)
    losses.append(loss)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;##### 結果表示 #####&quot;</span>)    
lists = <span class=hljs-built_in >range</span>(epoch)

plt.plot(lists, losses) <span class=hljs-comment ># 散布図より直線のほうが見やすいので変更</span>
plt.show()</code></pre> <p>##### 結果表示 #####</p> <p><img src="/assets/1_3_stochastic_gradient_descent_2_1.svg" alt=svg  /></p> <h3 id="tipsディープラーニングの開発環境"><a href="#tipsディープラーニングの開発環境" class=header-anchor >Tips：ディープラーニングの開発環境</a></h3> <p>「その前に開発環境を紹介しておきたいとおもいます」 という一言から録画が始まってるのだが、「その前に」って何の前だろう..</p> <ul> <li><p>ローカルPC と クラウド環境</p> <ul> <li><p>ローカルPC &#61; 家や会社に置いてあるPC</p> <li><p>クラウド &#61; データセンター&#40;に置いてあるPC&#41;を間借り</p> <ul> <li><p>例: AWS, GCP</p> </ul> </ul> <li><p>プロセッサ</p> <ul> <li><p>各種プロセッサ &#40;一般的には下に行くほど早いが、用意するのにお金いる&#41;</p> <ul> <li><p>CPU : 汎用演算, 高クロックだが昨今頭打ち&#40;数GHz&#41;, 並列度低め&#40;数十コアくらいまで&#41;</p> <li><p>GPU : ゲーム&#40;グラフィック&#41;特化の演算がDLに流用可能、クロックはCPUよりは低いが、並列度が高い分演算が早い</p> <li><p>FPGA : 論理回路をプログラム可能 &#40;自分で適切な演算器を設計できる&#41; なため、高速な演算ユニットをつくることが可能</p> <li><p>ASIC : Google の TPUなどが具体例。完全ハードウェアロジック化したもので、FPGAよりさらに早い</p> </ul> </ul> </ul> <h3 id="tips_その他の一般的な機械学習の手法について"><a href="#tips_その他の一般的な機械学習の手法について" class=header-anchor >Tips: その他の一般的な機械学習の手法について</a></h3> <ul> <li><p>データ拡張&#40;Data Augmentation&#41;</p> <ul> <li><p>データ拡張の適用先</p> <ul> <li><p>分類タスクに有効</p> <ul> <li><p>様々なバリエーションに対応する必要がある</p> <li><p>新しいデータを作ることが容易&#40;例:回転させても同じ分類をしてほしいなど&#41;</p> </ul> <li><p>それ以外のタスクには向かないこともある</p> <ul> <li><p>データの密度分布を推定する問題には使えない</p> <ul> <li><p>真の分布を知らずに、真の分布の推定の助けになるデータをでっち上げるのは不可能</p> </ul> </ul> </ul> <li><p>データ拡張の方法</p> <ul> <li><p>入力データ改変: 問題の性質を考慮して行う</p> <ul> <li><p>オフセット, 回転など</p> </ul> <li><p>モデル内部での対応: 一般的に適用可能な方法を用いる</p> <ul> <li><p>ドロップアウト</p> <li><p>入力層・中間層へのノイズ注入 &#40;バリエーションの増加&#41; </p> </ul> </ul> <li><p>効果</p> <ul> <li><p>劇的に汎化性能が向上することがある</p> <li><p>ランダム性のあるデータ拡張を行うときは再現性に注意</p> </ul> </ul> </ul> <ul> <li><p>転移学習</p> <ul> <li><p>学習済みモデルをベースに、タスク固有の処理に対応する&quot;一部の層のみ&quot;を再学習する手法</p> <ul> <li><p>cf: ファインチューニング: 全パラメータを再学習する点が違う</p> <li><p>深層学習モデルでは 前半部が汎用的な特徴抽出部、後半部がタスク固有処理部である、というような理解に基づく</p> </ul> </ul> </ul> <h2 id="day_2_レポート"><a href="#day_2_レポート" class=header-anchor >Day 2 レポート</a></h2> <ul> <li><p>前半: 中間層をある程度増やした大きいネットワークを学習するときによく起こる問題</p> <li><p>後半: 畳み込みニューラルネットワーク</p> </ul> <h3 id="勾配消失問題"><a href="#勾配消失問題" class=header-anchor ><ol> <li><p>勾配消失問題</p> </ol> </a></h3> <ul> <li><p>誤差逆伝播法の復習</p> <ul> <li><p>割愛.</p> <li><p>&#40;確認テスト:1-1&#41; 連鎖律を使ってdz/dx を求めよ。ただし <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><msup><mi>t</mi><mn>2</mn></msup><mo separator=true >,</mo><mi>t</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>y</mi></mrow><annotation encoding="application/x-tex"> z = t^2, t = x+y</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.008548em;vertical-align:-0.19444em;"></span><span class=mord ><span class="mord mathnormal">t</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">t</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span></p> <ul> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>z</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>d</mi><mi>z</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi>t</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mn>2</mn><mi>t</mi><mover accent=true ><mn>1</mn><mo>˙</mo></mover><mo>=</mo><mn>2</mn><mo stretchy=false >(</mo><mi>x</mi><mo>+</mo><mi>y</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> \frac{d z}{dx} = \frac{d z}{dt} \frac{dt}{dx} = 2t \dot 1 = 2(x+y) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">x</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">x</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.8813em;vertical-align:0em;"></span><span class=mord >2</span><span class="mord mathnormal">t</span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8813em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=mord >1</span></span><span style="top:-3.21344em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.13889em;"><span class=mord >˙</span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >2</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mclose >)</span></span></span></span> </p> </ul> </ul> <li><p>勾配消失問題</p> <ul> <li><p>誤差逆伝播法で出力から入力に向かって計算をすすめていくにつれて勾配がどんどん緩やかになっていくことで、入力層に近い側の層のパラメータがほとんど変わらず、最適値に収束しなくなる</p> <ul> <li><p>微分値は 大きさ0~1の間をとるものが多い</p> <ul> <li><p>&#40;確認テスト:1-2&#41; sigmoid 関数の 傾きは 0~0.25 &#40;答えは &#40;2&#41; 0.25 &#41;</p> <ul> <li><p>sigmoid 関数 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> の微分 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><mi>σ</mi><mo stretchy=false >)</mo><mi>σ</mi></mrow><annotation encoding="application/x-tex"> = (1-\sigma) \sigma </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.36687em;vertical-align:0em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class=mclose >)</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span> </p> </ul> <li><p>中間層で sigmoid を使うと、1層遡るたびに 0.25よりも小さい値が掛け算されていく</p> </ul> </ul> </ul> <li><p>勾配消失問題の対策方法</p> <ol> <li><p>活性化関数の選択</p> </ol> <ul> <li><p>勾配消失が起きづらい&#40;微分値が小さくならない&#41; 活性化関数を使う</p> <li><p>例: ReLU関数 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy=false >(</mo><mi>x</mi><mo stretchy=false >)</mo><mo>=</mo><mrow><mi mathvariant=normal >m</mi><mi mathvariant=normal >a</mi><mi mathvariant=normal >x</mi></mrow><mo stretchy=false >(</mo><mn>0</mn><mo separator=true >,</mo><mi>x</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex"> f(x) = \mathrm{max}(0,x) </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">x</span><span class=mclose >)</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord ><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class=mopen >(</span><span class=mord >0</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">x</span><span class=mclose >)</span></span></span></span></p> <li><p>0 より大きいときは、微分値1 -&gt; 勾配消失問題を回避</p> <li><p>0 より小さいときは、微分値0にする -&gt; このノードを経由しての誤差は伝播しない -&gt; スパース化</p> </ul> <ol start=2 > <li><p>重みの初期化方法の工夫</p> </ol> <ul> <li><p>重みは適切に設定する必要がある</p> <ul> <li><p>&#40;確認テスト:1-3&#41; たとえばすべての重みを0に設定すると、正しい学習が行えない</p> <ul> <li><p>すべての重みの値が均一に更新され、多数の重みを有効に働くモデルが学習できない。&#40;モデルが持つ自由度を発揮でない&#41;</p> </ul> </ul> <li><p>Xavier &#40;ザビエル&#41; の初期値設定法</p> <ul> <li><p>sigmoid 等 S字カーブ系の活性化関数を使う時に有効</p> <li><p>標準正規分布&#40;平均が0, 分散が1 &#41;に従った乱数 を 前のレイヤーのノード数のルートで割る</p> <ul> <li><p>補足資料: 5つの中間層を持つNN の各層の出力のヒストグラムの観察</p> <ul> <li><p>標準正規分布で初期化した場合</p> <ul> <li><p>各層の出力のヒストグラムを見ると、0付近 と1付近に集中する -&gt; sigmoid 使った場合勾配が非常に小さくなるので、勾配消失問題が起きやすい状態</p> </ul> <li><p>標準正規分布を小さな値で割った値&#40;標準偏差を小さくする&#41;で初期化した場合</p> <ul> <li><p>各中間層の出力は 0.5 付近に集中する -&gt; 何を入れても出力が似通っている -&gt; モデルが仕事してない状態</p> </ul> <li><p>Xavier の初期化を使用した場合</p> <ul> <li><p>各層の出力は、適度に0から1の間でバラツキがあっていい感じの出力が得られていそう</p> </ul> </ul> </ul> </ul> <li><p>Heの方法</p> <ul> <li><p>ReLU 関数に対して使う</p> <li><p>正規分布を <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mfrac><mn>2</mn><mi>N</mi></mfrac></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\frac{2}{N}}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.84em;vertical-align:-0.604946em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:1.235054em;"><span class=svg-align  style="top:-3.8em;"><span class=pstrut  style="height:3.8em;"></span><span class=mord  style="padding-left:1em;"><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.195054em;"><span class=pstrut  style="height:3.8em;"></span><span class=hide-tail  style="min-width:1.02em;height:1.8800000000000001em;"><svg width='400em' height='1.8800000000000001em' viewBox='0 0 400000 1944' preserveAspectRatio='xMinYMin slice'><path d='M983 90 l0 -0 c4,-6.7,10,-10,18,-10 H400000v40 H1013.1s-83.4,268,-264.1,840c-180.7,572,-277,876.3,-289,913c-4.7,4.7,-12.7,7,-24,7 s-12,0,-12,0c-1.3,-3.3,-3.7,-11.7,-7,-25c-35.3,-125.3,-106.7,-373.3,-214,-744 c-10,12,-21,25,-33,39s-32,39,-32,39c-6,-5.3,-15,-14,-27,-26s25,-30,25,-30 c26.7,-32.7,52,-63,76,-91s52,-60,52,-60s208,722,208,722 c56,-175.3,126.3,-397.3,211,-666c84.7,-268.7,153.8,-488.2,207.5,-658.5 c53.7,-170.3,84.5,-266.8,92.5,-289.5z M1001 80h400000v40h-400000z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.604946em;"><span></span></span></span></span></span></span></span></span> </p> <ul> <li><p>補足資料: 各層の出力</p> <ul> <li><p>標準偏差が1のとき -&gt; ほとんど出力が0による</p> <li><p>標準偏差をもっと小さくした時 -&gt; やはり出力が0による</p> <li><p>He の初期化 -&gt; いい感じ</p> </ul> </ul> </ul> </ul> <ol start=3 > <li><p>バッチ正規化 &#40;batch normalization&#41;</p> <ul> <li><p>ミニバッチの単位で入力データの偏りを抑制する</p> <ul> <li><p>バッチ内データを平均0分散1に正規化する&#40;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent=true ><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.69444em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal">x</span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.22222em;"><span class=mord >^</span></span></span></span></span></span></span></span></span></span>とする&#41;</p> <li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>γ</mi><mover accent=true ><mi>x</mi><mo>^</mo></mover><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y=\gamma \hat{x} + \beta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.69444em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class="mord mathnormal">x</span></span></span><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.22222em;"><span class=mord >^</span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> &#40;<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo separator=true >,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma, \beta</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> スケール・シフトの微調整を行うパラメータとして学習される&#41;</p> </ul> <li><p>&#40;確認テスト:1-4&#41; 期待される効用</p> <ul> <li><p>収束を早める</p> <li><p>過学習が起きづらい</p> </ul> <li><p>参考: バッチサイズ</p> <ul> <li><p>画像系のタスクのとき、GPUなら1~64枚, TPUなら1~256枚程度</p> <li><p>2のべき乗をよく使うことが多い</p> </ul> </ul> </ol> </ul> <h4 id="実装演習"><a href="#実装演習" class=header-anchor >実装演習</a></h4> <h5 id="p33_例題チャレンジ"><a href="#p33_例題チャレンジ" class=header-anchor >p.33 例題チャレンジ</a></h5> <ul> <li><p>Q. 特徴データ data<em>x, ラベルデータ data</em>t に対してミニバッチを学習を行うコード例 &#40;き&#41; に当てはまるのは?</p> <li><p>A. &#40;1&#41; </p> </ul> <pre><code class="julia hljs">data_x[i:i_end], data_t[i:i_end]</code></pre>
<ul>
<li><p>こういうプログラムの虫食い問題はE資格でも結構あるらしい</p>

</ul>
<h5 id="初期化による勾配消失問題の改善"><a href="#初期化による勾配消失問題の改善" class=header-anchor >初期化による勾配消失問題の改善 </a></h5>
<pre><code class="python hljs"><span class=hljs-keyword >import</span> sys
sys.path.append(<span class=hljs-string >&#x27;/.&#x27;</span>)</code></pre></p>
<pre><code class="python hljs"><span class=hljs-comment ># MLP クラス</span>
<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> layers
<span class=hljs-keyword >from</span> collections <span class=hljs-keyword >import</span> OrderedDict
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> functions
<span class=hljs-keyword >from</span> data.mnist <span class=hljs-keyword >import</span> load_mnist
<span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt


<span class=hljs-keyword >class</span> <span class="hljs-title class_">MultiLayerNet</span>:
    <span class=hljs-string >&#x27;&#x27;&#x27;
    input_size: 入力層のノード数
    hidden_size_list: 隠れ層のノード数のリスト
    output_size: 出力層のノード数
    activation: 活性化関数
    weight_init_std: 重みの初期化方法
    &#x27;&#x27;&#x27;</span>
    <span class=hljs-keyword >def</span> __init__(self, input_size, hidden_size_list, output_size, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;relu&#x27;</span>):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size_list = hidden_size_list
        self.hidden_layer_num = <span class=hljs-built_in >len</span>(hidden_size_list)
        self.params = {}

        <span class=hljs-comment ># 重みの初期化</span>
        self.__init_weight(weight_init_std)

        <span class=hljs-comment ># レイヤの生成, sigmoidとreluのみ扱う</span>
        activation_layer = {<span class=hljs-string >&#x27;sigmoid&#x27;</span>: layers.Sigmoid, <span class=hljs-string >&#x27;relu&#x27;</span>: layers.Relu}
        self.layers = OrderedDict() <span class=hljs-comment ># 追加した順番に格納</span>
        <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, self.hidden_layer_num+<span class=hljs-number >1</span>):
            self.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = layers.Affine(self.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)], self.params[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)])
            self.layers[<span class=hljs-string >&#x27;Activation_function&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = activation_layer[activation]()

        idx = self.hidden_layer_num + <span class=hljs-number >1</span>
        self.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = layers.Affine(self.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)], self.params[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)])

        self.last_layer = layers.SoftmaxWithLoss()

    <span class=hljs-keyword >def</span> __init_weight(self, weight_init_std):
        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]
        <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, <span class=hljs-built_in >len</span>(all_size_list)):
            scale = weight_init_std
            <span class=hljs-keyword >if</span> <span class=hljs-built_in >str</span>(weight_init_std).lower() <span class=hljs-keyword >in</span> (<span class=hljs-string >&#x27;relu&#x27;</span>, <span class=hljs-string >&#x27;he&#x27;</span>):
                scale = np.sqrt(<span class=hljs-number >2.0</span> / all_size_list[idx - <span class=hljs-number >1</span>])
            <span class=hljs-keyword >elif</span> <span class=hljs-built_in >str</span>(weight_init_std).lower() <span class=hljs-keyword >in</span> (<span class=hljs-string >&#x27;sigmoid&#x27;</span>, <span class=hljs-string >&#x27;xavier&#x27;</span>):
                scale = np.sqrt(<span class=hljs-number >1.0</span> / all_size_list[idx - <span class=hljs-number >1</span>])

            self.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = scale * np.random.randn(all_size_list[idx-<span class=hljs-number >1</span>], all_size_list[idx])
            self.params[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = np.zeros(all_size_list[idx])

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">predict</span>(<span class=hljs-params >self, x</span>):
        <span class=hljs-keyword >for</span> layer <span class=hljs-keyword >in</span> self.layers.values():
            x = layer.forward(x)

        <span class=hljs-keyword >return</span> x

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">loss</span>(<span class=hljs-params >self, x, d</span>):
        y = self.predict(x)

        weight_decay = <span class=hljs-number >0</span>
        <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, self.hidden_layer_num + <span class=hljs-number >2</span>):
            W = self.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]

        <span class=hljs-keyword >return</span> self.last_layer.forward(y, d) + weight_decay

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">accuracy</span>(<span class=hljs-params >self, x, d</span>):
        y = self.predict(x)
        y = np.argmax(y, axis=<span class=hljs-number >1</span>)
        <span class=hljs-keyword >if</span> d.ndim != <span class=hljs-number >1</span> : d = np.argmax(d, axis=<span class=hljs-number >1</span>)

        accuracy = np.<span class=hljs-built_in >sum</span>(y == d) / <span class=hljs-built_in >float</span>(x.shape[<span class=hljs-number >0</span>])
        <span class=hljs-keyword >return</span> accuracy

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">gradient</span>(<span class=hljs-params >self, x, d</span>):
        <span class=hljs-comment ># forward</span>
        self.loss(x, d)

        <span class=hljs-comment ># backward</span>
        dout = <span class=hljs-number >1</span>
        dout = self.last_layer.backward(dout)

        layers = <span class=hljs-built_in >list</span>(self.layers.values())
        layers.reverse()
        <span class=hljs-keyword >for</span> layer <span class=hljs-keyword >in</span> layers:
            dout = layer.backward(dout)

        <span class=hljs-comment ># 設定</span>
        grad = {}
        <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, self.hidden_layer_num+<span class=hljs-number >2</span>):
            grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = self.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].dW
            grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = self.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].db

        <span class=hljs-keyword >return</span> grad</code></pre>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">learn_and_plot</span>(<span class=hljs-params >network, iters_num=<span class=hljs-number >2000</span></span>):

    <span class=hljs-comment ># データの読み込み</span>
    (x_train, d_train), (x_test, d_test) = load_mnist(normalize=<span class=hljs-literal >True</span>, one_hot_label=<span class=hljs-literal >True</span>)

    <span class=hljs-comment >#iters_num = 2000</span>
    train_size = x_train.shape[<span class=hljs-number >0</span>]
    batch_size = <span class=hljs-number >100</span>
    learning_rate = <span class=hljs-number >0.1</span>

    train_loss_list = []
    accuracies_train = []
    accuracies_test = []

    plot_interval=<span class=hljs-number >10</span>

    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(iters_num):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        d_batch = d_train[batch_mask]

        <span class=hljs-comment ># 勾配</span>
        grad = network.gradient(x_batch, d_batch)
        
        <span class=hljs-keyword >for</span> key <span class=hljs-keyword >in</span> (<span class=hljs-string >&#x27;W1&#x27;</span>, <span class=hljs-string >&#x27;W2&#x27;</span>, <span class=hljs-string >&#x27;W3&#x27;</span>, <span class=hljs-string >&#x27;b1&#x27;</span>, <span class=hljs-string >&#x27;b2&#x27;</span>, <span class=hljs-string >&#x27;b3&#x27;</span>):
            network.params[key] -= learning_rate * grad[key]
        
        loss = network.loss(x_batch, d_batch)
        train_loss_list.append(loss)
        
        <span class=hljs-keyword >if</span> (i + <span class=hljs-number >1</span>) % plot_interval == <span class=hljs-number >0</span>:
            accr_test = network.accuracy(x_test, d_test)
            accuracies_test.append(accr_test)        
            accr_train = network.accuracy(x_batch, d_batch)
            accuracies_train.append(accr_train)
            <span class=hljs-comment ># print(&#x27;Generation: &#x27; + str(i+1) + &#x27;. 正答率(トレーニング) = &#x27; + str(accr_train))</span>
            <span class=hljs-comment ># print(&#x27;                : &#x27; + str(i+1) + &#x27;. 正答率(テスト) = &#x27; + str(accr_test))</span>
            

    lists = <span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>, iters_num, plot_interval)
    plt.plot(lists, accuracies_train, label=<span class=hljs-string >&quot;training set&quot;</span>)
    plt.plot(lists, accuracies_test,  label=<span class=hljs-string >&quot;test set&quot;</span>)
    plt.legend(loc=<span class=hljs-string >&quot;lower right&quot;</span>)
    plt.title(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.xlabel(<span class=hljs-string >&quot;count&quot;</span>)
    plt.ylabel(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.ylim(<span class=hljs-number >0</span>, <span class=hljs-number >1.0</span>)
    <span class=hljs-comment ># グラフの表示</span>
    plt.show()</code></pre>
<pre><code class="python hljs"><span class=hljs-comment ># 活性化関数 と 初期化関数の組み合わせ</span>
hidden_size_list= [<span class=hljs-number >40</span>,<span class=hljs-number >20</span>]

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;----------------- Sigmoid&quot;</span>)
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: gauss&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-number >0.01</span>))

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: Xavier&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;Xavier&#x27;</span>))

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: He&quot;</span>) <span class=hljs-comment ># try</span>
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;He&#x27;</span>))

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;----------------- Relu&quot;</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: gauss&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-number >0.01</span>))

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: Xavier&quot;</span>) <span class=hljs-comment ># try </span>
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;Xavier&#x27;</span>))

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: He&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;He&#x27;</span>))</code></pre>
<p>----------------- Sigmoid
initializer: gauss</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_3_1.svg" alt=svg  /></p>
<p>initializer: Xavier</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_3_3.svg" alt=svg  /></p>
<p>initializer: He</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_3_5.svg" alt=svg  /></p>
<p>----------------- Relu
initializer: gauss</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_3_7.svg" alt=svg  /></p>
<p>initializer: Xavier</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_3_9.svg" alt=svg  /></p>
<p>initializer: He</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_3_11.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-comment ># 活性化関数 と 初期化関数の組み合わせ</span>
hidden_size_list= [<span class=hljs-number >40</span>,<span class=hljs-number >30</span>,<span class=hljs-number >20</span>]

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;----------------- Sigmoid&quot;</span>)
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: gauss&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-number >0.01</span>),iters_num=<span class=hljs-number >4000</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: Xavier&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;Xavier&#x27;</span>),iters_num=<span class=hljs-number >4000</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: He&quot;</span>) <span class=hljs-comment ># try</span>
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;He&#x27;</span>),iters_num=<span class=hljs-number >4000</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;----------------- Relu&quot;</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: gauss&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-number >0.01</span>),iters_num=<span class=hljs-number >4000</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: Xavier&quot;</span>) <span class=hljs-comment ># try </span>
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;Xavier&#x27;</span>),iters_num=<span class=hljs-number >4000</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;initializer: He&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=hidden_size_list, output_size=<span class=hljs-number >10</span>, activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;He&#x27;</span>),iters_num=<span class=hljs-number >4000</span>)</code></pre>
<p>----------------- Sigmoid
initializer: gauss</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_4_1.svg" alt=svg  /></p>
<p>initializer: Xavier</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_4_3.svg" alt=svg  /></p>
<p>initializer: He</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_4_5.svg" alt=svg  /></p>
<p>----------------- Relu
initializer: gauss</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_4_7.svg" alt=svg  /></p>
<p>initializer: Xavier</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_4_9.svg" alt=svg  /></p>
<p>initializer: He</p>
<p><img src="/assets/2_2_2_vanishing_gradient_modified_files/2_2_2_vanishing_gradient_modified_4_11.svg" alt=svg  />
<ul>
<li><p>とにかく ReLU の効果が大きい</p>

<li><p>ReLu &#43; Xavier より 気持ち ReLU &#43; He のほうが早い? </p>

</ul>
<h5 id="バッチ正規化"><a href="#バッチ正規化" class=header-anchor >バッチ正規化</a></h5>
<pre><code class="python hljs"><span class=hljs-keyword >import</span> sys
sys.path.append(<span class=hljs-string >&#x27;.&#x27;</span>)</code></pre></p>
<pre><code class="python hljs"><span class=hljs-comment >## バッチ正則化 layer の定義</span>
<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> collections <span class=hljs-keyword >import</span> OrderedDict
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> layers
<span class=hljs-keyword >from</span> data.mnist <span class=hljs-keyword >import</span> load_mnist
<span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt
<span class=hljs-keyword >from</span> multi_layer_net <span class=hljs-keyword >import</span> MultiLayerNet
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> optimizer

<span class=hljs-comment ># バッチ正則化 layer</span>
<span class=hljs-keyword >class</span> <span class="hljs-title class_">BatchNormalization</span>:
    <span class=hljs-string >&#x27;&#x27;&#x27;
    gamma: スケール係数
    beta: オフセット
    momentum: 慣性
    running_mean: テスト時に使用する平均
    running_var: テスト時に使用する分散
    &#x27;&#x27;&#x27;</span>
    <span class=hljs-keyword >def</span> __init__(self, gamma, beta, momentum=<span class=hljs-number >0.9</span>, running_mean=<span class=hljs-literal >None</span>, running_var=<span class=hljs-literal >None</span>):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = <span class=hljs-literal >None</span>

        self.running_mean = running_mean
        self.running_var = running_var  
        
        <span class=hljs-comment ># backward時に使用する中間データ</span>
        self.batch_size = <span class=hljs-literal >None</span>
        self.xc = <span class=hljs-literal >None</span>
        self.std = <span class=hljs-literal >None</span>
        self.dgamma = <span class=hljs-literal >None</span>
        self.dbeta = <span class=hljs-literal >None</span>

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >self, x, train_flg=<span class=hljs-literal >True</span></span>):
        <span class=hljs-keyword >if</span> self.running_mean <span class=hljs-keyword >is</span> <span class=hljs-literal >None</span>:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)
        <span class=hljs-comment >#### batch normalization (バッチ正規化に該当する箇所)　ここから-----------------                </span>
        <span class=hljs-keyword >if</span> train_flg:
            mu = x.mean(axis=<span class=hljs-number >0</span>) <span class=hljs-comment ># 平均</span>
            xc = x - mu <span class=hljs-comment ># xをセンタリング</span>
            var = np.mean(xc**<span class=hljs-number >2</span>, axis=<span class=hljs-number >0</span>) <span class=hljs-comment ># 分散</span>
            std = np.sqrt(var + <span class=hljs-number >10e-7</span>) <span class=hljs-comment ># スケーリング</span>
            xn = xc / std
            
            self.batch_size = x.shape[<span class=hljs-number >0</span>]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (<span class=hljs-number >1</span>-self.momentum) * mu <span class=hljs-comment ># 平均値の加重平均</span>
            self.running_var = self.momentum * self.running_var + (<span class=hljs-number >1</span>-self.momentum) * var <span class=hljs-comment >#分散値の加重平均</span>
        <span class=hljs-keyword >else</span>:
            xc = x - self.running_mean
            xn = xc / ((np.sqrt(self.running_var + <span class=hljs-number >10e-7</span>)))
            
        out = self.gamma * xn + self.beta 
        
        <span class=hljs-keyword >return</span> out

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">backward</span>(<span class=hljs-params >self, dout</span>):
        dbeta = dout.<span class=hljs-built_in >sum</span>(axis=<span class=hljs-number >0</span>)
        dgamma = np.<span class=hljs-built_in >sum</span>(self.xn * dout, axis=<span class=hljs-number >0</span>)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.<span class=hljs-built_in >sum</span>((dxn * self.xc) / (self.std * self.std), axis=<span class=hljs-number >0</span>)
        dvar = <span class=hljs-number >0.5</span> * dstd / self.std
        dxc += (<span class=hljs-number >2.0</span> / self.batch_size) * self.xc * dvar
        dmu = np.<span class=hljs-built_in >sum</span>(dxc, axis=<span class=hljs-number >0</span>)
        dx = dxc - dmu / self.batch_size
        
        self.dgamma = dgamma
        self.dbeta = dbeta

        <span class=hljs-keyword >return</span> dx</code></pre>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">learn_and_plot</span>(<span class=hljs-params >network, iters_num=<span class=hljs-number >1000</span></span>):
    (x_train, d_train), (x_test, d_test) = load_mnist(normalize=<span class=hljs-literal >True</span>)

    <span class=hljs-comment >#iters_num = 1000</span>
    train_size = x_train.shape[<span class=hljs-number >0</span>]
    batch_size = <span class=hljs-number >100</span>
    learning_rate=<span class=hljs-number >0.01</span>

    train_loss_list = []
    accuracies_train = []
    accuracies_test = []

    plot_interval=<span class=hljs-number >10</span>


    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(iters_num):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        d_batch = d_train[batch_mask]

        grad = network.gradient(x_batch, d_batch)
        <span class=hljs-keyword >for</span> key <span class=hljs-keyword >in</span> (<span class=hljs-string >&#x27;W1&#x27;</span>, <span class=hljs-string >&#x27;W2&#x27;</span>, <span class=hljs-string >&#x27;W3&#x27;</span>, <span class=hljs-string >&#x27;b1&#x27;</span>, <span class=hljs-string >&#x27;b2&#x27;</span>, <span class=hljs-string >&#x27;b3&#x27;</span>):
            network.params[key] -= learning_rate * grad[key]

            loss = network.loss(x_batch, d_batch)
            train_loss_list.append(loss)        
            
        <span class=hljs-keyword >if</span> (i + <span class=hljs-number >1</span>) % plot_interval == <span class=hljs-number >0</span>:
            accr_test = network.accuracy(x_test, d_test)
            accuracies_test.append(accr_test)        
            accr_train = network.accuracy(x_batch, d_batch)
            accuracies_train.append(accr_train)
            
            <span class=hljs-comment ># print(&#x27;Generation: &#x27; + str(i+1) + &#x27;. 正答率(トレーニング) = &#x27; + str(accr_train))</span>
            <span class=hljs-comment ># print(&#x27;                : &#x27; + str(i+1) + &#x27;. 正答率(テスト) = &#x27; + str(accr_test))</span>
                    

    lists = <span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>, iters_num, plot_interval)
    plt.plot(lists, accuracies_train, label=<span class=hljs-string >&quot;training set&quot;</span>)
    plt.plot(lists, accuracies_test,  label=<span class=hljs-string >&quot;test set&quot;</span>)
    plt.legend(loc=<span class=hljs-string >&quot;lower right&quot;</span>)
    plt.title(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.xlabel(<span class=hljs-string >&quot;count&quot;</span>)
    plt.ylabel(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.ylim(<span class=hljs-number >0</span>, <span class=hljs-number >1.0</span>)
    <span class=hljs-comment ># グラフの表示</span>
    plt.show()</code></pre>
<pre><code class="python hljs"><span class=hljs-comment >## 元々あった状態</span>
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>,
                        activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;Xavier&#x27;</span>, use_batchnorm=<span class=hljs-literal >True</span>))</code></pre>
<p>データ読み込み完了</p>
<p><img src="/assets/2_3_batch_normalization_files/2_3_batch_normalization_3_1.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-comment >## gauss 初期化にしたり、層の数を増やしたりして勾配消失問題が起きやすい状況をつくってみる</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;gauss 初期化で収束しにくくする&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>,
                        activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-number >0.01</span>, use_batchnorm=<span class=hljs-literal >True</span>))
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;さらに層を増やして勾配消失問題も起きやすくする&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >30</span> ,<span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>,
                        activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-number >0.01</span>, use_batchnorm=<span class=hljs-literal >True</span>))</code></pre>
<p>gauss 初期化で収束しにくくする</p>
<p><img src="/assets/2_3_batch_normalization_files/2_3_batch_normalization_4_1.svg" alt=svg  /></p>
<p>さらに層を増やして勾配消失問題も起きやすくする</p>
<p><img src="/assets/2_3_batch_normalization_files/2_3_batch_normalization_4_3.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;He,relu,bn&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >30</span> ,<span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>,
                        activation=<span class=hljs-string >&#x27;relu&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;He&#x27;</span>, use_batchnorm=<span class=hljs-literal >True</span>))

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;xavier,sigmoid,bn&quot;</span>)
learn_and_plot(MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >30</span> ,<span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>,
                        activation=<span class=hljs-string >&#x27;sigmoid&#x27;</span>, weight_init_std=<span class=hljs-string >&#x27;Xavier&#x27;</span>, use_batchnorm=<span class=hljs-literal >True</span>))</code></pre>
<p>He,relu,bn</p>
<p><img src="/assets/2_3_batch_normalization_files/2_3_batch_normalization_5_1.svg" alt=svg  /></p>
<p>xavier,sigmoid,bn</p>
<p><img src="/assets/2_3_batch_normalization_files/2_3_batch_normalization_5_3.svg" alt=svg  />
<ul>
<li><p>バッチ正則化の効果で sigmoid &#43; gauss 初期化でもなんとか更新は進んでいるようす &#40;でもReLUにした方が効果大きい&#41;</p>

<li><p>この実験のみで判断できないが、He &#43; ReLU などで適切・十分に収束していそうなケースでは、BN入れない方が早いのかも?</p>

</ul>
<h3 id="ol_start2_学習率最適化手法"><a href="#ol_start2_学習率最適化手法" class=header-anchor ><ol start=2 >
<li><p>学習率最適化手法</p>

</ol>
</a></h3>
<ul>
<li><p>Optimizer 動作概要</p>
<ul>
<li><p>学習の初期の段階では、学習率大きめに設定し、徐々に学習率を小さくしていく</p>

<li><p>パラメータ毎に学習率を可変にする</p>
<ul>
<li><p>パラメータ毎に学習率が変わるということは、パラメータ更新のベクトルの方向自体が変わるということ. &quot;学習率最適化&quot;という言葉はしっくりこないなぁ..</p>

</ul>

</ul>

<li><p>代表的な手法</p>
<ul>
<li><p>モメンタム</p>
<ul>
<li><p>更新式</p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup><mo>+</mo><msub><mi>V</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex"> \bm{w}^{(t+1)} = \bm{w}^{(t)} + V_t </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li><p>パラメータ更新量は <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">V_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>

</ul>

<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub><mo>=</mo><mi>μ</mi><msub><mi>V</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mi>ϵ</mi><mi mathvariant=normal >∇</mi><mi>E</mi></mrow><annotation encoding="application/x-tex"> V_t = \mu V_{t-1} - \epsilon \nabla E</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathnormal">μ</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.208331em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span></p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">V_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> は、勾配降下法のパラメータ更新量 <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi>ϵ</mi><mi mathvariant=normal >∇</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">-\epsilon \nabla E</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.76666em;vertical-align:-0.08333em;"></span><span class=mord >−</span><span class="mord mathnormal">ϵ</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span> を1次IIRをつかって Recursive average をとったもの</p>

</ul>

</ul>

<li><p>効用</p>
<ul>
<li><p>ささいな局所最適解につかまりにくい</p>

<li><p>谷間についてから、最低値付近に収束するまでの時間が早い &#40;しかし最終的にピタッととまりにくい&#41;</p>

</ul>

</ul>

<li><p>AdaGrad</p>
<ul>
<li><p>更新式</p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup><mo>−</mo><mi>ϵ</mi><mfrac><mn>1</mn><mrow><msqrt><msub><mi>h</mi><mi>t</mi></msub></msqrt><mo>+</mo><mi>θ</mi></mrow></mfrac><mi mathvariant=normal >∇</mi><mi>E</mi></mrow><annotation encoding="application/x-tex"> \bm{w}^{(t+1)} = \bm{w}^{(t)} - \epsilon \frac{1}{\sqrt{h_t} + \theta }\nabla E </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.383108em;vertical-align:-0.5379999999999999em;"></span><span class="mord mathnormal">ϵ</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.845108em;"><span style="top:-2.5835585em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.866345em;"><span class=svg-align  style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.826345em;"><span class=pstrut  style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.173655em;"><span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.5379999999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span></p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> が大きいほど遅くなるよね</p>

</ul>

<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy=false >(</mo><mi mathvariant=normal >∇</mi><mi>E</mi><msup><mo stretchy=false >)</mo><mn>2</mn></msup><mo separator=true >,</mo><msub><mi>h</mi><mn>0</mn></msub><mo>=</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex"> h_t = h_{t-1} + (\nabla E)^2 , h_0 = \theta </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.902771em;vertical-align:-0.208331em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.208331em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.064108em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mclose ><span class=mclose >)</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></p>
<ul>
<li><p>最初の方は、<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >∣</mi><mi mathvariant=normal >∇</mi><mi>E</mi><mi mathvariant=normal >∣</mi></mrow><annotation encoding="application/x-tex">|\nabla E| </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >∣</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mord >∣</span></span></span></span> で ノーマライズするような効果なので、斜面の緩急によらず速やかに収束していきそう</p>

<li><p>毎回二乗で足していくので、<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> は単調増加して、どんどんステップサイズは小さくなる。</p>

</ul>

</ul>

<li><p>効用</p>
<ul>
<li><p>勾配の緩やかな斜面で最適値に近づいていきやすい&#40;逆に急な斜面は不得意&#41;</p>

<li><p>鞍点問題 &#40;SGDよりはマシなようだが..&#41;</p>

<li><p>ハイパパラメータの調整が難しい</p>

</ul>

</ul>

<li><p>RMSProp</p>
<ul>
<li><p>鞍点問題対策を施したAdaGradの改良版</p>

<li><p>更新式</p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup><mo>−</mo><mi>ϵ</mi><mfrac><mn>1</mn><mrow><msqrt><msub><mi>h</mi><mi>t</mi></msub></msqrt><mo>+</mo><mi>θ</mi></mrow></mfrac><mi mathvariant=normal >∇</mi><mi>E</mi></mrow><annotation encoding="application/x-tex"> \bm{w}^{(t+1)} = \bm{w}^{(t)} - \epsilon \frac{1}{\sqrt{h_t} + \theta }\nabla E </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.383108em;vertical-align:-0.5379999999999999em;"></span><span class="mord mathnormal">ϵ</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.845108em;"><span style="top:-2.5835585em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.866345em;"><span class=svg-align  style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.826345em;"><span class=pstrut  style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.173655em;"><span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.5379999999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span></p>
<ul>
<li><p>AdaGrad と同じ</p>

</ul>

<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>α</mi><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=false >)</mo><mo stretchy=false >(</mo><mi mathvariant=normal >∇</mi><mi>E</mi><msup><mo stretchy=false >)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> h_t = \alpha h_{t-1} + (1-\alpha) (\nabla E)^2</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.208331em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mclose >)</span><span class=mopen >(</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mclose ><span class=mclose >)</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> の設定は AdaGrad と同じなのだろうか? </p>

<li><p>AdaGradで 単調増加していた <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> が、単純に一個前との按分をとる形になった</p>

</ul>

</ul>

<li><p>効用</p>
<ul>
<li><p>AdaGradと比較して より帯域的最適解を見つけやすく、ハイパラメータの調整の難易度も下がった</p>

</ul>

</ul>

<li><p>Adam </p>
<ul>
<li><p>モメンタムと、RMSProp のいいとこ取り版</p>

<li><p>更新式 &#40;なぜ資料にのっていないのか?&#33;&#41; <a href="https://qiita.com/omiita/items/1735c1d048fe5f611f80">参考</a></p>
<ul>
<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=false >)</mo></mrow></msup><mo>=</mo><msup><mi mathvariant=bold-italic >w</mi><mrow><mo stretchy=false >(</mo><mi>t</mi><mo stretchy=false >)</mo></mrow></msup><mo>−</mo><mi>ϵ</mi><mfrac><msub><mi>V</mi><mi>t</mi></msub><mrow><msqrt><msub><mi>h</mi><mi>t</mi></msub></msqrt><mo>+</mo><mi>θ</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \bm{w}^{(t+1)} = \bm{w}^{(t)} - \epsilon \frac{V_t}{\sqrt{h_t} + \theta} </annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8879999999999999em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.9713299999999999em;vertical-align:-0.08333em;"></span><span class=mord ><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.02778em;">w</span></span></span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8879999999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.4264309999999998em;vertical-align:-0.5379999999999999em;"></span><span class="mord mathnormal">ϵ</span><span class=mord ><span class="mopen nulldelimiter"></span><span class=mfrac ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8884309999999999em;"><span style="top:-2.5835585em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.866345em;"><span class=svg-align  style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.826345em;"><span class=pstrut  style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.173655em;"><span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3.23em;"><span class=pstrut  style="height:3em;"></span><span class=frac-line  style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class=pstrut  style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.22222em;margin-right:0.07142857142857144em;"><span class=pstrut  style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.5379999999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>

<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub><mo>=</mo><mi>μ</mi><msub><mi>V</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><mi>μ</mi><mo stretchy=false >)</mo><mi mathvariant=normal >∇</mi><mi>E</mi></mrow><annotation encoding="application/x-tex"> V_t = \mu V_{t-1} + (1-\mu) \nabla E</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.83333em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathnormal">μ</span><span class=mord ><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.208331em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">μ</span><span class=mclose >)</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span></p>

<li><p><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>α</mi><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy=false >(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=false >)</mo><mo stretchy=false >(</mo><mi mathvariant=normal >∇</mi><mi>E</mi><msup><mo stretchy=false >)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> h_t = \alpha h_{t-1} + (1-\alpha) (\nabla E)^2</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.84444em;vertical-align:-0.15em;"></span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2777777777777778em;"></span><span class=mrel >=</span><span class=mspace  style="margin-right:0.2777777777777778em;"></span></span><span class=base ><span class=strut  style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mord ><span class="mord mathnormal">h</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.208331em;"><span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >−</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class=mclose >)</span><span class=mopen >(</span><span class=mord >∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class=mclose ><span class=mclose >)</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></p>

</ul>

</ul>

</ul>

</ul>
<h4 id="実装演習__2"><a href="#実装演習__2" class=header-anchor >実装演習</a></h4>
<ul>
<li><p>Momentum をもとに AdaGrad の実装を行う</p>
<ul>
<li><p>講師の方、Momentum のコードが書かれているのに、AdaGrad の解説してて、コード見てないのがモロバレですよ。</p>

</ul>

<li><p>Try パート</p>
<ul>
<li><p>学習率増やしたら、Momenutum, AdaGrad で収束するようになったが、RMSProp, AdamがNGに,,</p>

<li><p>Relu, He にすると各手法よくなる</p>

<li><p>Batch Normalization を入れても良くなるが 活性化関数/初期化手法の方がより安定的に収束してそう?</p>

</ul>

</ul>
<pre><code class="python hljs"><span class=hljs-keyword >import</span> sys
sys.path.append(<span class=hljs-string >&#x27;.&#x27;</span>)

<span class=hljs-keyword >import</span> sys, os
<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> collections <span class=hljs-keyword >import</span> OrderedDict
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> layers
<span class=hljs-keyword >from</span> data.mnist <span class=hljs-keyword >import</span> load_mnist
<span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt
<span class=hljs-keyword >from</span> multi_layer_net <span class=hljs-keyword >import</span> MultiLayerNet</code></pre></p>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">learn_and_plot</span>(<span class=hljs-params >network, optimizer=<span class=hljs-string >&quot;sgd&quot;</span>, iters_num=<span class=hljs-number >1000</span>, learning_rate=<span class=hljs-number >0.01</span></span>):
    <span class=hljs-comment ># データの読み込み</span>
    (x_train, d_train), (x_test, d_test) = load_mnist(normalize=<span class=hljs-literal >True</span>, one_hot_label=<span class=hljs-literal >True</span>)


    train_size = x_train.shape[<span class=hljs-number >0</span>]
    batch_size = <span class=hljs-number >100</span>
    <span class=hljs-comment ># momentum </span>
    momentum = <span class=hljs-number >0.9</span>
    <span class=hljs-comment ># adagrad</span>
    theta = <span class=hljs-number >1e-4</span>
    <span class=hljs-comment ># rmsprop</span>
    decay_rate = <span class=hljs-number >0.99</span>
    <span class=hljs-comment ># adam</span>
    beta1 = <span class=hljs-number >0.9</span>
    beta2 = <span class=hljs-number >0.999</span>

    train_loss_list = []
    accuracies_train = []
    accuracies_test = []

    plot_interval=<span class=hljs-number >10</span>

    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(iters_num):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        d_batch = d_train[batch_mask]

        <span class=hljs-comment ># 勾配</span>
        grad = network.gradient(x_batch, d_batch)
        
        <span class=hljs-keyword >if</span> i == <span class=hljs-number >0</span>:
            <span class=hljs-keyword >if</span> optimizer == <span class=hljs-string >&quot;momentum&quot;</span>:
                v = {} 
            <span class=hljs-keyword >elif</span> optimizer == <span class=hljs-string >&quot;adagrad&quot;</span> <span class=hljs-keyword >or</span> optimizer == <span class=hljs-string >&quot;rmsprop&quot;</span> :
                h = {}
            <span class=hljs-keyword >elif</span> optimizer == <span class=hljs-string >&quot;adam&quot;</span>:
                m = {}
                v = {} 
        

        <span class=hljs-keyword >for</span> key <span class=hljs-keyword >in</span> (<span class=hljs-string >&#x27;W1&#x27;</span>, <span class=hljs-string >&#x27;W2&#x27;</span>, <span class=hljs-string >&#x27;W3&#x27;</span>, <span class=hljs-string >&#x27;b1&#x27;</span>, <span class=hljs-string >&#x27;b2&#x27;</span>, <span class=hljs-string >&#x27;b3&#x27;</span>):
            <span class=hljs-keyword >if</span> optimizer == <span class=hljs-string >&quot;sgd&quot;</span>:
                network.params[key] -= learning_rate * grad[key]
            <span class=hljs-keyword >elif</span> optimizer == <span class=hljs-string >&quot;momentum&quot;</span>:
                <span class=hljs-keyword >if</span> i == <span class=hljs-number >0</span>:
                    v[key] = np.zeros_like(network.params[key]) 
                v[key] = momentum * v[key] - learning_rate * grad[key]
                network.params[key] += v[key]
            <span class=hljs-keyword >elif</span> optimizer == <span class=hljs-string >&quot;adagrad&quot;</span>:
                <span class=hljs-keyword >if</span> i == <span class=hljs-number >0</span>:
                    h[key] = np.full_like(network.params[key],theta)
                h[key] = h[key] + np.square(grad[key]) 
                network.params[key] -= learning_rate / np.sqrt(h[key]) * grad[key]
            <span class=hljs-keyword >elif</span> optimizer == <span class=hljs-string >&quot;rmsprop&quot;</span>:
                <span class=hljs-keyword >if</span> i == <span class=hljs-number >0</span>:
                    h[key] = np.zeros_like(network.params[key])
                h[key] *= decay_rate
                h[key] += (<span class=hljs-number >1</span>-decay_rate) * np.square(grad[key]) 
                network.params[key] -= learning_rate * grad[key] / (np.sqrt(h[key]) + <span class=hljs-number >1e-7</span>) 
            <span class=hljs-keyword >elif</span> optimizer == <span class=hljs-string >&quot;adam&quot;</span>:
                <span class=hljs-keyword >if</span> i == <span class=hljs-number >0</span>:
                    m[key] = np.zeros_like(network.params[key])
                    v[key] = np.zeros_like(network.params[key])

                m[key] += (<span class=hljs-number >1</span>-beta1) * (grad[key] - m[key])
                v[key] += (<span class=hljs-number >1</span>-beta2) * (grad[key]**<span class=hljs-number >2</span> - v[key])

                network.params[key] -= learning_rate * m[key] / (np.sqrt(v[key]) + <span class=hljs-number >1e-7</span>) 

            loss = network.loss(x_batch, d_batch)
            train_loss_list.append(loss)
        
        
        <span class=hljs-keyword >if</span> (i + <span class=hljs-number >1</span>) % plot_interval == <span class=hljs-number >0</span>:
            accr_test = network.accuracy(x_test, d_test)
            accuracies_test.append(accr_test)        
            accr_train = network.accuracy(x_batch, d_batch)
            accuracies_train.append(accr_train)
            
            <span class=hljs-comment ># print(&#x27;Generation: &#x27; + str(i+1) + &#x27;. 正答率(トレーニング) = &#x27; + str(accr_train))</span>
            <span class=hljs-comment ># print(&#x27;                : &#x27; + str(i+1) + &#x27;. 正答率(テスト) = &#x27; + str(accr_test))</span>

            
    lists = <span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>, iters_num, plot_interval)
    plt.plot(lists, accuracies_train, label=<span class=hljs-string >&quot;training set&quot;</span>)
    plt.plot(lists, accuracies_test,  label=<span class=hljs-string >&quot;test set&quot;</span>)
    plt.legend(loc=<span class=hljs-string >&quot;lower right&quot;</span>)
    plt.title(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.xlabel(<span class=hljs-string >&quot;count&quot;</span>)
    plt.ylabel(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.ylim(<span class=hljs-number >0</span>, <span class=hljs-number >1.0</span>)
    <span class=hljs-comment ># グラフの表示</span>
    plt.show()</code></pre>
<pre><code class="python hljs"><span class=hljs-comment ># default 状態</span>
iters_num = <span class=hljs-number >1000</span>
activation = <span class=hljs-string >&quot;sigmoid&quot;</span>
initiazlier = <span class=hljs-number >0.01</span>
use_batchnorm = <span class=hljs-literal >False</span>

optimizer_list = [<span class=hljs-string >&quot;sgd&quot;</span>, <span class=hljs-string >&quot;momentum&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;rmsprop&quot;</span>, <span class=hljs-string >&quot;adam&quot;</span>]

<span class=hljs-keyword >for</span> optimizer <span class=hljs-keyword >in</span> optimizer_list:
    <span class=hljs-built_in >print</span>(optimizer)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>, activation=activation, weight_init_std=initiazlier,
                        use_batchnorm=use_batchnorm)
    learn_and_plot(network,optimizer, iters_num)</code></pre>
<p>sgd</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_2_1.svg" alt=svg  /></p>
<p>momentum</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_2_3.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_2_5.svg" alt=svg  /></p>
<p>rmsprop</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_2_7.svg" alt=svg  /></p>
<p>adam</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_2_9.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-comment ># 学習率を増やしてみる 0.01 -&gt; 0.1 </span>
iters_num = <span class=hljs-number >1000</span>
activation = <span class=hljs-string >&quot;sigmoid&quot;</span>
learning_rate = <span class=hljs-number >0.1</span>
initiazlier = <span class=hljs-number >0.01</span>
use_batchnorm = <span class=hljs-literal >False</span>

optimizer_list = [<span class=hljs-string >&quot;sgd&quot;</span>, <span class=hljs-string >&quot;momentum&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;rmsprop&quot;</span>, <span class=hljs-string >&quot;adam&quot;</span>]

<span class=hljs-keyword >for</span> optimizer <span class=hljs-keyword >in</span> optimizer_list:
    <span class=hljs-built_in >print</span>(optimizer)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>, activation=activation, weight_init_std=initiazlier,
                        use_batchnorm=use_batchnorm)
    learn_and_plot(network,optimizer, iters_num,learning_rate)</code></pre>
<p>sgd</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_3_1.svg" alt=svg  /></p>
<p>momentum</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_3_3.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_3_5.svg" alt=svg  /></p>
<p>rmsprop</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_3_7.svg" alt=svg  /></p>
<p>adam</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_3_9.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-comment ># デフォルト状態から 活性化関数を</span>
iters_num = <span class=hljs-number >1000</span>
activation = <span class=hljs-string >&quot;relu&quot;</span>
initiazlier = <span class=hljs-number >0.01</span>
use_batchnorm = <span class=hljs-literal >False</span>

optimizer_list = [<span class=hljs-string >&quot;sgd&quot;</span>, <span class=hljs-string >&quot;momentum&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;rmsprop&quot;</span>, <span class=hljs-string >&quot;adam&quot;</span>]

<span class=hljs-keyword >for</span> optimizer <span class=hljs-keyword >in</span> optimizer_list:
    <span class=hljs-built_in >print</span>(optimizer)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>, activation=activation, weight_init_std=initiazlier,
                        use_batchnorm=use_batchnorm)
    learn_and_plot(network,optimizer, iters_num)</code></pre>
<p>sgd</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_4_1.svg" alt=svg  /></p>
<p>momentum</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_4_3.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_4_5.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_4_7.svg" alt=svg  /></p>
<p>rmsprop</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_4_9.svg" alt=svg  /></p>
<p>adam</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_4_11.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-comment ># さらに初期化を He へ</span>
iters_num = <span class=hljs-number >1000</span>
activation = <span class=hljs-string >&quot;relu&quot;</span>
initiazlier = <span class=hljs-string >&quot;He&quot;</span>
use_batchnorm = <span class=hljs-literal >False</span>

optimizer_list = [<span class=hljs-string >&quot;sgd&quot;</span>, <span class=hljs-string >&quot;momentum&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;rmsprop&quot;</span>, <span class=hljs-string >&quot;adam&quot;</span>]

<span class=hljs-keyword >for</span> optimizer <span class=hljs-keyword >in</span> optimizer_list:
    <span class=hljs-built_in >print</span>(optimizer)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>, activation=activation, weight_init_std=initiazlier,
                        use_batchnorm=use_batchnorm)
    learn_and_plot(network,optimizer, iters_num)</code></pre>
<p>sgd</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_5_1.svg" alt=svg  /></p>
<p>momentum</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_5_3.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_5_5.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_5_7.svg" alt=svg  /></p>
<p>rmsprop</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_5_9.svg" alt=svg  /></p>
<p>adam</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_5_11.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-comment ># すでにひとつ前の状態でどの手法でもある程度収束しているので、</span>
<span class=hljs-comment ># BNいれても変化が見つけづらそう? -&gt; 活性化関数・初期化手法を戻して BN入れて見る</span>
iters_num = <span class=hljs-number >1000</span>
activation = <span class=hljs-string >&quot;sigmoid&quot;</span>
initiazlier = <span class=hljs-number >0.01</span>
use_batchnorm = <span class=hljs-literal >True</span>

optimizer_list = [<span class=hljs-string >&quot;sgd&quot;</span>, <span class=hljs-string >&quot;momentum&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;adagrad&quot;</span>, <span class=hljs-string >&quot;rmsprop&quot;</span>, <span class=hljs-string >&quot;adam&quot;</span>]

<span class=hljs-keyword >for</span> optimizer <span class=hljs-keyword >in</span> optimizer_list:
    <span class=hljs-built_in >print</span>(optimizer)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >40</span>, <span class=hljs-number >20</span>], output_size=<span class=hljs-number >10</span>, activation=activation, weight_init_std=initiazlier,
                        use_batchnorm=use_batchnorm)
    learn_and_plot(network,optimizer, iters_num)</code></pre>
<p>sgd</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_6_1.svg" alt=svg  /></p>
<p>momentum</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_6_3.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_6_5.svg" alt=svg  /></p>
<p>adagrad</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_6_7.svg" alt=svg  /></p>
<p>rmsprop</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_6_9.svg" alt=svg  /></p>
<p>adam</p>
<p><img src="/assets/2_4_optimizer_files/2_4_optimizer_6_11.svg" alt=svg  /></p>
<p>
<h3 id="ol_start3_過学習"><a href="#ol_start3_過学習" class=header-anchor ><ol start=3 >
<li><p>過学習</p>

</ol>
</a></h3>
<ul>
<li><p>Day1 の 13_過学習 の動画の内容</p>
<ul>
<li><p>機械学習の項でやった内容なので、詳細割愛</p>

<li><p>過学習はパラメータ数の大きい巨大なモデルだと容易に起こる</p>

</ul>

</ul>
<ul>
<li><p>過学習</p>
<ul>
<li><p>データに対してはうまくいくが、テストデータに対してうまくいかない減少</p>
<ul>
<li><p>特定の訓練サンプルに特化した振る舞いを学習されてしまっている</p>

<li><p>訓練誤差は下がるが、テスト誤差が下がらない&#40;悪化する&#41;</p>

</ul>

<li><p>原因</p>
<ul>
<li><p>訓練データの次元数・サンプル数に対して、ネットワークの自由度が高すぎる</p>

</ul>

<li><p>対策</p>
<ul>
<li><p>early stopping</p>
<ul>
<li><p>学習曲線をプロットして訓練誤差とテスト誤差が乖離し始める前のエポックのパラメータを採用する</p>

</ul>

<li><p>正則化</p>
<ul>
<li><p>着眼点: 過学習のときは特定のサンプルの特徴が過大評価されているため重みが大きくなりがち</p>
<ul>
<li><p>-&gt; 重みの大きさを下げるような制約をモデルに課せばよい</p>

</ul>

<li><p>具体例</p>
<ul>
<li><p>L1正則化 &#40;Lasso回帰&#41;</p>
<ul>
<li><p>重みベクトルの1ノルム&#40;マンハッタン距離&#41; の総和を正則化項として目的関数に足す</p>
<ul>
<li><p>スパース化</p>

<li><p><img src="/assets/lasso1.jpg" alt=lasso  /></p>

</ul>

</ul>

<li><p>L2正則化 &#40;Ridge回帰&#41;</p>
<ul>
<li><p>重みベクトルの2ノルム&#40;ユークリッド距離&#41; の総和を正則化項として目的関数に足す</p>
<ul>
<li><p>全体として原点に近い位置に目的関数の谷底をシフト</p>

<li><p><img src="/assets/ridge1.jpg" alt=ridge  /></p>

</ul>

</ul>

</ul>

<li><p>&#40;確認テスト:3-1&#41; Lassoは下図右側</p>
<ul>
<li><p><img src="/assets/lasso_and_ridge_edited.jpg" alt=lasso_and_ridge  /></p>

</ul>

</ul>

<li><p>ドロップアウト</p>
<ul>
<li><p>学習のパラメータ更新のたびに、ランダムにノードを削除する</p>

<li><p>データにバリエーションを与えているようなもの</p>

</ul>

</ul>

</ul>

</ul>
<h4 id="実装演習__3"><a href="#実装演習__3" class=header-anchor >実装演習</a></h4>
<h5 id="例題チャレンジ"><a href="#例題チャレンジ" class=header-anchor >例題チャレンジ</a></h5>
<p>L2正則化の勾配計算</p>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">ridge</span>(<span class=hljs-params >param, grad, rate</span>):
  <span class=hljs-string >&quot;&quot;&quot;
  param: target parameter
  grad: gradients to param
  rate: ridge coefficient
  &quot;&quot;&quot;</span>
  grad += rate * param <span class=hljs-comment ># &lt;- ココ (4)</span></code></pre>
<p>L1正則化の勾配計算            </p>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">lasso</span>(<span class=hljs-params >param, grad, rate</span>):
  <span class=hljs-string >&quot;&quot;&quot;
  param: target parameter
  grad: gradients to param
  rate: lasso coefficient
  &quot;&quot;&quot;</span>    
  x = sign(param) <span class=hljs-comment ># &lt;-ココ (3)</span>
  grad += rate * x</code></pre>
<h5 id=overfitting ><a href="#overfitting" class=header-anchor >overfitting  </a></h5>
<pre><code class="python hljs"><span class=hljs-keyword >import</span> sys
sys.path.append(<span class=hljs-string >&#x27;.&#x27;</span>)

<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> collections <span class=hljs-keyword >import</span> OrderedDict
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> layers
<span class=hljs-keyword >from</span> data.mnist <span class=hljs-keyword >import</span> load_mnist
<span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt
<span class=hljs-keyword >from</span> multi_layer_net <span class=hljs-keyword >import</span> MultiLayerNet
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> optimizer</code></pre></p>
<p>パラメータを変えてなんども試すので、関数化しておく。</p>
<ul>
<li><p>regulation 引数で、正則化 なし、L1,L2 を切り替える</p>

<li><p>正則化ありのパラメータ更新は勉強のために、オリジナルのコードと同じくベタで書いている &#40;OptimizerはSDG前提のパラメータ更新&#41;</p>

<li><p>network 生成時の 引数を変えることで、dropout true/false, ratio 他も変更可能</p>

</ul>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">train</span>(<span class=hljs-params >network, optimizer_obj, learning_rate, weight_decay_lambda = <span class=hljs-number >0.0</span>, regulation=<span class=hljs-literal >None</span>,  iters_num = <span class=hljs-number >1000</span> </span>):
    (x_train, d_train), (x_test, d_test) = load_mnist(normalize=<span class=hljs-literal >True</span>)
    <span class=hljs-comment ># print(&quot;データ読み込み完了&quot;)</span>

    <span class=hljs-comment ># 過学習を再現するために、学習データを削減</span>
    x_train = x_train[:<span class=hljs-number >300</span>]
    d_train = d_train[:<span class=hljs-number >300</span>]

    <span class=hljs-comment >#iters_num = 1000</span>
    train_size = x_train.shape[<span class=hljs-number >0</span>]
    batch_size = <span class=hljs-number >100</span>

    train_loss_list = []
    accuracies_train = []
    accuracies_test = []

    plot_interval=<span class=hljs-number >10</span>

    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(iters_num):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        d_batch = d_train[batch_mask]

        grad = network.gradient(x_batch, d_batch)
        loss = <span class=hljs-number >0</span>
        <span class=hljs-keyword >if</span> regulation == <span class=hljs-literal >None</span>:
            optimizer_obj.update(network.params, grad)
        <span class=hljs-keyword >elif</span> regulation == <span class=hljs-string >&#x27;L2&#x27;</span>:
            weight_decay = <span class=hljs-number >0</span>
            <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, network.hidden_layer_num+<span class=hljs-number >1</span>):
                grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = network.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].dW + weight_decay_lambda * network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]
                grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = network.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].db
                network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] -= learning_rate * grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]
                network.params[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] -= learning_rate * grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]        
                <span class=hljs-comment ># weight_decay += 0.5 * weight_decay_lambda * np.sqrt(np.sum(network.params[&#x27;W&#x27; + str(idx)] ** 2)) # こうかかれていたが..</span>
                weight_decay += <span class=hljs-number >0.5</span> * weight_decay_lambda * np.<span class=hljs-built_in >sum</span>(network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] ** <span class=hljs-number >2</span>) <span class=hljs-comment ># こっちじゃないか?</span>
            loss = weight_decay 
        <span class=hljs-keyword >elif</span> regulation == <span class=hljs-string >&#x27;L1&#x27;</span>:
            weight_decay = <span class=hljs-number >0</span>
            <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, network.hidden_layer_num+<span class=hljs-number >1</span>):
                grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = network.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].dW + weight_decay_lambda * np.sign(network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)])
                grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = network.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].db
                network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] -= learning_rate * grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]
                network.params[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] -= learning_rate * grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]        
                weight_decay += weight_decay_lambda * np.<span class=hljs-built_in >sum</span>(np.<span class=hljs-built_in >abs</span>(network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]))

            loss = weight_decay            
        
        loss = loss + network.loss(x_batch, d_batch)
        train_loss_list.append(loss)
            
        <span class=hljs-keyword >if</span> (i+<span class=hljs-number >1</span>) % plot_interval == <span class=hljs-number >0</span>:
            accr_train = network.accuracy(x_train, d_train)
            accr_test = network.accuracy(x_test, d_test)
            accuracies_train.append(accr_train)
            accuracies_test.append(accr_test)

            <span class=hljs-comment ># print(&#x27;Generation: &#x27; + str(i+1) + &#x27;. 正答率(トレーニング) = &#x27; + str(accr_train))</span>
            <span class=hljs-comment ># print(&#x27;                : &#x27; + str(i+1) + &#x27;. 正答率(テスト) = &#x27; + str(accr_test))        </span>

    lists = <span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>, iters_num, plot_interval)
    plt.plot(lists, accuracies_train, label=<span class=hljs-string >&quot;training set&quot;</span>)
    plt.plot(lists, accuracies_test,  label=<span class=hljs-string >&quot;test set&quot;</span>)
    plt.legend(loc=<span class=hljs-string >&quot;lower right&quot;</span>)
    plt.title(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.xlabel(<span class=hljs-string >&quot;count&quot;</span>)
    plt.ylabel(<span class=hljs-string >&quot;accuracy&quot;</span>)
    plt.ylim(<span class=hljs-number >0</span>, <span class=hljs-number >1.0</span>)
    <span class=hljs-comment ># グラフの表示</span>
    plt.show()</code></pre>
<p>基準となる 正則化無し, Dropout無し, SDGの場合は訓練データの正解率100&#37;に対して 検証データの正解率75&#37;程度。かなり結果に差があり過学習が起きていると思われる</p>
<pre><code class="python hljs"><span class=hljs-comment ># 正則化無し, Dropout 無し</span>
learning_rate = <span class=hljs-number >0.01</span>
network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>)
optimizer_obj = optimizer.SGD(learning_rate=learning_rate)

train(network,optimizer_obj,learning_rate)</code></pre>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_4_0.svg" alt=svg  /></p>
<p>L2正則化の重みを変えてみると、0.01では小さすぎ&#40;正則化無しとほぼ変わらない&#41;、1.0 は大きすぎ。 その間で探すと、そこそこの結果っぽくはなるが、これでいいのかはちょっとわからない。</p>
<pre><code class="python hljs"><span class=hljs-comment ># L2 </span>

lambda_list = [<span class=hljs-number >0.01</span>, <span class=hljs-number >0.08</span>, <span class=hljs-number >1.0</span>]
learning_rate = <span class=hljs-number >0.01</span>
regulation = <span class=hljs-string >&#x27;L2&#x27;</span>
iters_num = <span class=hljs-number >1000</span>

<span class=hljs-keyword >for</span> weight_decay_lambda <span class=hljs-keyword >in</span> lambda_list:
    <span class=hljs-built_in >print</span>(weight_decay_lambda)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>)
    optimizer_obj = optimizer.SGD(learning_rate=learning_rate)
    train(network,optimizer_obj,learning_rate,weight_decay_lambda, regulation,iters_num=iters_num)</code></pre>
<p>0.01</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_6_1.svg" alt=svg  /></p>
<p>0.08</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_6_3.svg" alt=svg  /></p>
<p>1.0</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_6_5.svg" alt=svg  /></p>
<p>L1正則化。与えられたコードでは、学習率が0.1 だったが、L2正則化と合わせて 0.01にした。 元のloss関数も、学習に使うデータも変わらないのだし、正則化項とのバランスは weight<em>decay</em>lambda 使えばいいので、 そちらのほうが、振舞いなども比較しやすいだろう。</p>
<p>L2正則化と似たような結果が得られる。講義動画でグラフがバタついてたのは L1正則化を使っていたこと「だけ」が原因でなく、その時の学習率が大きすぎて必要以上に0重みが増えたり減ったりしていたからでは? </p>
<pre><code class="python hljs"><span class=hljs-comment ># L1</span>
lambda_list = [<span class=hljs-number >0.0005</span>, <span class=hljs-number >0.008</span>, <span class=hljs-number >0.05</span>]
learning_rate = <span class=hljs-number >0.01</span>
regulation = <span class=hljs-string >&#x27;L1&#x27;</span>
iters_num = <span class=hljs-number >1000</span>

<span class=hljs-keyword >for</span> weight_decay_lambda <span class=hljs-keyword >in</span> lambda_list:
    <span class=hljs-built_in >print</span>(weight_decay_lambda)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>)
    optimizer_obj = optimizer.SGD(learning_rate=learning_rate)
    train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation, iters_num)</code></pre>
<p>0.0005</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_8_1.svg" alt=svg  /></p>
<p>0.008</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_8_3.svg" alt=svg  /></p>
<p>0.05</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_8_5.svg" alt=svg  /></p>
<pre><code class="python hljs"><span class=hljs-keyword >class</span> <span class="hljs-title class_">Dropout</span>:
    <span class=hljs-keyword >def</span> __init__(self, dropout_ratio=<span class=hljs-number >0.5</span>):
        self.dropout_ratio = dropout_ratio
        self.mask = <span class=hljs-literal >None</span>

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >self, x, train_flg=<span class=hljs-literal >True</span></span>):
        <span class=hljs-keyword >if</span> train_flg:
            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio
            <span class=hljs-keyword >return</span> x * self.mask
        <span class=hljs-keyword >else</span>:
            <span class=hljs-keyword >return</span> x * (<span class=hljs-number >1.0</span> - self.dropout_ratio) <span class=hljs-comment ># ランダム要素をいれない分、学習時に有効なノードの割合をかけて辻褄合わせる?</span>

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">backward</span>(<span class=hljs-params >self, dout</span>):
        <span class=hljs-keyword >return</span> dout * self.mask</code></pre>
<p>Dropout. SGD で、正則化無。</p>
<p>講義動画では、「うまくいっている」風なことが言われていたが、イタレーションを増やすと、ただ収束がゆっくりになっただけで、訓練データは正解率100&#37; に到達する。 最終的には過学習してしまっているのではないか？という気もするが、たとえば、dropout ratio 0.2 のケースで、early stopping を前提にすると、検証データの正解率がサチリかけたときに、訓練データの正解率がまだ、100&#37;に達しておらず、ワンチャン絶妙なモデルが得られるかもしれない?</p>
<pre><code class="python hljs"><span class=hljs-comment ># common settings </span>
use_dropout = <span class=hljs-literal >True</span>
weight_decay_lambda = <span class=hljs-number >0</span>
regulation = <span class=hljs-literal >None</span>
learning_rate = <span class=hljs-number >0.01</span>
dropout_ratio_list = [<span class=hljs-number >0.15</span>, <span class=hljs-number >0.2</span>, <span class=hljs-number >0.3</span>, <span class=hljs-number >0.4</span>]
iters_num_list    = [<span class=hljs-number >3000</span>, <span class=hljs-number >4000</span>, <span class=hljs-number >5000</span>, <span class=hljs-number >6000</span>]

<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-built_in >len</span>(dropout_ratio_list)):
    dropout_ratio = dropout_ratio_list[i]
    <span class=hljs-built_in >print</span>(dropout_ratio)
    iters_num = iters_num_list[i]
    optimizer_obj = optimizer.SGD(learning_rate=learning_rate)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                            use_dropout = use_dropout, dropout_ratio = dropout_ratio)
    train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)</code></pre>
<p>0.15</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_11_1.svg" alt=svg  /></p>
<p>0.2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_11_3.svg" alt=svg  /></p>
<p>0.3</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_11_5.svg" alt=svg  /></p>
<p>0.4</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_11_7.svg" alt=svg  /></p>
<p>dropout ratio 0, 0.15, 0.2 で各種 optimizer を試してみる。正則化はとりあえずなし そもそもdroptout 無しの状態で、SGDと比べて、収束早い &amp; 検証データの成果率が若干高い。 Droptout有りのときもあまり収束がSlowDownしないし、訓練データはすぐに正解率100&#37;になる。これをどう捉えるべきか?</p>
<pre><code class="python hljs"><span class=hljs-comment ># common settings </span>
use_dropout = <span class=hljs-literal >True</span>
weight_decay_lambda = <span class=hljs-number >0</span>
regulation = <span class=hljs-literal >None</span>
learning_rate = <span class=hljs-number >0.01</span>
dropout_ratio_list = [<span class=hljs-number >0</span>, <span class=hljs-number >0.15</span>, <span class=hljs-number >0.2</span>]
iters_num_list    = [<span class=hljs-number >1000</span>, <span class=hljs-number >3000</span>, <span class=hljs-number >4000</span>]

<span class=hljs-comment ># -------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Momentum+Droptout&quot;</span>)
<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-built_in >len</span>(dropout_ratio_list)):
    dropout_ratio = dropout_ratio_list[i]
    <span class=hljs-built_in >print</span>(dropout_ratio)
    iters_num = iters_num_list[i]
    optimizer_obj = optimizer.Momentum(learning_rate=learning_rate,momentum=<span class=hljs-number >0.9</span>)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                            use_dropout = use_dropout, dropout_ratio = dropout_ratio)
    train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)
<span class=hljs-comment ># -------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;AdaGrad+Droptout&quot;</span>)
<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-built_in >len</span>(dropout_ratio_list)):
    dropout_ratio = dropout_ratio_list[i]
    <span class=hljs-built_in >print</span>(dropout_ratio)
    iters_num = iters_num_list[i]
    optimizer_obj = optimizer.AdaGrad(learning_rate=learning_rate)
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                            use_dropout = use_dropout, dropout_ratio = dropout_ratio)
    train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)
<span class=hljs-comment ># -------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Adam+Droptout&quot;</span>)
<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-built_in >len</span>(dropout_ratio_list)):
    dropout_ratio = dropout_ratio_list[i]
    <span class=hljs-built_in >print</span>(dropout_ratio)
    iters_num = iters_num_list[i]
    optimizer_obj = optimizer.Adam()
    network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                            use_dropout = use_dropout, dropout_ratio = dropout_ratio)
    train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)</code></pre>
<p>Momentum&#43;Droptout
0</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_1.svg" alt=svg  /></p>
<p>0.15</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_3.svg" alt=svg  /></p>
<p>0.2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_5.svg" alt=svg  /></p>
<p>AdaGrad&#43;Droptout
0</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_7.svg" alt=svg  /></p>
<p>0.15</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_9.svg" alt=svg  /></p>
<p>0.2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_11.svg" alt=svg  /></p>
<p>Adam&#43;Droptout
0</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_13.svg" alt=svg  /></p>
<p>0.15</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_15.svg" alt=svg  /></p>
<p>0.2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_13_17.svg" alt=svg  /></p>
<p>Dropout &#40;ratio 0.1&#41; &#43; L2 正則化。 最初に実装したtrain関数は、正則化を行う時のパラメータ更新のコードがSGD前提になってしまっている。 各種Optimizer 全部ためすのは面倒だなぁと思っていたが、multi<em>layer</em>net.py のコードを見ると、 実はnetwork生成時に、weight decay lambda の数値を与えると、コスト関数に 重み係数の二乗和が追加されるので、L2正則化っぽいことが試せる. </p>
<p>&#40;ちなみに、デフォルトのコードでは、Droptouのコード例で、このweight<em>decay</em>lambda が network 生成時に与えられているので、何も考えずに頭から実行した場合は、実は、L1正則化単体の実験で使われていた、weight<em>decay</em>lambda &#61; 0.005 が使われることになるのだが、コードを準備した方の意図通りだろうか?&#41;</p>
<pre><code class="python hljs"><span class=hljs-comment ># common settings </span>
use_dropout = <span class=hljs-literal >True</span>
weight_decay_lambda = <span class=hljs-number >0.05</span>
regulation = <span class=hljs-literal >None</span>
learning_rate = <span class=hljs-number >0.01</span>
dropout_ratio = <span class=hljs-number >0.08</span>
iters_num    = <span class=hljs-number >2000</span>

<span class=hljs-comment >#------------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;SGD+Droptout+L2&quot;</span>)
optimizer_obj = optimizer.SGD(learning_rate=learning_rate)
network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                        weight_decay_lambda=weight_decay_lambda,use_dropout = use_dropout, dropout_ratio = dropout_ratio)
train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)

<span class=hljs-comment ># -------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Momentum+Droptout+L2&quot;</span>)
optimizer_obj = optimizer.Momentum(learning_rate=learning_rate, momentum=<span class=hljs-number >0.9</span>)
network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                        weight_decay_lambda=weight_decay_lambda,  use_dropout = use_dropout, dropout_ratio = dropout_ratio)
train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)

<span class=hljs-comment ># -------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;AdaGrad+Droptout+L2&quot;</span>)
optimizer_obj = optimizer.AdaGrad(learning_rate=learning_rate)
network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                        weight_decay_lambda=weight_decay_lambda, use_dropout = use_dropout, dropout_ratio = dropout_ratio)
train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)
<span class=hljs-comment ># -------------------------------------------------------</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Adam+Droptout+L2&quot;</span>)
optimizer_obj = optimizer.Adam()
network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                        weight_decay_lambda=weight_decay_lambda, use_dropout = use_dropout, dropout_ratio = dropout_ratio)
train(network, optimizer_obj, learning_rate, weight_decay_lambda, regulation,iters_num)</code></pre>
<p>SGD&#43;Droptout&#43;L2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_15_1.svg" alt=svg  /></p>
<p>Momentum&#43;Droptout&#43;L2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_15_3.svg" alt=svg  /></p>
<p>AdaGrad&#43;Droptout&#43;L2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_15_5.svg" alt=svg  /></p>
<p>Adam&#43;Droptout&#43;L2</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_15_7.svg" alt=svg  /></p>
<p>SGDで Dropout &#43; L1 正則化。反復回数だけ増やして実行。 1000回まではよさげにみえたが、その後は、訓練データは微増に対して、検証データは微減。このケースの場合は 1000回ちょっと手前くらいで止めておこうということになるのかな?</p>
<pre><code class="python hljs"><span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> optimizer
(x_train, d_train), (x_test, d_test) = load_mnist(normalize=<span class=hljs-literal >True</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;データ読み込み完了&quot;</span>)

<span class=hljs-comment ># 過学習を再現するために、学習データを削減</span>
x_train = x_train[:<span class=hljs-number >300</span>]
d_train = d_train[:<span class=hljs-number >300</span>]

<span class=hljs-comment ># ドロップアウト設定 ======================================</span>
use_dropout = <span class=hljs-literal >True</span>
dropout_ratio = <span class=hljs-number >0.08</span>
<span class=hljs-comment ># ====================================================</span>

network = MultiLayerNet(input_size=<span class=hljs-number >784</span>, hidden_size_list=[<span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>, <span class=hljs-number >100</span>], output_size=<span class=hljs-number >10</span>,
                        use_dropout = use_dropout, dropout_ratio = dropout_ratio)

iters_num = <span class=hljs-number >3000</span>
train_size = x_train.shape[<span class=hljs-number >0</span>]
batch_size = <span class=hljs-number >100</span>
learning_rate=<span class=hljs-number >0.01</span>

train_loss_list = []
accuracies_train = []
accuracies_test = []
hidden_layer_num = network.hidden_layer_num

plot_interval=<span class=hljs-number >10</span>

<span class=hljs-comment ># 正則化強度設定 ======================================</span>
weight_decay_lambda=<span class=hljs-number >0.004</span>
<span class=hljs-comment ># =================================================</span>

<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    d_batch = d_train[batch_mask]

    grad = network.gradient(x_batch, d_batch)
    weight_decay = <span class=hljs-number >0</span>
    
    <span class=hljs-keyword >for</span> idx <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-number >1</span>, hidden_layer_num+<span class=hljs-number >1</span>):
        grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = network.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].dW + weight_decay_lambda * np.sign(network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)])
        grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] = network.layers[<span class=hljs-string >&#x27;Affine&#x27;</span> + <span class=hljs-built_in >str</span>(idx)].db
        network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] -= learning_rate * grad[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]
        network.params[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)] -= learning_rate * grad[<span class=hljs-string >&#x27;b&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]        
        weight_decay += weight_decay_lambda * np.<span class=hljs-built_in >sum</span>(np.<span class=hljs-built_in >abs</span>(network.params[<span class=hljs-string >&#x27;W&#x27;</span> + <span class=hljs-built_in >str</span>(idx)]))

    loss = network.loss(x_batch, d_batch) + weight_decay
    train_loss_list.append(loss)        
        
    <span class=hljs-keyword >if</span> (i+<span class=hljs-number >1</span>) % plot_interval == <span class=hljs-number >0</span>:
        accr_train = network.accuracy(x_train, d_train)
        accr_test = network.accuracy(x_test, d_test)
        accuracies_train.append(accr_train)
        accuracies_test.append(accr_test)
        
        <span class=hljs-comment ># print(&#x27;Generation: &#x27; + str(i+1) + &#x27;. 正答率(トレーニング) = &#x27; + str(accr_train))</span>
        <span class=hljs-comment ># print(&#x27;                : &#x27; + str(i+1) + &#x27;. 正答率(テスト) = &#x27; + str(accr_test))               </span>
        
lists = <span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>, iters_num, plot_interval)
plt.plot(lists, accuracies_train, label=<span class=hljs-string >&quot;training set&quot;</span>)
plt.plot(lists, accuracies_test,  label=<span class=hljs-string >&quot;test set&quot;</span>)
plt.legend(loc=<span class=hljs-string >&quot;lower right&quot;</span>)
plt.title(<span class=hljs-string >&quot;accuracy&quot;</span>)
plt.xlabel(<span class=hljs-string >&quot;count&quot;</span>)
plt.ylabel(<span class=hljs-string >&quot;accuracy&quot;</span>)
plt.ylim(<span class=hljs-number >0</span>, <span class=hljs-number >1.0</span>)
<span class=hljs-comment ># グラフの表示</span>
plt.show()</code></pre>
<p>データ読み込み完了</p>
<p><img src="/assets/2_5_overfiting_files/2_5_overfiting_17_1.svg" alt=svg  /></p>
<p>
<h3 id="ol_start4_畳み込みニューラルネットワークcnnの概念"><a href="#ol_start4_畳み込みニューラルネットワークcnnの概念" class=header-anchor ><ol start=4 >
<li><p>畳み込みニューラルネットワーク&#40;CNN&#41;の概念</p>

</ol>
</a></h3>
<ul>
<li><p>CNN で扱うデータ</p>
<ul>
<li><p>Day1 &quot;15<em>CNNで扱えるデータの種類&quot;  &amp; Day2 &quot;21</em>CNN -構造1&quot; の動画 共通の内容 </p>

<li><p>CNNで扱うと有効なデータ &#61; 次元間でのつながりに意味があるデータ</p>
<ul>
<li><p>例: </p>
<ul>
<li><p>単一チャンネル 音声&#40;1次元&#41;、スペクトログラム画像&#40;2次元&#41;,　CTスキャン画像&#40;3次元&#41;</p>

<li><p>複数チャンネル アニメのスケルトン&#40;1次元&#41;, カラー画像&#40;2次元&#41;,  動画&#40;3次元&#41;</p>

</ul>

</ul>

</ul>

</ul>
<ul>
<li><p>CNN の構造</p>
<ul>
<li><p>全体の把握: LeNet を例に 各層におけるデータサイズに着目して概要ざっくり説明</p>
<ul>
<li><p>入力 : 32 x 32 &#61; 1024 の白黒画像 </p>

<li><p>特徴量の抽出: 次元のつながりを保つ</p>

<li><p>C1: 28x28x6 &#61; 4704 &#40;6人の人に28x28のサイズで感想を聞いた&#41;</p>

<li><p>S2: 14x14x6 &#61; 1176 &#40;6人の感想それぞれを 14x14 に要約&#41;</p>

<li><p>C3: 10x10x16 &#61; 1600 &#40;要約された6人の感想を16人の人に読んでもらって、それぞれに感想を言ってもらう&#41;</p>

<li><p>S4:  5x 5x16 &#61; 400  &#40;16人それぞれの感想を 5x5のサイズに要約&#41;</p>

<li><p>欲しい形式のデータに変換:  </p>

<li><p>C5~ : 全結合層</p>

<li><p>出力 : 10次元ベクトル&#40;分類クラスのホットエンコーディングかな?&#41;</p>

</ul>

<li><p>畳み込み層 &#40;Convolution Layer&#41; の演算</p>
<ul>
<li><p>画像の場合, 縦、横, チャンネル の3次元データに対して行う</p>

<li><p>畳み込み演算 </p>
<ul>
<li><p>フィルター処理:  &#40;重み&#41; を使った畳み込み計算 &#40;数式上は相関計算&#41;</p>
<ul>
<li><p>&#40;動画では言及がなかったが&#41;　入力チャンネル数分だけ別々のフィルタを書けて、フィルタ処理後の値は加算する. </p>

</ul>

<li><p>バイアス: フィルター処理の出力の各画素に、バイアスを足す</p>

<li><p>パディング: フィルター処理まえに上下左右に n ピクセルずつ広げて、それを入力画像として使う</p>
<ul>
<li><p>畳み込み層を重ねるたびに、画像サイズがどんどん小さくなるのを防ぐ</p>

</ul>

<li><p>ストライド: フィルタ処理の際のホップサイズ</p>

<li><p>プーリング層: フィルタ処理のように演算対象範囲をずらしながら、演算を行う</p>
<ul>
<li><p>Max Pooling　: 最大値をとってくる</p>

<li><p>Average Pooling : 範囲内の値の平均をとる</p>

</ul>

<li><p>&#40;出力&#41;チャンネル : 上記までに説明された処理を、同じ入力に対して別々のフィルタを使って行う</p>

</ul>

<li><p>&#40;確認テスト:4-1&#41; 6x6の画像に2x2のフィルタを畳み込む、ストライド・パディングは1, 出力サイズは?</p>
<ul>
<li><p>answer : 7x7 </p>
<ul>
<li><p>パディング後のサイズ &#40;size 6&#41; &#43; &#40;padding 1&#41; * 2 &#61; 8</p>

<li><p>一番右端にフィルタをかける時、フィルタの左端の位置は  8 - &#40; &#40;kernel size 2&#41; -1&#41; &#61; 7</p>

<li><p>ストライド 1 なので、 7/ &#40;stride 1&#41; &#61; 7</p>

<li><p>dilation があるともうちょっと複雑</p>

</ul>

<li><p>疑問点: </p>
<ul>
<li><p>ストライド 2 以上にしたとき、割り切れない場合はどうするの?</p>

<li><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">PyTorchの実装ではマニュアル</a> みると切り落としっぽい。</p>

</ul>

</ul>

</ul>

</ul>

</ul>
<ul>
<li><p>畳み込み処理の実装演習</p>
<ul>
<li><p>フィルタ処理のメモリアクセスは効率が悪いので、dot積でフィルタ処理ができるようなデータに並び替える&#40;冗長になる&#41;</p>
<ul>
<li><p>実装演習の image2col のパート参照</p>

<li><p>kernelsize 2 なら、2x2&#61;4 つのデータを一列&#40;or一行&#41;に kernelsize 3 なら 3x3&#61;9つのデータを一列に。</p>

</ul>

</ul>

</ul>
<h4 id="実装演習_畳み込みレイヤーの実装例"><a href="#実装演習_畳み込みレイヤーの実装例" class=header-anchor >実装演習: 畳み込みレイヤーの実装例</a></h4>
<ul>
<li><p>im2col : forward 計算で使用。上述の通り 畳み込み処理を np.dot&#40;&#41; を使って効率よく行うために、入力データとフィルタの変換を行う </p>

<li><p>col2im : backward 計算で 誤差逆伝播を計算するときにつかう。基本的にはim2col の逆変換のようなことをするのだが、入力のある画素のデータに起因した誤差が、col形式では、各列に分散されているので、それらを足し合わせる必要がある。</p>

</ul>
<pre><code class="python hljs"><span class=hljs-keyword >import</span> pickle
<span class=hljs-keyword >import</span> numpy <span class=hljs-keyword >as</span> np
<span class=hljs-keyword >from</span> collections <span class=hljs-keyword >import</span> OrderedDict
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> layers
<span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> optimizer
<span class=hljs-keyword >from</span> data.mnist <span class=hljs-keyword >import</span> load_mnist
<span class=hljs-keyword >import</span> matplotlib.pyplot <span class=hljs-keyword >as</span> plt

<span class=hljs-keyword >import</span> sys
sys.path.append(<span class=hljs-string >&#x27;.&#x27;</span>)</code></pre></p>
<p>効率よく行列計算をするためのデータ変換</p>
<p>im2col &lt;-&gt; col2im の実装</p>
<pre><code class="python hljs"><span class=hljs-string >&#x27;&#x27;&#x27;
input_data/col : 畳み込み層への入力データ / 変換後データ
filter_h: フィルターの高さ
filter_w: フィルターの横幅
stride: ストライド
pad: パディング
&#x27;&#x27;&#x27;</span>
<span class=hljs-comment ># 画像データを２次元配列に変換</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">im2col</span>(<span class=hljs-params >input_data, filter_h, filter_w, stride=<span class=hljs-number >1</span>, pad=<span class=hljs-number >0</span></span>):
    <span class=hljs-comment ># N: number, C: channel, H: height, W: width</span>
    N, C, H, W = input_data.shape
    out_h = (H + <span class=hljs-number >2</span> * pad - filter_h)//stride + <span class=hljs-number >1</span>
    out_w = (W + <span class=hljs-number >2</span> * pad - filter_w)//stride + <span class=hljs-number >1</span>

    img = np.pad(input_data, [(<span class=hljs-number >0</span>,<span class=hljs-number >0</span>), (<span class=hljs-number >0</span>,<span class=hljs-number >0</span>), (pad, pad), (pad, pad)], <span class=hljs-string >&#x27;constant&#x27;</span>)
    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

    <span class=hljs-keyword >for</span> y <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(filter_h):
        y_max = y + stride * out_h
        <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(filter_w):
            x_max = x + stride * out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]
    
    col = col.transpose(<span class=hljs-number >0</span>, <span class=hljs-number >4</span>, <span class=hljs-number >5</span>, <span class=hljs-number >1</span>, <span class=hljs-number >2</span>, <span class=hljs-number >3</span>) <span class=hljs-comment ># (N, C, filter_h, filter_w, out_h, out_w) -&gt; (N, filter_w, out_h, out_w, C, filter_h)    </span>
    
    col = col.reshape(N * out_h * out_w, -<span class=hljs-number >1</span>)
    <span class=hljs-keyword >return</span> col

<span class=hljs-comment ># ２次元配列を画像データに変換</span>
<span class=hljs-keyword >def</span> <span class="hljs-title function_">col2im</span>(<span class=hljs-params >col, input_shape, filter_h, filter_w, stride=<span class=hljs-number >1</span>, pad=<span class=hljs-number >0</span></span>):
    <span class=hljs-comment ># N: number, C: channel, H: height, W: width</span>
    N, C, H, W = input_shape
    <span class=hljs-comment ># 切り捨て除算    </span>
    out_h = (H + <span class=hljs-number >2</span> * pad - filter_h)//stride + <span class=hljs-number >1</span>
    out_w = (W + <span class=hljs-number >2</span> * pad - filter_w)//stride + <span class=hljs-number >1</span>
    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(<span class=hljs-number >0</span>, <span class=hljs-number >3</span>, <span class=hljs-number >4</span>, <span class=hljs-number >5</span>, <span class=hljs-number >1</span>, <span class=hljs-number >2</span>) <span class=hljs-comment ># (N, filter_h, filter_w, out_h, out_w, C)</span>

    img = np.zeros((N, C, H + <span class=hljs-number >2</span> * pad + stride - <span class=hljs-number >1</span>, W + <span class=hljs-number >2</span> * pad + stride - <span class=hljs-number >1</span>))
    <span class=hljs-keyword >for</span> y <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(filter_h):
        y_max = y + stride * out_h
        <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(filter_w):
            x_max = x + stride * out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]

    <span class=hljs-keyword >return</span> img[:, :, pad:H + pad, pad:W + pad]</code></pre>
<pre><code class="python hljs"><span class=hljs-keyword >def</span> <span class="hljs-title function_">print_im2col_col2im</span>(<span class=hljs-params >input_data, conv_param</span>):
    filter_h, filter_w, stride, pad = conv_param
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;====== input_data =======\n&#x27;</span>, input_data)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;==========================&#x27;</span>)
    col = im2col(input_data, filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;========= im2col ==========\n&#x27;</span>, col)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;=========================&#x27;</span>)

    img = col2im(col, input_shape=input_data.shape, filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;========= col2im ==========\n&#x27;</span>, img)
    <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;=========================&#x27;</span>)
    <span class=hljs-built_in >print</span>()</code></pre>
<p>im2col の振舞いを理解するため、&#40;わかりやすさのため&#41;全要素1の入力を考える 2x2 のフィルタだと、col2im の結果は</p>
<ul>
<li><p>stride &#61; 2 にすればオーバーラップがなくなるため、全部 1 になる</p>

<li><p>padding &#61; 1 にすれば全画素が同じだけ使用されて、全部 4 になる</p>

</ul>
<p>うん。納得。</p>
<pre><code class="python hljs">input_data = np.reshape(np.ones(<span class=hljs-number >16</span>), (<span class=hljs-number >1</span>, <span class=hljs-number >1</span>, <span class=hljs-number >4</span>, <span class=hljs-number >4</span>)) <span class=hljs-comment ># number, channel, height, widthを表す</span>

conv_param =  (<span class=hljs-number >2</span>,<span class=hljs-number >2</span>,<span class=hljs-number >1</span>,<span class=hljs-number >0</span>)  <span class=hljs-comment ># filter_h, filter_w, stride, pad</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;filter_h, fitler_w, stride, padding = &quot;</span>, conv_param)
print_im2col_col2im( input_data , conv_param)

conv_param =  (<span class=hljs-number >2</span>,<span class=hljs-number >2</span>,<span class=hljs-number >2</span>,<span class=hljs-number >0</span>) <span class=hljs-comment ># filter_h, filter_w, stride, pad</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;filter_h, fitler_w, stride, padding = &quot;</span>, conv_param)
print_im2col_col2im( input_data , conv_param)


conv_param =  (<span class=hljs-number >2</span>,<span class=hljs-number >2</span>,<span class=hljs-number >1</span>,<span class=hljs-number >2</span>) <span class=hljs-comment ># filter_h, filter_w, stride, pad</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;filter_h, fitler_w, stride, padding = &quot;</span>, conv_param)
print_im2col_col2im( input_data , conv_param)



<span class=hljs-comment ># input_data = np.random.rand(2, 1, 4, 4)*100//1 # number, channel, height, widthを表す</span></code></pre>
<pre><code class="julia hljs">filter_h, fitler_w, stride, padding =  (<span class=hljs-number >2</span>, <span class=hljs-number >2</span>, <span class=hljs-number >1</span>, <span class=hljs-number >0</span>)
    ====== input_data =======
     [[[[<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]]]]
    ==========================
    ========= im2col ==========
     [[<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]]
    =========================
    ========= col2im ==========
     [[[[<span class=hljs-number >1.</span> <span class=hljs-number >2.</span> <span class=hljs-number >2.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >2.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >2.</span>]
       [<span class=hljs-number >2.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >2.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >2.</span> <span class=hljs-number >2.</span> <span class=hljs-number >1.</span>]]]]
    =========================
    
    filter_h, fitler_w, stride, padding =  (<span class=hljs-number >2</span>, <span class=hljs-number >2</span>, <span class=hljs-number >2</span>, <span class=hljs-number >0</span>)
    ====== input_data =======
     [[[[<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]]]]
    ==========================
    ========= im2col ==========
     [[<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]]
    =========================
    ========= col2im ==========
     [[[[<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]]]]
    =========================
    
    filter_h, fitler_w, stride, padding =  (<span class=hljs-number >2</span>, <span class=hljs-number >2</span>, <span class=hljs-number >1</span>, <span class=hljs-number >2</span>)
    ====== input_data =======
     [[[[<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
       [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]]]]
    ==========================
    ========= im2col ==========
     [[<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >1.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >1.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]
     [<span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span> <span class=hljs-number >0.</span>]]
    =========================
    ========= col2im ==========
     [[[[<span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span>]
       [<span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span>]
       [<span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span>]
       [<span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span> <span class=hljs-number >4.</span>]]]]
    =========================</code></pre>
<p>そのほかのデータ構造でも試してみる。  &#40; im2col までは &#41; 通し番号の方がわかりやすいので randでなくrangeをつかう。</p>
<pre><code class="python hljs">input_data = np.reshape(<span class=hljs-built_in >range</span>(<span class=hljs-number >100</span>), (<span class=hljs-number >2</span>, <span class=hljs-number >2</span>, <span class=hljs-number >5</span>, <span class=hljs-number >5</span>)) <span class=hljs-comment ># number, channel, height, widthを表す</span>

conv_param =  (<span class=hljs-number >3</span>,<span class=hljs-number >3</span>,<span class=hljs-number >2</span>,<span class=hljs-number >1</span>)  <span class=hljs-comment ># filter_h, filter_w, stride, pad</span>
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;filter_h, fitler_w, stride, padding = &quot;</span>, conv_param)
print_im2col_col2im( input_data , conv_param)</code></pre>
<pre><code class="julia hljs">filter_h, fitler_w, stride, padding =  (<span class=hljs-number >3</span>, <span class=hljs-number >3</span>, <span class=hljs-number >2</span>, <span class=hljs-number >1</span>)
    ====== input_data =======
     [[[[ <span class=hljs-number >0</span>  <span class=hljs-number >1</span>  <span class=hljs-number >2</span>  <span class=hljs-number >3</span>  <span class=hljs-number >4</span>]
       [ <span class=hljs-number >5</span>  <span class=hljs-number >6</span>  <span class=hljs-number >7</span>  <span class=hljs-number >8</span>  <span class=hljs-number >9</span>]
       [<span class=hljs-number >10</span> <span class=hljs-number >11</span> <span class=hljs-number >12</span> <span class=hljs-number >13</span> <span class=hljs-number >14</span>]
       [<span class=hljs-number >15</span> <span class=hljs-number >16</span> <span class=hljs-number >17</span> <span class=hljs-number >18</span> <span class=hljs-number >19</span>]
       [<span class=hljs-number >20</span> <span class=hljs-number >21</span> <span class=hljs-number >22</span> <span class=hljs-number >23</span> <span class=hljs-number >24</span>]]
    
      [[<span class=hljs-number >25</span> <span class=hljs-number >26</span> <span class=hljs-number >27</span> <span class=hljs-number >28</span> <span class=hljs-number >29</span>]
       [<span class=hljs-number >30</span> <span class=hljs-number >31</span> <span class=hljs-number >32</span> <span class=hljs-number >33</span> <span class=hljs-number >34</span>]
       [<span class=hljs-number >35</span> <span class=hljs-number >36</span> <span class=hljs-number >37</span> <span class=hljs-number >38</span> <span class=hljs-number >39</span>]
       [<span class=hljs-number >40</span> <span class=hljs-number >41</span> <span class=hljs-number >42</span> <span class=hljs-number >43</span> <span class=hljs-number >44</span>]
       [<span class=hljs-number >45</span> <span class=hljs-number >46</span> <span class=hljs-number >47</span> <span class=hljs-number >48</span> <span class=hljs-number >49</span>]]]
    
    
     [[[<span class=hljs-number >50</span> <span class=hljs-number >51</span> <span class=hljs-number >52</span> <span class=hljs-number >53</span> <span class=hljs-number >54</span>]
       [<span class=hljs-number >55</span> <span class=hljs-number >56</span> <span class=hljs-number >57</span> <span class=hljs-number >58</span> <span class=hljs-number >59</span>]
       [<span class=hljs-number >60</span> <span class=hljs-number >61</span> <span class=hljs-number >62</span> <span class=hljs-number >63</span> <span class=hljs-number >64</span>]
       [<span class=hljs-number >65</span> <span class=hljs-number >66</span> <span class=hljs-number >67</span> <span class=hljs-number >68</span> <span class=hljs-number >69</span>]
       [<span class=hljs-number >70</span> <span class=hljs-number >71</span> <span class=hljs-number >72</span> <span class=hljs-number >73</span> <span class=hljs-number >74</span>]]
    
      [[<span class=hljs-number >75</span> <span class=hljs-number >76</span> <span class=hljs-number >77</span> <span class=hljs-number >78</span> <span class=hljs-number >79</span>]
       [<span class=hljs-number >80</span> <span class=hljs-number >81</span> <span class=hljs-number >82</span> <span class=hljs-number >83</span> <span class=hljs-number >84</span>]
       [<span class=hljs-number >85</span> <span class=hljs-number >86</span> <span class=hljs-number >87</span> <span class=hljs-number >88</span> <span class=hljs-number >89</span>]
       [<span class=hljs-number >90</span> <span class=hljs-number >91</span> <span class=hljs-number >92</span> <span class=hljs-number >93</span> <span class=hljs-number >94</span>]
       [<span class=hljs-number >95</span> <span class=hljs-number >96</span> <span class=hljs-number >97</span> <span class=hljs-number >98</span> <span class=hljs-number >99</span>]]]]
    ==========================
    ========= im2col ==========
     [[ <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >1.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >5.</span>  <span class=hljs-number >6.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >25.</span> <span class=hljs-number >26.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >30.</span> <span class=hljs-number >31.</span>]
     [ <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >1.</span>  <span class=hljs-number >2.</span>  <span class=hljs-number >3.</span>  <span class=hljs-number >6.</span>  <span class=hljs-number >7.</span>  <span class=hljs-number >8.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >26.</span> <span class=hljs-number >27.</span> <span class=hljs-number >28.</span> <span class=hljs-number >31.</span> <span class=hljs-number >32.</span> <span class=hljs-number >33.</span>]
     [ <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >3.</span>  <span class=hljs-number >4.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >8.</span>  <span class=hljs-number >9.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >28.</span> <span class=hljs-number >29.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >33.</span> <span class=hljs-number >34.</span>  <span class=hljs-number >0.</span>]
     [ <span class=hljs-number >0.</span>  <span class=hljs-number >5.</span>  <span class=hljs-number >6.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >10.</span> <span class=hljs-number >11.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >15.</span> <span class=hljs-number >16.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >30.</span> <span class=hljs-number >31.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >35.</span> <span class=hljs-number >36.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >40.</span> <span class=hljs-number >41.</span>]
     [ <span class=hljs-number >6.</span>  <span class=hljs-number >7.</span>  <span class=hljs-number >8.</span> <span class=hljs-number >11.</span> <span class=hljs-number >12.</span> <span class=hljs-number >13.</span> <span class=hljs-number >16.</span> <span class=hljs-number >17.</span> <span class=hljs-number >18.</span> <span class=hljs-number >31.</span> <span class=hljs-number >32.</span> <span class=hljs-number >33.</span> <span class=hljs-number >36.</span> <span class=hljs-number >37.</span> <span class=hljs-number >38.</span> <span class=hljs-number >41.</span> <span class=hljs-number >42.</span> <span class=hljs-number >43.</span>]
     [ <span class=hljs-number >8.</span>  <span class=hljs-number >9.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >13.</span> <span class=hljs-number >14.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >18.</span> <span class=hljs-number >19.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >33.</span> <span class=hljs-number >34.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >38.</span> <span class=hljs-number >39.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >43.</span> <span class=hljs-number >44.</span>  <span class=hljs-number >0.</span>]
     [ <span class=hljs-number >0.</span> <span class=hljs-number >15.</span> <span class=hljs-number >16.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >20.</span> <span class=hljs-number >21.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >40.</span> <span class=hljs-number >41.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >45.</span> <span class=hljs-number >46.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>]
     [<span class=hljs-number >16.</span> <span class=hljs-number >17.</span> <span class=hljs-number >18.</span> <span class=hljs-number >21.</span> <span class=hljs-number >22.</span> <span class=hljs-number >23.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >41.</span> <span class=hljs-number >42.</span> <span class=hljs-number >43.</span> <span class=hljs-number >46.</span> <span class=hljs-number >47.</span> <span class=hljs-number >48.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>]
     [<span class=hljs-number >18.</span> <span class=hljs-number >19.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >23.</span> <span class=hljs-number >24.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >43.</span> <span class=hljs-number >44.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >48.</span> <span class=hljs-number >49.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>]
     [ <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >50.</span> <span class=hljs-number >51.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >55.</span> <span class=hljs-number >56.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >75.</span> <span class=hljs-number >76.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >80.</span> <span class=hljs-number >81.</span>]
     [ <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >51.</span> <span class=hljs-number >52.</span> <span class=hljs-number >53.</span> <span class=hljs-number >56.</span> <span class=hljs-number >57.</span> <span class=hljs-number >58.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >76.</span> <span class=hljs-number >77.</span> <span class=hljs-number >78.</span> <span class=hljs-number >81.</span> <span class=hljs-number >82.</span> <span class=hljs-number >83.</span>]
     [ <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >53.</span> <span class=hljs-number >54.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >58.</span> <span class=hljs-number >59.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >78.</span> <span class=hljs-number >79.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >83.</span> <span class=hljs-number >84.</span>  <span class=hljs-number >0.</span>]
     [ <span class=hljs-number >0.</span> <span class=hljs-number >55.</span> <span class=hljs-number >56.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >60.</span> <span class=hljs-number >61.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >65.</span> <span class=hljs-number >66.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >80.</span> <span class=hljs-number >81.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >85.</span> <span class=hljs-number >86.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >90.</span> <span class=hljs-number >91.</span>]
     [<span class=hljs-number >56.</span> <span class=hljs-number >57.</span> <span class=hljs-number >58.</span> <span class=hljs-number >61.</span> <span class=hljs-number >62.</span> <span class=hljs-number >63.</span> <span class=hljs-number >66.</span> <span class=hljs-number >67.</span> <span class=hljs-number >68.</span> <span class=hljs-number >81.</span> <span class=hljs-number >82.</span> <span class=hljs-number >83.</span> <span class=hljs-number >86.</span> <span class=hljs-number >87.</span> <span class=hljs-number >88.</span> <span class=hljs-number >91.</span> <span class=hljs-number >92.</span> <span class=hljs-number >93.</span>]
     [<span class=hljs-number >58.</span> <span class=hljs-number >59.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >63.</span> <span class=hljs-number >64.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >68.</span> <span class=hljs-number >69.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >83.</span> <span class=hljs-number >84.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >88.</span> <span class=hljs-number >89.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >93.</span> <span class=hljs-number >94.</span>  <span class=hljs-number >0.</span>]
     [ <span class=hljs-number >0.</span> <span class=hljs-number >65.</span> <span class=hljs-number >66.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >70.</span> <span class=hljs-number >71.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >90.</span> <span class=hljs-number >91.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >95.</span> <span class=hljs-number >96.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>]
     [<span class=hljs-number >66.</span> <span class=hljs-number >67.</span> <span class=hljs-number >68.</span> <span class=hljs-number >71.</span> <span class=hljs-number >72.</span> <span class=hljs-number >73.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >91.</span> <span class=hljs-number >92.</span> <span class=hljs-number >93.</span> <span class=hljs-number >96.</span> <span class=hljs-number >97.</span> <span class=hljs-number >98.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>]
     [<span class=hljs-number >68.</span> <span class=hljs-number >69.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >73.</span> <span class=hljs-number >74.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >93.</span> <span class=hljs-number >94.</span>  <span class=hljs-number >0.</span> <span class=hljs-number >98.</span> <span class=hljs-number >99.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>  <span class=hljs-number >0.</span>]]
    =========================
    ========= col2im ==========
     [[[[  <span class=hljs-number >0.</span>   <span class=hljs-number >2.</span>   <span class=hljs-number >2.</span>   <span class=hljs-number >6.</span>   <span class=hljs-number >4.</span>]
       [ <span class=hljs-number >10.</span>  <span class=hljs-number >24.</span>  <span class=hljs-number >14.</span>  <span class=hljs-number >32.</span>  <span class=hljs-number >18.</span>]
       [ <span class=hljs-number >10.</span>  <span class=hljs-number >22.</span>  <span class=hljs-number >12.</span>  <span class=hljs-number >26.</span>  <span class=hljs-number >14.</span>]
       [ <span class=hljs-number >30.</span>  <span class=hljs-number >64.</span>  <span class=hljs-number >34.</span>  <span class=hljs-number >72.</span>  <span class=hljs-number >38.</span>]
       [ <span class=hljs-number >20.</span>  <span class=hljs-number >42.</span>  <span class=hljs-number >22.</span>  <span class=hljs-number >46.</span>  <span class=hljs-number >24.</span>]]
    
      [[ <span class=hljs-number >25.</span>  <span class=hljs-number >52.</span>  <span class=hljs-number >27.</span>  <span class=hljs-number >56.</span>  <span class=hljs-number >29.</span>]
       [ <span class=hljs-number >60.</span> <span class=hljs-number >124.</span>  <span class=hljs-number >64.</span> <span class=hljs-number >132.</span>  <span class=hljs-number >68.</span>]
       [ <span class=hljs-number >35.</span>  <span class=hljs-number >72.</span>  <span class=hljs-number >37.</span>  <span class=hljs-number >76.</span>  <span class=hljs-number >39.</span>]
       [ <span class=hljs-number >80.</span> <span class=hljs-number >164.</span>  <span class=hljs-number >84.</span> <span class=hljs-number >172.</span>  <span class=hljs-number >88.</span>]
       [ <span class=hljs-number >45.</span>  <span class=hljs-number >92.</span>  <span class=hljs-number >47.</span>  <span class=hljs-number >96.</span>  <span class=hljs-number >49.</span>]]]
    
    
     [[[ <span class=hljs-number >50.</span> <span class=hljs-number >102.</span>  <span class=hljs-number >52.</span> <span class=hljs-number >106.</span>  <span class=hljs-number >54.</span>]
       [<span class=hljs-number >110.</span> <span class=hljs-number >224.</span> <span class=hljs-number >114.</span> <span class=hljs-number >232.</span> <span class=hljs-number >118.</span>]
       [ <span class=hljs-number >60.</span> <span class=hljs-number >122.</span>  <span class=hljs-number >62.</span> <span class=hljs-number >126.</span>  <span class=hljs-number >64.</span>]
       [<span class=hljs-number >130.</span> <span class=hljs-number >264.</span> <span class=hljs-number >134.</span> <span class=hljs-number >272.</span> <span class=hljs-number >138.</span>]
       [ <span class=hljs-number >70.</span> <span class=hljs-number >142.</span>  <span class=hljs-number >72.</span> <span class=hljs-number >146.</span>  <span class=hljs-number >74.</span>]]
    
      [[ <span class=hljs-number >75.</span> <span class=hljs-number >152.</span>  <span class=hljs-number >77.</span> <span class=hljs-number >156.</span>  <span class=hljs-number >79.</span>]
       [<span class=hljs-number >160.</span> <span class=hljs-number >324.</span> <span class=hljs-number >164.</span> <span class=hljs-number >332.</span> <span class=hljs-number >168.</span>]
       [ <span class=hljs-number >85.</span> <span class=hljs-number >172.</span>  <span class=hljs-number >87.</span> <span class=hljs-number >176.</span>  <span class=hljs-number >89.</span>]
       [<span class=hljs-number >180.</span> <span class=hljs-number >364.</span> <span class=hljs-number >184.</span> <span class=hljs-number >372.</span> <span class=hljs-number >188.</span>]
       [ <span class=hljs-number >95.</span> <span class=hljs-number >192.</span>  <span class=hljs-number >97.</span> <span class=hljs-number >196.</span>  <span class=hljs-number >99.</span>]]]]
    =========================</code></pre>
<pre><code class="python hljs"><span class=hljs-keyword >class</span> <span class="hljs-title class_">Convolution</span>:
    <span class=hljs-comment ># W: フィルター, b: バイアス</span>
    <span class=hljs-keyword >def</span> __init__(self, W, b, stride=<span class=hljs-number >1</span>, pad=<span class=hljs-number >0</span>):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        
        <span class=hljs-comment ># 中間データ（backward時に使用）</span>
        self.x = <span class=hljs-literal >None</span>   
        self.col = <span class=hljs-literal >None</span>
        self.col_W = <span class=hljs-literal >None</span>
        
        <span class=hljs-comment ># フィルター・バイアスパラメータの勾配</span>
        self.dW = <span class=hljs-literal >None</span>
        self.db = <span class=hljs-literal >None</span>

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >self, x</span>):
        <span class=hljs-comment ># FN: filter_number, C: channel, FH: filter_height, FW: filter_width</span>
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        <span class=hljs-comment ># 出力値のheight, width</span>
        out_h = <span class=hljs-number >1</span> + <span class=hljs-built_in >int</span>((H + <span class=hljs-number >2</span> * self.pad - FH) / self.stride)
        out_w = <span class=hljs-number >1</span> + <span class=hljs-built_in >int</span>((W + <span class=hljs-number >2</span> * self.pad - FW) / self.stride)
        
        <span class=hljs-comment ># xを行列に変換</span>
        col = im2col(x, FH, FW, self.stride, self.pad)
        <span class=hljs-comment ># フィルターをxに合わせた行列に変換</span>
        col_W = self.W.reshape(FN, -<span class=hljs-number >1</span>).T

        out = np.dot(col, col_W) + self.b
        <span class=hljs-comment ># 計算のために変えた形式を戻す</span>
        out = out.reshape(N, out_h, out_w, -<span class=hljs-number >1</span>).transpose(<span class=hljs-number >0</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>, <span class=hljs-number >2</span>)

        self.x = x
        self.col = col
        self.col_W = col_W

        <span class=hljs-keyword >return</span> out

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">backward</span>(<span class=hljs-params >self, dout</span>):
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(<span class=hljs-number >0</span>, <span class=hljs-number >2</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>).reshape(-<span class=hljs-number >1</span>, FN)

        self.db = np.<span class=hljs-built_in >sum</span>(dout, axis=<span class=hljs-number >0</span>)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(<span class=hljs-number >1</span>, <span class=hljs-number >0</span>).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        <span class=hljs-comment ># dcolを画像データに変換</span>
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)

        <span class=hljs-keyword >return</span> dx

    
<span class=hljs-keyword >class</span> <span class="hljs-title class_">Pooling</span>:
    <span class=hljs-keyword >def</span> __init__(self, pool_h, pool_w, stride=<span class=hljs-number >1</span>, pad=<span class=hljs-number >0</span>):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = <span class=hljs-literal >None</span>
        self.arg_max = <span class=hljs-literal >None</span>

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">forward</span>(<span class=hljs-params >self, x</span>):
        N, C, H, W = x.shape
        out_h = <span class=hljs-built_in >int</span>(<span class=hljs-number >1</span> + (H - self.pool_h) / self.stride)
        out_w = <span class=hljs-built_in >int</span>(<span class=hljs-number >1</span> + (W - self.pool_w) / self.stride)
        
        <span class=hljs-comment ># xを行列に変換</span>
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        <span class=hljs-comment ># プーリングのサイズに合わせてリサイズ</span>
        col = col.reshape(-<span class=hljs-number >1</span>, self.pool_h*self.pool_w)
        
        <span class=hljs-comment ># 行ごとに最大値を求める</span>
        arg_max = np.argmax(col, axis=<span class=hljs-number >1</span>)
        out = np.<span class=hljs-built_in >max</span>(col, axis=<span class=hljs-number >1</span>)
        <span class=hljs-comment ># 整形</span>
        out = out.reshape(N, out_h, out_w, C).transpose(<span class=hljs-number >0</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>, <span class=hljs-number >2</span>)

        self.x = x
        self.arg_max = arg_max

        <span class=hljs-keyword >return</span> out

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">backward</span>(<span class=hljs-params >self, dout</span>):
        dout = dout.transpose(<span class=hljs-number >0</span>, <span class=hljs-number >2</span>, <span class=hljs-number >3</span>, <span class=hljs-number >1</span>)
        
        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,)) 
        
        dcol = dmax.reshape(dmax.shape[<span class=hljs-number >0</span>] * dmax.shape[<span class=hljs-number >1</span>] * dmax.shape[<span class=hljs-number >2</span>], -<span class=hljs-number >1</span>)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
        
        <span class=hljs-keyword >return</span> dx</code></pre>
<pre><code class="python hljs"><span class=hljs-comment ># simple CNN の実装・学習例</span>

<span class=hljs-keyword >class</span> <span class="hljs-title class_">SimpleConvNet</span>:
    <span class=hljs-comment ># conv - relu - pool - affine - relu - affine - softmax</span>
    <span class=hljs-keyword >def</span> __init__(self, input_dim=(<span class=hljs-number >1</span>, <span class=hljs-number >28</span>, <span class=hljs-number >28</span>), conv_param={<span class=hljs-string >&#x27;filter_num&#x27;</span>:<span class=hljs-number >30</span>, <span class=hljs-string >&#x27;filter_size&#x27;</span>:<span class=hljs-number >5</span>, <span class=hljs-string >&#x27;pad&#x27;</span>:<span class=hljs-number >0</span>, <span class=hljs-string >&#x27;stride&#x27;</span>:<span class=hljs-number >1</span>},
                 hidden_size=<span class=hljs-number >100</span>, output_size=<span class=hljs-number >10</span>, weight_init_std=<span class=hljs-number >0.01</span>):
        filter_num = conv_param[<span class=hljs-string >&#x27;filter_num&#x27;</span>]        
        filter_size = conv_param[<span class=hljs-string >&#x27;filter_size&#x27;</span>]
        filter_pad = conv_param[<span class=hljs-string >&#x27;pad&#x27;</span>]
        filter_stride = conv_param[<span class=hljs-string >&#x27;stride&#x27;</span>]
        input_size = input_dim[<span class=hljs-number >1</span>]
        conv_output_size = (input_size - filter_size + <span class=hljs-number >2</span> * filter_pad) / filter_stride + <span class=hljs-number >1</span>
        pool_output_size = <span class=hljs-built_in >int</span>(filter_num * (conv_output_size / <span class=hljs-number >2</span>) * (conv_output_size / <span class=hljs-number >2</span>))

        <span class=hljs-comment ># 重みの初期化</span>
        self.params = {}
        self.params[<span class=hljs-string >&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(filter_num, input_dim[<span class=hljs-number >0</span>], filter_size, filter_size)
        self.params[<span class=hljs-string >&#x27;b1&#x27;</span>] = np.zeros(filter_num)
        self.params[<span class=hljs-string >&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(pool_output_size, hidden_size)
        self.params[<span class=hljs-string >&#x27;b2&#x27;</span>] = np.zeros(hidden_size)
        self.params[<span class=hljs-string >&#x27;W3&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params[<span class=hljs-string >&#x27;b3&#x27;</span>] = np.zeros(output_size)

        <span class=hljs-comment ># レイヤの生成</span>
        self.layers = OrderedDict()
        self.layers[<span class=hljs-string >&#x27;Conv1&#x27;</span>] = layers.Convolution(self.params[<span class=hljs-string >&#x27;W1&#x27;</span>], self.params[<span class=hljs-string >&#x27;b1&#x27;</span>], conv_param[<span class=hljs-string >&#x27;stride&#x27;</span>], conv_param[<span class=hljs-string >&#x27;pad&#x27;</span>])
        self.layers[<span class=hljs-string >&#x27;Relu1&#x27;</span>] = layers.Relu()
        self.layers[<span class=hljs-string >&#x27;Pool1&#x27;</span>] = layers.Pooling(pool_h=<span class=hljs-number >2</span>, pool_w=<span class=hljs-number >2</span>, stride=<span class=hljs-number >2</span>)
        self.layers[<span class=hljs-string >&#x27;Affine1&#x27;</span>] = layers.Affine(self.params[<span class=hljs-string >&#x27;W2&#x27;</span>], self.params[<span class=hljs-string >&#x27;b2&#x27;</span>])
        self.layers[<span class=hljs-string >&#x27;Relu2&#x27;</span>] = layers.Relu()
        self.layers[<span class=hljs-string >&#x27;Affine2&#x27;</span>] = layers.Affine(self.params[<span class=hljs-string >&#x27;W3&#x27;</span>], self.params[<span class=hljs-string >&#x27;b3&#x27;</span>])

        self.last_layer = layers.SoftmaxWithLoss()

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">predict</span>(<span class=hljs-params >self, x</span>):
        <span class=hljs-keyword >for</span> key <span class=hljs-keyword >in</span> self.layers.keys():
            x = self.layers[key].forward(x)
        <span class=hljs-keyword >return</span> x
        
    <span class=hljs-keyword >def</span> <span class="hljs-title function_">loss</span>(<span class=hljs-params >self, x, d</span>):
        y = self.predict(x)
        <span class=hljs-keyword >return</span> self.last_layer.forward(y, d)

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">accuracy</span>(<span class=hljs-params >self, x, d, batch_size=<span class=hljs-number >100</span></span>):
        <span class=hljs-keyword >if</span> d.ndim != <span class=hljs-number >1</span> : d = np.argmax(d, axis=<span class=hljs-number >1</span>)
        
        acc = <span class=hljs-number >0.0</span>
        
        <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(<span class=hljs-built_in >int</span>(x.shape[<span class=hljs-number >0</span>] / batch_size)):
            tx = x[i*batch_size:(i+<span class=hljs-number >1</span>)*batch_size]
            td = d[i*batch_size:(i+<span class=hljs-number >1</span>)*batch_size]
            y = self.predict(tx)
            y = np.argmax(y, axis=<span class=hljs-number >1</span>)
            acc += np.<span class=hljs-built_in >sum</span>(y == td) 
        
        <span class=hljs-keyword >return</span> acc / x.shape[<span class=hljs-number >0</span>]

    <span class=hljs-keyword >def</span> <span class="hljs-title function_">gradient</span>(<span class=hljs-params >self, x, d</span>):
        <span class=hljs-comment ># forward</span>
        self.loss(x, d)
        
        <span class=hljs-comment ># backward</span>
        dout = <span class=hljs-number >1</span>
        dout = self.last_layer.backward(dout)
        layers = <span class=hljs-built_in >list</span>(self.layers.values())
        
        layers.reverse()
        <span class=hljs-keyword >for</span> layer <span class=hljs-keyword >in</span> layers:
            dout = layer.backward(dout)

        <span class=hljs-comment ># 設定</span>
        grad = {}
        grad[<span class=hljs-string >&#x27;W1&#x27;</span>], grad[<span class=hljs-string >&#x27;b1&#x27;</span>] = self.layers[<span class=hljs-string >&#x27;Conv1&#x27;</span>].dW, self.layers[<span class=hljs-string >&#x27;Conv1&#x27;</span>].db
        grad[<span class=hljs-string >&#x27;W2&#x27;</span>], grad[<span class=hljs-string >&#x27;b2&#x27;</span>] = self.layers[<span class=hljs-string >&#x27;Affine1&#x27;</span>].dW, self.layers[<span class=hljs-string >&#x27;Affine1&#x27;</span>].db
        grad[<span class=hljs-string >&#x27;W3&#x27;</span>], grad[<span class=hljs-string >&#x27;b3&#x27;</span>] = self.layers[<span class=hljs-string >&#x27;Affine2&#x27;</span>].dW, self.layers[<span class=hljs-string >&#x27;Affine2&#x27;</span>].db

        <span class=hljs-keyword >return</span> grad</code></pre>
<pre><code class="python hljs"><span class=hljs-keyword >from</span> common <span class=hljs-keyword >import</span> optimizer

<span class=hljs-comment ># データの読み込み</span>
(x_train, d_train), (x_test, d_test) = load_mnist(flatten=<span class=hljs-literal >False</span>)

<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;データ読み込み完了&quot;</span>)

<span class=hljs-comment ># 処理に時間のかかる場合はデータを削減 </span>
x_train, d_train = x_train[:<span class=hljs-number >5000</span>], d_train[:<span class=hljs-number >5000</span>]
x_test, d_test = x_test[:<span class=hljs-number >1000</span>], d_test[:<span class=hljs-number >1000</span>]


network = SimpleConvNet(input_dim=(<span class=hljs-number >1</span>,<span class=hljs-number >28</span>,<span class=hljs-number >28</span>), conv_param = {<span class=hljs-string >&#x27;filter_num&#x27;</span>: <span class=hljs-number >30</span>, <span class=hljs-string >&#x27;filter_size&#x27;</span>: <span class=hljs-number >5</span>, <span class=hljs-string >&#x27;pad&#x27;</span>: <span class=hljs-number >0</span>, <span class=hljs-string >&#x27;stride&#x27;</span>: <span class=hljs-number >1</span>},
                        hidden_size=<span class=hljs-number >100</span>, output_size=<span class=hljs-number >10</span>, weight_init_std=<span class=hljs-number >0.01</span>)

optimizer = optimizer.Adam()

iters_num = <span class=hljs-number >1000</span>
train_size = x_train.shape[<span class=hljs-number >0</span>]
batch_size = <span class=hljs-number >100</span>

train_loss_list = []
accuracies_train = []
accuracies_test = []

plot_interval=<span class=hljs-number >10</span>



<span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-built_in >range</span>(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    d_batch = d_train[batch_mask]
    
    grad = network.gradient(x_batch, d_batch)
    optimizer.update(network.params, grad)

    loss = network.loss(x_batch, d_batch)
    train_loss_list.append(loss)

    <span class=hljs-keyword >if</span> (i+<span class=hljs-number >1</span>) % plot_interval == <span class=hljs-number >0</span>:
        accr_train = network.accuracy(x_train, d_train)
        accr_test = network.accuracy(x_test, d_test)
        accuracies_train.append(accr_train)
        accuracies_test.append(accr_test)
        
        <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;Generation: &#x27;</span> + <span class=hljs-built_in >str</span>(i+<span class=hljs-number >1</span>) + <span class=hljs-string >&#x27;. 正答率(トレーニング) = &#x27;</span> + <span class=hljs-built_in >str</span>(accr_train))
        <span class=hljs-built_in >print</span>(<span class=hljs-string >&#x27;                : &#x27;</span> + <span class=hljs-built_in >str</span>(i+<span class=hljs-number >1</span>) + <span class=hljs-string >&#x27;. 正答率(テスト) = &#x27;</span> + <span class=hljs-built_in >str</span>(accr_test))               

lists = <span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>, iters_num, plot_interval)
plt.plot(lists, accuracies_train, label=<span class=hljs-string >&quot;training set&quot;</span>)
plt.plot(lists, accuracies_test,  label=<span class=hljs-string >&quot;test set&quot;</span>)
plt.legend(loc=<span class=hljs-string >&quot;lower right&quot;</span>)
plt.title(<span class=hljs-string >&quot;accuracy&quot;</span>)
plt.xlabel(<span class=hljs-string >&quot;count&quot;</span>)
plt.ylabel(<span class=hljs-string >&quot;accuracy&quot;</span>)
plt.ylim(<span class=hljs-number >0</span>, <span class=hljs-number >1.0</span>)
<span class=hljs-comment ># グラフの表示</span>
plt.show()</code></pre>
<pre><code class="julia hljs">データ読み込み完了
    Generation: <span class=hljs-number >10.</span> 正答率(トレーニング) = <span class=hljs-number >0.5386</span>
                    : <span class=hljs-number >10.</span> 正答率(テスト) = <span class=hljs-number >0.514</span>
    Generation: <span class=hljs-number >20.</span> 正答率(トレーニング) = <span class=hljs-number >0.5666</span>
                    : <span class=hljs-number >20.</span> 正答率(テスト) = <span class=hljs-number >0.539</span>
    Generation: <span class=hljs-number >30.</span> 正答率(トレーニング) = <span class=hljs-number >0.7254</span>
                    : <span class=hljs-number >30.</span> 正答率(テスト) = <span class=hljs-number >0.713</span>
    Generation: <span class=hljs-number >40.</span> 正答率(トレーニング) = <span class=hljs-number >0.793</span>
                    : <span class=hljs-number >40.</span> 正答率(テスト) = <span class=hljs-number >0.775</span>
    Generation: <span class=hljs-number >50.</span> 正答率(トレーニング) = <span class=hljs-number >0.8136</span>
                    : <span class=hljs-number >50.</span> 正答率(テスト) = <span class=hljs-number >0.783</span>
    Generation: <span class=hljs-number >60.</span> 正答率(トレーニング) = <span class=hljs-number >0.8538</span>

    (中略)

    Generation: <span class=hljs-number >1000.</span> 正答率(トレーニング) = <span class=hljs-number >0.9962</span>
                    : <span class=hljs-number >1000.</span> 正答率(テスト) = <span class=hljs-number >0.96</span></code></pre>
<p><img src="/assets/2_6_simple_convolution_network_files/2_6_simple_convolution_network_11_1.svg" alt=svg  />
<h3 id="ol_start5_最新のcnn"><a href="#ol_start5_最新のcnn" class=header-anchor ><ol start=5 >
<li><p>最新のCNN</p>

</ol>
</a></h3>
<ul>
<li><p>AlexNetの紹介をとおして落ち穂拾い</p>
<ul>
<li><p>Section タイトルは &quot;最新の&quot; CNN と書かれているが、動画講義のせつめいでは &quot;比較的初期の&quot; と言われている. </p>
<ul>
<li><p>本当に最新の持ってくると、複雑すぎるのでシンプルなものを取り扱う、ということらしい.</p>

</ul>

<li><p>扱う問題: </p>
<ul>
<li><p>ImageNet と呼ばれる 画像の分類</p>

<li><p>入力画像サイズ &#61; 224x224x3,  出力クラス分類 &#61; 1000 ?</p>

</ul>

<li><p>構造: 具体的な構造は実装演習のコードで示すので割愛</p>
<ul>
<li><p>1層目の説明で、「カーネルサイズがでかいから、一気に次の層のサイズが小さくなる」と説明されているが、kernel size による出力サイズへの影響はたかだか、kernel size -1。ストライドが4であることのほうが影響大きいのでは?</p>

<li><p>畳み込み層から、全結合層への以降方法</p>
<ul>
<li><p>AlexNet では、Flatten &#40;複数次元データを単純に1次元ベクトルに並べ替え&#41;</p>

<li><p>後に、Global Max Pooling,  Global Average Pooling などが提案された</p>
<ul>
<li><p>画像サイズと同じサイズの kernel をつかった Pooling</p>

<li><p>Flatten より Global xxx Pooling のほうがうまくいくケースが多いらしい</p>
<ul>
<li><p>講師の先生は、Flatten のほうが情報保持しているからうまく行きそうなのに.. 的な前置きをしていたが、その直感には賛同しかねる。情報量多いとそれだけ全結合層のパラメータ増えるので学習進みにくくなる可能性はあるし..</p>

</ul>

</ul>

</ul>

</ul>

<li><p>学習方法の工夫</p>
<ul>
<li><p>Dropout を全結合層の出力に使用</p>
<ul>
<li><p>畳み込み層に使う事もなくはないが、全結合層に使うケースが多いらしい</p>

</ul>

</ul>

</ul>

</ul>
<p>実装演習無し</p>
<h2 id="取り組み概要_感想"><a href="#取り組み概要_感想" class=header-anchor >取り組み概要 &amp; 感想</a></h2>
<h3 id="取り組みの記録"><a href="#取り組みの記録" class=header-anchor >取り組みの記録</a></h3>
<p>今回は、実装演習にじっくり時間をかける感じでもなかったので、 Section単位で 動画みながらまとめ-&gt;実装演習 と進めていった。</p>
<ul>
<li><p>5/7 : Day1 Section.1 までの動画、コードをざっと眺めて、レポートの要件について事務局問い合わせ。&#40;1h&#41;</p>
<ul>
<li><p>レポート提出方法の&quot;実装演習&quot;というのが何を指しているのか不明確であったため。</p>

<li><p>事務局回答を踏まえて、本ページ 冒頭に書いたようなまとめ方をすることにした。</p>

</ul>

</ul>
<p>　- Stage.1,2 動画は基本座学で、コードの内容や動画で取り上げられることはほぼなかったが、このStage.3 では動画講義中にもコードを見て動かすシーンが多発するので、エディタ上で動画を見ながらレポートをまとめつつ、コードも実行しつつのスタイルですすめる形を取ることにした.</p>
<ul>
<li><p>5/13: プロローグ を動画みながらざっくりまとめ  &#40;0.5h&#41;</p>

<li><p>5/14: Day1 Section 0, 1, 2 を動画みながらざっくりまとめ &#40;1.5h&#41;</p>

<li><p>5/15: Day1 Section 3 &#40;1h&#41;</p>

<li><p>5/16: Day1 Section 4 &#40;1.5h&#41;</p>

<li><p>5/17: Day1 Section 5 &#40;1h&#41;</p>

<li><p>5/20: Day1 のこり &#40;1h くらい&#41;</p>

<li><p>5/22: Day2 Section 1 &#40;1.75h&#41;</p>

<li><p>5/23: Day2 Section 2 &#40;2.5h&#41; </p>
<ul>
<li><p>演習でのハイパパラメータを色々試していたら時間を食った</p>

</ul>

<li><p>5/25: Day2 Section 3 &#40;3h&#41;</p>
<ul>
<li><p>同じく、ハイパパラメータを色々試していたら時間がかかってしまった。特に Dropoutを入れると収束するまでに 必要なiterationが増えるのが大きかった。</p>

</ul>

<li><p>5/31: Day2 Section 4,5 &#40;2h&#41;</p>

<li><p>6/1 : ステージテスト &#40;2h&#41;</p>

</ul>
<h3 id="感想ほか"><a href="#感想ほか" class=header-anchor >感想ほか　</a></h3>
<p>今回のステージの動画に出てきた講師の方は Stage1,2の方とは違う方。&#40;スタートテスト前のPython講座などをやっていた方&#41; 講師として実績はある方なのだと思うが、私とは、指導・解説の仕方のフィーリングが合わず、意図を汲み取りかねる箇所などがあった。 同じような感想を持った方は、資料の数式を正として要旨を理解し、講師の先生の言い回しや独自の説明の内容についてはあまり細かいところまで気にしすぎないのがよいかもしれない。</p>
<p>例えば.. </p>
<ul>
<li><p>説明で使う言葉に若干の違和感を感じることがあった</p>
<ul>
<li><p>一例: 重み付き和を　まぜ合わせる, シャッフルと表現。 </p>
<ul>
<li><p>&quot;カードを交ぜる&quot; / &quot;醤油と塩を混ぜる&quot; という２つの表現は、まぜたあと個々が区別できるかどうかが違う。シャッフルは前者に該当。絶対間違えだと主張するつもりはないが、なぜあえて、シャッフルという表現をつかったのかな?と1分立ち止まってしまった。</p>

</ul>

</ul>

</ul>
<ul>
<li><p>図の説明が独特</p>
<ul>
<li><p>例: 学習率の違いによって収束のしかたが異なることを手書きの図を使って説明したとき、loss 関数のグラフの傾きが異なる点でも、同じようにパラメータが変化していくような矢印を書いている。おそらく、初学者にとっては混乱を招く図の書き方だろう</p>

</ul>

</ul>
<ul>
<li><p>細かい点の見間違いがちらほら</p>
<ul>
<li><p>例: 確認テストの細かい意味を取り違えて、おそらく想定した正解とは異なる解答を示すことがある&#40;確認テストの5-2 など&#41;</p>

</ul>

</ul>
<ul>
<li><p>前後関係を無視した説明, 用語の使い方</p>
<ul>
<li><p>例: ミニバッチ勾配降下法の説明をしたあとで、確認テストでその利点を説明させる流れなのに、分散学習的な実装を想定した説明をする。箇所箇所で言っていることは正しいのだが、資料作成者が想定した流れではないと思うし、初学者はミニバッチ勾配降下法の理解に不安を持つと思う。分散学習の話するならするで、「これはまたミニバッチ勾配降下法の基礎の学習とは別の段階の話ですが、分散学習というのがあって、、」みたいな前置きをするのがいいのかな、と思う。</p>

</ul>

</ul>
<p>実装演習に関しては、やはり Stage.2 と同じで基本そのままで動いてしまうので、&quot;勉強の仕方が上手い人&quot; でないとさーっと通り過ぎてしまってコードから多くを学ぶことができないのではないのかな? と思った。</p>
<p>テストについて。他の受講生の方でちらほら 合格に苦労したという方がいたので、ケアレスミスの内容にゆっくり解いた。 全問正解だったが、必要以上に時間を書けすぎてしまったかな。。同じペースで本番やったら、問題解ききれない。</p>
<p>内容については 詳細は触れないが、Stage.3 にかぎらずこれまでの講義のどこかでやった内容からの出題で、特に未知の情報は無い。 出題文の日本語の意味がちょっとわかりにくいな、という出題はいくらかあったが、4択問題なので正解はまぁ導けるかな、、とは思う。 「誤ってる/正しいものを選べ」形式の設問でいくつか迷ったものがあったので、それは復習をしておこう。</p>
<h2 id="計画_前回から変更無し"><a href="#計画_前回から変更無し" class=header-anchor >計画 &#40;前回から変更無し&#41;</a></h2>
<p>Stage.2 終了時に見直したスケジュールはほぼキープできた。 ペースは上がってきているが、ステージ3より4の方がボリュームありそうなので、 前倒しにはせず、今の所変更無しとしておく。 前評判ではステージ4のテストはめっぽう難しいらしいので、クリア前に計画見直す可能性もあるかも?</p>
<ul>
<li><p>~2021/2/15  : スタートテスト &#40;2021/02/07完了, 10h&#41;</p>

<li><p>~2021/3/30  : ステージ1      &#40;2021/03/30完了, 8h&#41;</p>

<li><p>~2021/5/6   : ステージ2      &#40;2021/05/06完了, 21h&#41;</p>

<li><p>~2021/5/30  : ステージ3      &#40;2021/06/02完了, 17.75h&#41; </p>

<li><p>~2021/6/27  : ステージ4 </p>

<li><p>~2021/7/4   : 復習 -&gt; 修了テスト </p>

<li><p>~2021/7/15  : Eもぎライト -&gt; 今後の計画具体化 </p>

<li><p>~2021/7/30  : シラバスの未習箇所の学習 </p>

<li><p>~2021/8/26  : 全体の復習</p>

<li><p>2021/8/27,28: E資格 受験 </p>

</ul>
<div class=page-foot >
  <div class=copyright >
    &copy; kyokke. Last modified: June 02, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>