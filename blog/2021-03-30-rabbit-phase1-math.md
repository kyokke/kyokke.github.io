@def title = "ラビットチャレンジ: Phase.1 応用数学"
@def author = "kyokke" 
@def tags = [ "Deep-Learning", "Rabbit-Challenge" ]


# ラビットチャレンジ: Phase.1 応用数学 

本ページはラビットチャレンジの、
Phase.1 応用数学の科目のレポートの提出を兼ねた、受講記録です。

ラビットチャレンジのオリエンテーション資料に書かれた要件に従って講義動画の内容をまとめた後、述べています。
 - 1点100文字以上で要点をまとめ
 - レポートを講義本筋に沿ってまとめる
 - 受講者自身の言葉で課題や気付きを記載する

## 講義動画まとめ: 線形代数 

各用語の定義など、既知のものばかりだった。それらを教科書チックにまとめるのはちょっと時間が無駄な気がする。講義動画でどのような内容がどのように説明されたか、インデックスをつけるつもりでメモ書きをまとめる。

### 動画 1-0~4 : 行列・ベクトルとその積を理解するまで

 - スカラー/ベクトル/行列とは
    - どれも、四則演算的な計算ができている「数」とみなせる
    - スカラー : いわゆる普通の数
    - ベクトル : スカラーをセットにして、大きさ・向きなどの意味をもたせたもの
      - この動画では断リのない限り、ベクトル=列ベクトルを指す
    - 行列 : スカラーを表敬式にしたもの = ベクトルのベクトル
      - 一次の連立方程式を $A x = b$ という比例の式で形式的に表すためのものとして発明された
 - 行列をかける、ということ
    - (行列) ✕ (ベクトル) : ベクトルを別のベクトルに変換する操作
      - NeuralNetwork の説明で使用するような図を利用した説明
        - これだと確かに次元の増減もすなおに考えられて、NNの導入としての行列の説明にはよい
        - よくある "回転と拡大" という説明ではない
    - (行列1) ✕ (行列2) : 行列を別の行列に変換する操作
      - 行列2 を 列ベクトルが横にならんだものとして、説明

### 動画 1-5~10 : 行列を使って一次連立方程式を解く

  - まずは 中学校で習った連立方程式の解き方の復習・整理
    - 下記作業の組合せ
      - i 式目を c 倍 する
      - i 式目に j式目の　c 倍をかける
      - i 式目と j式を入れ替える 
  - 方程式を解く 
    - → 係数行列を 未知数が直接わかるような別の係数行列=単位行列に変換する
    - → 逆行列を左からかける
      - 逆行列は、上記操作に対応する行列の掛け算の組合せ(積)
      - 逆行列の求め方は、掃き出し法で説明がされていた

### 動画 1-11~15 : 逆行列の存在しない条件・行列式

  - 動画11 : 逆行列 
    - 逆行列の存在しない条件 = ( 解無し or 解不定 ) =係数行列の行ベクトルが作る平行四辺形の面積ゼロ
      - なるほど一次独立とかの話につながっていきそう.. (と思ったらそうでもなかった)
  - 動画12 : 行列式 (n次元拡張)
    - n次元に拡張するために、面積からの類推で基本性質を決めていく
      - 同じベクトルがあるとゼロ
        - 動画では連立方程式から考えていたが、にわとり卵問題で、ぐるぐる回ってない？
      - 線形性（ある行のベクトルから、行列式の値を計算する関数を考えたとき、その関数は線形）
        - どれか一つのベクトルをラムダ倍したら行列式もラムダ倍
        - ある行の行ベクトルに別のベクトル足したらそれぞれに対する行列式の和と等しい
      - n次元の行列式は、n-1次元平面の面積✖️n次元方向の長さで、計算できると考えて、基本性質を使って変形すると、よく知られている行列式の計算方法になる
  - 動画13-15 は練習題


### 動画 2-1~2 : 行列の固有ベクトルと固有ベクトル

  - その行列にベクトルをかけたら、元のベクトルにスカラーがかかった形になる。
  - この時、そのベクトルを固有ベクトルと言い、スカラーを固有値という。
  - 固有ベクトルは複数ある
    - 同じ固有値に対する固有ベクトルは複数ある（ノルム違い）
    - 複数の固有値を持つことがある

### 動画 2-3~4 : 固有ベクトルの求め方
 - (a-λI)x=0 
   - 自明な解0を除くと行列式が0である必要がある
   - 固有値は各要素の比を決めるだけでおけ
     - 例題では一つめを1にしてた
     - 数値計算では固有ベクトルのノルムを1にすることが多い
 - 感想
   - 行列式=ゼロを解いた時点では必要条件であって、固有値であると断言はできない。最終的に確認する必要があると思う。

### 動画 2-5~7 :  固有値分解
 - 行列分解 -> 行列を分類したり、似ているものを見つけたりするために使用できる
   - 固有値分解はそのもっとも基本的な例
 - 固有値分解は、$ A = V \Lambda  V^{-1} $ とすること
   - ただし, $ \Lambda $ は対角行列で各固有値を対角成分に持つ
   - $ V $ は、$ \Lambda $ でならべ各固有値に対応する固有ベクトルを並べてできる行列
 - 例えばどんなご利益が? -> Aのべき乗の計算が簡単に行える!! 
 - その他備考
   - 固有ベクトルは定数倍の任意性がある -> 行列Vも 列ごとに定数倍の任意性がある
   - 固有値が大きい数字から並べることが多い


### 動画　2-8~11 : 特異値分解

 - 非正方行列に対する、固有値分解のようなもの
   - $ M v = \sigma u$
   - $ M^T u = \sigma v$
   - このような単位ベクトル $ u, v $ を見つけられれば、
   - $ M = U S V^T $
   - のように特異値分解できる。( $ U, V $ は直交行列 (ないしユニタリ行列))
 - 特異値の求め方の概要
   - $ MV = US $ ,  $ M^T U = V S ^T$
   - 直交行列 の逆行列はその転置なので、
   - $ M = USV^T$ , $ M^T = VS^T U^T$
   - これらの積を取ると,,
   - $ MM^T = USS^T U^T $
   - -> $ MM^T $ を固有値分解すれば、よい。
 - 特異値分解の利用例
   - 例1 : 画像圧縮
     - 960x720の画像を行列とみて特異値分解
     - 大きい順に並べた特異値や、特異値のベクトルをを最初からk番目までつかう. (k+1 ~ はすてる) 
     - 結果
       - 大きい方から 128 だけ取り出してもそれなりの画像に見える
       - 64でも、32でも、それなりにみえる。
       - 16まで落とすと, ぼやけてくる
   - 例2: 行列の類似性チェック
     - 特異値の大きい部分が似ていれば、同じ画像もしくは似てる画像なのではないか? 

## 講義動画まとめ: 統計学 

やはり、各用語の定義など既知のものばかりだったのだが、線形代数に比べれば、ど忘れする可能性の高い事項も含まれるので、こちらについては数式表現を用いた定義もある程度書き留めておくことにする。


### 動画1-1 : 目的
統計学・情報科学の 専門用語・数学の言葉で書かれた記述を読めるようにしたい

### 動画 1-2~4 : 集合

 - 2 : 集合とはなにか　
   - もののあつまり
     - もの = 要素(元:げん) 
       - 名前がつけられる (a,b,c,d..) 
       - 分割できず、互いに明確に区別できる。
   - 集合　S = {a,b,c,d,e,f,g}
     - $ a \in S $  : 集合Sに a が含まれる, 
     - $ h \notin S$ : h は含まれない. 
   - 部分集合 M = {c,d,g} 
     - $ M \subset S$ と書く
   - 確率・統計に登場する事象は集合として扱う

 - 3 : 集合同士の 和・積
   - $ A \cup B$ : 和集合 = Aの要素とBの要素を足した要素を集合に持つ
   - $ A \cap B$ : 共通部分 (積集合ではない) 
   - $U \backslash A = \bar{A} $ : 絶対補 = 全体の集合から A をのぞいたもの (Aの否定ともいう)
   - $B \backslash A $ : 相対補　= 集合B から　Aに含まれる要素をのぞいたもの
 - 4 : 練習問題

### 動画 1-5~10: 確率
 - 5: 二種類の確率
   - 両者の定義は異なるが、その後に適用できる数学は共通
   1. 頻度確率 (客観確率) = 発生する頻度
   2. ベイズ確率 (主観確率) = 信念の度合い

 - 6: (頻度)確率の定義
   - P(A) = 事象Aが起こる数 / すべての事象の数  $ := \frac{n(A)}{n(U)}$ 
     - 0から1の間の値
     - つまり、$ P(\bar{A}) = 1-P(A)$
 - 7: もうちょっと複雑な場合
   - $ P(A\cap B) = P(A) P(B|A) = P(B) P(A|B) $ 
   - $ P(B|A)$ : Aが起きた条件の元でBが起きる確率 
 - 8: 条件付き確率の定義
   - $ P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{n(A\cap B)}{n(B)}$
     - 例: 雨が降っている条件下での交通事故に遭う確率
   - 通常 確率計算の全体集合は Uだが、 これを事象Bに限定する
     - これはベン図で書くと明らか
 - 9: 同時確率
     - $ P(A\cap B) = P(A)P(B|A)$ だが、 
     - 事象A,Bが独立なら $ P(A\cap B) =  P(A) P(B) $
       - $P(B|A) = P(B) $ だから
 - 10: 和集合
   - $ P(A\cup B) = P(A) + P(B) - P(A\cap B)$
   - 引き算は、二重に数えている事象の数の分
 - 11: ベイズ則
   - 条件付き確率と、同時確率の式を組合せて導かれる
     - $ P(B|A) = \frac{P(A \cap B )}{P(A)} = \frac{P(A|B)P(B) }{P(A)} $ 
   - 使用例
      - A: 飴玉持ってる　P(A) = 1/4
      - B: 子供が笑顔  P(B) = 1/3
      - P(B|A) = 1/2 
      - 笑顔の子供が飴玉を持っている確率は? 
        - $ P(A|B) = P(A\cap B) /P(B) = P(A) P(B|A) /P(B) $
        - = 1/4 * 1/2 / (1/3) = 3/8


### 動画 2-1~2 : 統計の超基本用語
 - 1　
    - 記述統計 : 
      - 母集団が増えるとデータが増えて、一つ一つの情報を見るのは大変 -> 集団の性質を要約し記述する
      - 機械学習はどっちかっていうとコチラ
    - 推測統計 : 
      - 必ずしも全数調査はできない -> 一部を抽出した "標本" から、逆算して母集団の性質を推測する
 - 2 
   - 確率変数
     - 確率的な変数? -> 事象と結び付けられた数値だが、事象そのものをさすと解釈する場合も多い
     - 例: くじ引きの賞金 (あたりが出たらいくら、はずれがでたらいくら) 
   - 確率分布
     - 事象の発生する確率の分布
     - 例: 賞金100円になる確率は x, 賞金200円になる確率はy,,... みたいな
     - 離散値なら表として示せる

### 動画 2-3~5 : 基本的な統計量　
 - 3 :期待値
    - 定性的には 確率変数の平均の値 or 「ありえそう」な値
    - 確率で重み付けした、確率変数の平均
      - $ \mathrm{E}(f) = \Sigma_{k=1}^{n} P(X=x_k) f(X=x_k) $
      - $ \mathrm{E}(f) = \int P(X=x_k) f(X=x_k) dx $
 - 4 :  分散・共分散　
    - 分散
      - 定性的には、データの散らばり具合を表す
      - $\mathrm{Var} (f) = E\bigl[ \bigl( f(X=x) - \mathrm{E}(f) \bigr)^2 \bigr]  = \mathrm{E} (f^2(X=x) )  - ( \mathrm{E}(f))^2 $
      - もともとは、二乗ではなく絶対値だったんじゃないか?  
    - 共分散
      - 定性的には2つのデータの傾向の違いを表す
        - 正-> にてる, 負-> 逆の傾向, ゼロ->関係性に乏しい
      - $ \mathrm{Cov}(f,g ) \mathrm{ \bigl( ( f(X=x) - \mathrm{E}(f) ) \bigl( ( g(Y=y) - \mathrm{E}(g) ) \bigr) } = \mathrm{E} (fg)  - \mathrm{E}(f)\mathrm{E}(g)   $
 - 5 : 分散と標準偏差
    - 分散は2乗を使っているので元の値から単位が変わってしまう。 
    - ->  $ \sigma = \sqrt{\mathrm{Var}(f)}$ を "標準偏差" として導入する

### 動画　2-6~7 : 確率分布
 - 離散分布
    - ベルヌーイ分布
      - コイントス。裏と表で(一般的には)出る確率が異なっていてもよい
      - x=1なら表、x=0なら裏, 
      - $ P(x | \mu) = \mu^x (1-\mu)^{1-x} $
    - マルチヌーイ分布 (カテゴリカル分布)
      - kx=0,1 でなく、x = 0,1,2...N-1 をとり得る
      - ベルヌーイ分布と同じような式で表せる
    - 二項分布
      - ベルヌーイ分布の多試行版
      - $ P(x|\lambda,n) = \frac{n!}{x!(n-x)!} \lambda^x (1-\lambda)^{n-x}$
      - ※ cf. 多項分布
 - 連続分布
   - ガウス分布
     - $ N(x; \mu, \sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}} \exp\bigl( -\frac{1}{2\sigma^2} (x-\mu)^2 \bigr)  $
     - 真の分布が分からなくてもサンプルが多ければ正規分布に近づくと言われている?
       - 中心極限定理のことを言ってるのかな?
       - 例えば 二項分布のnを大きくすると近づくらしい..

### 動画 2-8~11 : 統計的推定
 - 8 : 統計的推定とは
    - 標本から、母集団の統計量を知ることを、統計的推定という
    - ここでいう母数 = 母集団の統計的パラメータ (母集団の要素数ではない) 
    - 点推定　- 平均値などを1つの値に推定する
    - 区間推定 - 幅をもたせた推定
 - 9
   - 推定量 (estimator) : パラメータを推定するために利用する関数, 計算方法・計算式
     - 推定関数ともいう
     - 例: 導関数
   - 推定値 (estimate) : 実際に試行を行った結果から計算した値
     - 例: 導関数に値を入れて、実際に得られた特定の点における微分値
   - 真の値 $\theta $ に対して $ \hat{\theta}$ を推定値/推定量として書くことがある
     - $ \hat{\theta}(x) $ のようになにかの関数になっていれば推定量

 - 点推定の例
   - 10 : 平均の推定
     - 標本平均 = 母集団から取り出した標本の平均値 
     - $ \hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} x_i $  の特徴
        - 一致性: サンプル数が大きくなれば、母集団の値に近づく
          - 注意: 一致性をもたない統計量もある
        - 不偏性: サンプル数がいくらであっても、その期待値は、母集団の値と同様
          - 数式で表すと $ \mathrm{E}( \hat{\theta}) = \theta $
            - ここの期待値は、標本そのものを増やして、それらの平均とるイメージ
   - 11 分散の推定
     - 推定標本分散 = $ \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x}) ^2$
        - 一致性 有り
        - 不変性 無し
          - 母分散に対して小さくなると知られている
          - 補正方法 $ \frac{n}{n-1} $ 倍にする = n で割るところを n-1 で割っている 
            - 定性的な説明 : 平均を決めた上で計算している = 標本の自由度が 1データ分減っている.  

### 動画 2-12~16 : 情報科学のさわり 

 - 12 情報をどのように数量化するのか
   - 箱の中の点の数が 1個増えたことに簡単に気づけるのは前者
     - 箱の中の点が 11個のA, 12個のBを比べる
     - 箱の中の点が  1個のC,  2個のDを比べる
   - 元の量に対しての増減の違いが重要なのでは? 
     - A, B の場合 $ \frac{\Delta w }{w} = \frac{1}{10} $
     - C, D の場合 $ \frac{\Delta w }{w} = \frac{1}{1} $
   - 点10個のE, 20個のFを比べるケースは、$\frac{\Delta w }{w} = \frac{1}{1} $ となるが、たしかに簡単. 
     - 個数は数えられなくても明らかに違うのが分かる
     - 情報の変化量が比で表されるならば、情報量そのものを知るには積分(和文)すればよい → 対数関数

 - 13 : 自己情報量
   - $ I(x) = -log(P(x)) = log(W(x)) $ 
     - 動画では、"$ W(x) $ は、x という事象に対する場合の数と捉えると、逆数は確率" というような説明がされていたが、この議論あまり直感的に感じられないなぁ。
       - 前節の例では、"点の数"という整数値で議論されていたが、実際には微分値として扱っているので、"1個の差を見つたい" と言う例にこだわらず, 最初から 場合の数が n 個のときの情報量を $f(n)$ で定義できるとして、その関数 $f$ の形を見つける問題を考えていた、と思った方がわかりやすい. 
   - 単位
      - 対数の底が2のとき、単位はビット
      - 対数の底がeのとき、単位はナット(nat)
   - 情報理論という分野で情報量が定義された時の使われ方
     - 0/1のスイッチを使って情報を表す場合の、スイッチの数を計算する (bit) 
 
 - 14 : シャノンエントロピー
   - 自己情報量の期待値 : $ H(x) = E(I(x)) = -E (log(P(x))) = - \sum (P(x) log(P(x))) $
     - 積分で計算することももちろんあるが、ここでは和文で表記する
   - 例: コイン投げ (表が出る確率 $p$ )
     - 平均値が一番高いのは $p=0.5$ のとき 
       - -> コイン投げたときに、表裏出る確率が等しいときが、一回投げたときに得られる情報量が大きい
     - 確率 0 とか 1  
       - 投げる前からどちらが出るか分かっているのだから、結果を見て得られる情報量はゼロ
   - エントロピー自体を誤差関数のように使って、問題を解くことがある

 - 分布の違いを表す量
   - ある事象が、分布 $Q$ に従う想定を持っていたが、実際に観測されたデータが従う分布は $P$という状況を想定する
     - より新しい情報である $P$ の分布を使って 期待値を計算する
　　
   - 15 : カルバック・ライブラー ダイバージェンス (KLダイバージェンス)
     - $  D_{\mathrm{KL}} (P \| Q) =  E_{x \sim P } \bigl[ \log \frac{P(x)}{Q(x)}  \bigr] =  \sum_x P(x) \log \frac{P(x)}{Q(x)}$
       -  期待値の中身 $ \log \frac{P(x)}{Q(x)}  = I(Q(x)) - I(P(x)) $ は 分布 $P,Q$ の情報量の差
     - "距離っぽい" 特性として $Q=P$ ならばゼロとなるような量として定義
       - 数学的な意味で距離の公理を満たすわけではないので注意が必要だが、カルバック・ライブラー情報量 とか、かるバック・ライブラー距離 という呼び方をされる

   - 16 : 交差エントロピー
      - $ H(P,Q) = -E_{x \sim P} \log Q(x)  = -\sum_x P(x) \log Q(x) $
        - 期待値の中身は、分布Qの自己情報量
        - KLダイバージェンスのような距離っぽさっは無いが、分布 $P,Q$ の関係を表す項だけをシンプルに含む
      - $ H(P,Q) = H(P) + D_{\mathrm{KL}} (P \| Q) $ の関係
      - しかし、交差エントロピーが使われるようになった経緯は、もともとはKLダーヴァージェンスとは関係ない
        - シャノンエントロピーのような情報の価値を計算したい
        - リアルな状況では、想定した分布 $Q$ と観測から得られる分布 $P$ はことなる状況が多々ある

## 取り組み概要 & 感想

下記のような日程・内容で取り組んだ

線形代数は 2/10-15 の間に少しずつ消化。それからしばらく、勤め先の業務の忙しさにかまけて1ヶ月計画的に放置したが、その後 3/16-18, 22 に統計学の講義動画を試聴。内容に応じて 1.2-1.5倍速で再生しつつ、基本的には動画を止めずにメモを箇条書きレベルでまとめていった。合計 5h 程度。

3/28, 29に動画講義のまとめを行う。基本的には、動画視聴中の箇条書きメモを markdown として整形しつつ、読み返して講義の内容を思い出せないところがあれば、少し見返して補った。合計 2h 程度。

3/30。演習は 20分、ステージテストは 35分 なので 合計約 1h。スタートテストと同じく、Python のインタプリタを起動して検算できるようにしつつも、紙・鉛筆も用意して、適宜使いやすい方を使う。

以上、合計 8h で終了。

演習については、線形代数の四則演算をのぞけば、
用語の定義自体を問題文で説明した上でシンプルな問に答えさせる問題や、用語の定義自体の理解を問う問題が大半だった。実力試し・理解度確認のための演習というより、講義動画の理解を助けるものという印象。例えば、自己情報量やエントロピーの説明は、講義と若干違う説明の仕方がされており、個人的には演習の説明のほうがすっきり理解できる。講義動画見ながら該当箇所の演習をやればよかったな、と思う。(次からそうしよう)


## 計画の見直し (2021/03/30 時点)

元々の計画では、3月中にステージ2まで終わらせる予定だったので、計画を下記のように見直した。

 - ~2021/2/15  : スタートテスト (2021/02/07完了)
 - ~2021/3/30  : ステージ1      (2021/03/30完了)
 - ~2021/4/18  : ステージ2 
 - ~2021/5/09  : ステージ3 
 - ~2021/6/06  : ステージ4 
 - ~2021/6/27  : 復習 -> 修了テスト 
 - ~2021/7/15  : Eもぎライト -> 今後の計画具体化 
 - ~2021/7/30  : シラバスの未習箇所の学習 
 - ~2021/8/26  : 全体の復習
 - 2021/8/27,28: E資格 受験 

ステージ2,3,4 のボリュームが読めないので、上記を少し前倒しですすめられるといいなとは思っている。

ラビットチャレンジの修了が後ろ倒しになったが、最近E資格を取得された方の話を聞く限り、修了後の勉強期間にそこまで長い時間を設けなくて良さそうな印象だったので問題ないと考える。